{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Majority vote with 3 classifiers\n",
    "# Paper based on this approach\n",
    "#  doi=10.1109/TNSE.2021.3099371"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/manero/DL/lib/python3.8/site-packages/xgboost/compat.py:36: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import MultiIndex, Int64Index\n",
      "2023-02-17 21:42:22.095123: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-02-17 21:42:22.095145: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "import scikitplot as skplt\n",
    "from xgboost import XGBClassifier\n",
    "from xgboost import plot_importance\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "%matplotlib inline \n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.utils.multiclass import unique_labels\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils import to_categorical\n",
    "from keras import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "import pandas as pd\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./Files_Majority_vote/128_features/filter1.csv\n",
      "n_estimators = 100 \n",
      "\n",
      "10-fold cross validation for file :  1\n",
      "GradientBoostingClassifier(max_depth=12, min_samples_split=12, n_estimators=5,\n",
      "                           random_state=3)\n",
      "kfold  1  score: 83.10%\n",
      "kfold  1  score: 85.59%\n",
      "GradientBoostingClassifier(max_depth=12, min_samples_split=12, n_estimators=5,\n",
      "                           random_state=3)\n",
      "kfold  1  score: 86.32%\n",
      "kfold  1  score: 88.48%\n",
      "GradientBoostingClassifier(max_depth=12, min_samples_split=12, n_estimators=5,\n",
      "                           random_state=3)\n",
      "kfold  1  score: 83.10%\n",
      "kfold  1  score: 85.68%\n",
      "GradientBoostingClassifier(max_depth=12, min_samples_split=12, n_estimators=5,\n",
      "                           random_state=3)\n",
      "kfold  1  score: 88.93%\n",
      "kfold  1  score: 88.39%\n",
      "GradientBoostingClassifier(max_depth=12, min_samples_split=12, n_estimators=5,\n",
      "                           random_state=3)\n",
      "kfold  1  score: 88.73%\n",
      "kfold  1  score: 91.54%\n",
      "GradientBoostingClassifier(max_depth=12, min_samples_split=12, n_estimators=5,\n",
      "                           random_state=3)\n",
      "kfold  1  score: 90.34%\n",
      "kfold  1  score: 94.07%\n",
      "GradientBoostingClassifier(max_depth=12, min_samples_split=12, n_estimators=5,\n",
      "                           random_state=3)\n",
      "kfold  1  score: 86.29%\n",
      "kfold  1  score: 87.32%\n",
      "GradientBoostingClassifier(max_depth=12, min_samples_split=12, n_estimators=5,\n",
      "                           random_state=3)\n",
      "kfold  1  score: 84.88%\n",
      "kfold  1  score: 88.52%\n",
      "GradientBoostingClassifier(max_depth=12, min_samples_split=12, n_estimators=5,\n",
      "                           random_state=3)\n",
      "kfold  1  score: 85.48%\n",
      "kfold  1  score: 88.43%\n",
      "GradientBoostingClassifier(max_depth=12, min_samples_split=12, n_estimators=5,\n",
      "                           random_state=3)\n",
      "kfold  1  score: 84.27%\n",
      "kfold  1  score: 86.13%\n",
      "Average score over k-fold 86.15% (+/-2.37%) [Gradient Boosting]\n",
      "RandomForestClassifier(max_depth=12, n_estimators=10, random_state=3)\n",
      "kfold  1  score: 96.58%\n",
      "kfold  1  score: 99.60%\n",
      "RandomForestClassifier(max_depth=12, n_estimators=10, random_state=3)\n",
      "kfold  1  score: 95.77%\n",
      "kfold  1  score: 99.53%\n",
      "RandomForestClassifier(max_depth=12, n_estimators=10, random_state=3)\n",
      "kfold  1  score: 96.58%\n",
      "kfold  1  score: 99.49%\n",
      "RandomForestClassifier(max_depth=12, n_estimators=10, random_state=3)\n",
      "kfold  1  score: 96.18%\n",
      "kfold  1  score: 99.46%\n",
      "RandomForestClassifier(max_depth=12, n_estimators=10, random_state=3)\n",
      "kfold  1  score: 95.17%\n",
      "kfold  1  score: 99.44%\n",
      "RandomForestClassifier(max_depth=12, n_estimators=10, random_state=3)\n",
      "kfold  1  score: 93.56%\n",
      "kfold  1  score: 99.37%\n",
      "RandomForestClassifier(max_depth=12, n_estimators=10, random_state=3)\n",
      "kfold  1  score: 96.17%\n",
      "kfold  1  score: 99.57%\n",
      "RandomForestClassifier(max_depth=12, n_estimators=10, random_state=3)\n",
      "kfold  1  score: 96.37%\n",
      "kfold  1  score: 99.55%\n",
      "RandomForestClassifier(max_depth=12, n_estimators=10, random_state=3)\n",
      "kfold  1  score: 96.77%\n",
      "kfold  1  score: 99.40%\n",
      "RandomForestClassifier(max_depth=12, n_estimators=10, random_state=3)\n",
      "kfold  1  score: 95.77%\n",
      "kfold  1  score: 99.44%\n",
      "Average score over k-fold 95.89% (+/-0.90%) [Random Forest]\n",
      "[21:46:36] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:46:36] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
      "              gamma=0, gpu_id=-1, importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.300000012,\n",
      "              max_delta_step=0, max_depth=12, max_leaf_nodes=None,\n",
      "              min_child_weight=1, min_samples_split=1, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=10, n_jobs=8,\n",
      "              num_parallel_tree=1, predictor='auto', random_state=3,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)\n",
      "kfold  1  score: 94.97%\n",
      "kfold  1  score: 99.22%\n",
      "[21:46:36] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:46:36] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
      "              gamma=0, gpu_id=-1, importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.300000012,\n",
      "              max_delta_step=0, max_depth=12, max_leaf_nodes=None,\n",
      "              min_child_weight=1, min_samples_split=1, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=10, n_jobs=8,\n",
      "              num_parallel_tree=1, predictor='auto', random_state=3,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)\n",
      "kfold  1  score: 93.76%\n",
      "kfold  1  score: 98.95%\n",
      "[21:46:36] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:46:36] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
      "              gamma=0, gpu_id=-1, importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.300000012,\n",
      "              max_delta_step=0, max_depth=12, max_leaf_nodes=None,\n",
      "              min_child_weight=1, min_samples_split=1, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=10, n_jobs=8,\n",
      "              num_parallel_tree=1, predictor='auto', random_state=3,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)\n",
      "kfold  1  score: 95.57%\n",
      "kfold  1  score: 99.28%\n",
      "[21:46:37] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:46:37] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
      "              gamma=0, gpu_id=-1, importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.300000012,\n",
      "              max_delta_step=0, max_depth=12, max_leaf_nodes=None,\n",
      "              min_child_weight=1, min_samples_split=1, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=10, n_jobs=8,\n",
      "              num_parallel_tree=1, predictor='auto', random_state=3,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)\n",
      "kfold  1  score: 94.77%\n",
      "kfold  1  score: 99.28%\n",
      "[21:46:37] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:46:37] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
      "              gamma=0, gpu_id=-1, importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.300000012,\n",
      "              max_delta_step=0, max_depth=12, max_leaf_nodes=None,\n",
      "              min_child_weight=1, min_samples_split=1, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=10, n_jobs=8,\n",
      "              num_parallel_tree=1, predictor='auto', random_state=3,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)\n",
      "kfold  1  score: 93.36%\n",
      "kfold  1  score: 99.13%\n",
      "[21:46:37] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:46:37] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
      "              gamma=0, gpu_id=-1, importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.300000012,\n",
      "              max_delta_step=0, max_depth=12, max_leaf_nodes=None,\n",
      "              min_child_weight=1, min_samples_split=1, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=10, n_jobs=8,\n",
      "              num_parallel_tree=1, predictor='auto', random_state=3,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)\n",
      "kfold  1  score: 93.96%\n",
      "kfold  1  score: 99.26%\n",
      "[21:46:37] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:46:37] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
      "              gamma=0, gpu_id=-1, importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.300000012,\n",
      "              max_delta_step=0, max_depth=12, max_leaf_nodes=None,\n",
      "              min_child_weight=1, min_samples_split=1, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=10, n_jobs=8,\n",
      "              num_parallel_tree=1, predictor='auto', random_state=3,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)\n",
      "kfold  1  score: 95.97%\n",
      "kfold  1  score: 98.88%\n",
      "[21:46:37] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:46:37] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
      "              gamma=0, gpu_id=-1, importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.300000012,\n",
      "              max_delta_step=0, max_depth=12, max_leaf_nodes=None,\n",
      "              min_child_weight=1, min_samples_split=1, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=10, n_jobs=8,\n",
      "              num_parallel_tree=1, predictor='auto', random_state=3,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)\n",
      "kfold  1  score: 96.57%\n",
      "kfold  1  score: 99.08%\n",
      "[21:46:37] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:46:37] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
      "              gamma=0, gpu_id=-1, importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.300000012,\n",
      "              max_delta_step=0, max_depth=12, max_leaf_nodes=None,\n",
      "              min_child_weight=1, min_samples_split=1, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=10, n_jobs=8,\n",
      "              num_parallel_tree=1, predictor='auto', random_state=3,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)\n",
      "kfold  1  score: 95.36%\n",
      "kfold  1  score: 98.90%\n",
      "[21:46:37] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:46:37] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
      "              gamma=0, gpu_id=-1, importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.300000012,\n",
      "              max_delta_step=0, max_depth=12, max_leaf_nodes=None,\n",
      "              min_child_weight=1, min_samples_split=1, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=10, n_jobs=8,\n",
      "              num_parallel_tree=1, predictor='auto', random_state=3,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)\n",
      "kfold  1  score: 95.36%\n",
      "kfold  1  score: 98.95%\n",
      "Average score over k-fold 94.97% (+/-0.97%) [XGBClassifier]\n",
      "[21:46:38] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:46:38] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "VotingClassifier(estimators=[('gb',\n",
      "                              GradientBoostingClassifier(max_depth=12,\n",
      "                                                         min_samples_split=12,\n",
      "                                                         n_estimators=5,\n",
      "                                                         random_state=3)),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(max_depth=12,\n",
      "                                                     n_estimators=10,\n",
      "                                                     random_state=3)),\n",
      "                             ('xgb',\n",
      "                              XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "                                            colsample_bylevel=1,\n",
      "                                            colsample_bynode=1,\n",
      "                                            colsample_bytree=1,\n",
      "                                            enable_categorical=False, gamma=0,\n",
      "                                            gpu...\n",
      "                                            interaction_constraints='',\n",
      "                                            learning_rate=0.300000012,\n",
      "                                            max_delta_step=0, max_depth=12,\n",
      "                                            max_leaf_nodes=None,\n",
      "                                            min_child_weight=1,\n",
      "                                            min_samples_split=1, missing=nan,\n",
      "                                            monotone_constraints='()',\n",
      "                                            n_estimators=10, n_jobs=8,\n",
      "                                            num_parallel_tree=1,\n",
      "                                            predictor='auto', random_state=3,\n",
      "                                            reg_alpha=0, reg_lambda=1,\n",
      "                                            scale_pos_weight=1, subsample=1,\n",
      "                                            tree_method='exact',\n",
      "                                            validate_parameters=1,\n",
      "                                            verbosity=None))])\n",
      "kfold  1  score: 95.98%\n",
      "kfold  1  score: 99.31%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21:46:38] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:46:38] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "VotingClassifier(estimators=[('gb',\n",
      "                              GradientBoostingClassifier(max_depth=12,\n",
      "                                                         min_samples_split=12,\n",
      "                                                         n_estimators=5,\n",
      "                                                         random_state=3)),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(max_depth=12,\n",
      "                                                     n_estimators=10,\n",
      "                                                     random_state=3)),\n",
      "                             ('xgb',\n",
      "                              XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "                                            colsample_bylevel=1,\n",
      "                                            colsample_bynode=1,\n",
      "                                            colsample_bytree=1,\n",
      "                                            enable_categorical=False, gamma=0,\n",
      "                                            gpu...\n",
      "                                            interaction_constraints='',\n",
      "                                            learning_rate=0.300000012,\n",
      "                                            max_delta_step=0, max_depth=12,\n",
      "                                            max_leaf_nodes=None,\n",
      "                                            min_child_weight=1,\n",
      "                                            min_samples_split=1, missing=nan,\n",
      "                                            monotone_constraints='()',\n",
      "                                            n_estimators=10, n_jobs=8,\n",
      "                                            num_parallel_tree=1,\n",
      "                                            predictor='auto', random_state=3,\n",
      "                                            reg_alpha=0, reg_lambda=1,\n",
      "                                            scale_pos_weight=1, subsample=1,\n",
      "                                            tree_method='exact',\n",
      "                                            validate_parameters=1,\n",
      "                                            verbosity=None))])\n",
      "kfold  1  score: 94.77%\n",
      "kfold  1  score: 99.02%\n",
      "[21:46:39] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:46:39] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "VotingClassifier(estimators=[('gb',\n",
      "                              GradientBoostingClassifier(max_depth=12,\n",
      "                                                         min_samples_split=12,\n",
      "                                                         n_estimators=5,\n",
      "                                                         random_state=3)),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(max_depth=12,\n",
      "                                                     n_estimators=10,\n",
      "                                                     random_state=3)),\n",
      "                             ('xgb',\n",
      "                              XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "                                            colsample_bylevel=1,\n",
      "                                            colsample_bynode=1,\n",
      "                                            colsample_bytree=1,\n",
      "                                            enable_categorical=False, gamma=0,\n",
      "                                            gpu...\n",
      "                                            interaction_constraints='',\n",
      "                                            learning_rate=0.300000012,\n",
      "                                            max_delta_step=0, max_depth=12,\n",
      "                                            max_leaf_nodes=None,\n",
      "                                            min_child_weight=1,\n",
      "                                            min_samples_split=1, missing=nan,\n",
      "                                            monotone_constraints='()',\n",
      "                                            n_estimators=10, n_jobs=8,\n",
      "                                            num_parallel_tree=1,\n",
      "                                            predictor='auto', random_state=3,\n",
      "                                            reg_alpha=0, reg_lambda=1,\n",
      "                                            scale_pos_weight=1, subsample=1,\n",
      "                                            tree_method='exact',\n",
      "                                            validate_parameters=1,\n",
      "                                            verbosity=None))])\n",
      "kfold  1  score: 96.18%\n",
      "kfold  1  score: 99.24%\n",
      "[21:46:39] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:46:39] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "VotingClassifier(estimators=[('gb',\n",
      "                              GradientBoostingClassifier(max_depth=12,\n",
      "                                                         min_samples_split=12,\n",
      "                                                         n_estimators=5,\n",
      "                                                         random_state=3)),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(max_depth=12,\n",
      "                                                     n_estimators=10,\n",
      "                                                     random_state=3)),\n",
      "                             ('xgb',\n",
      "                              XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "                                            colsample_bylevel=1,\n",
      "                                            colsample_bynode=1,\n",
      "                                            colsample_bytree=1,\n",
      "                                            enable_categorical=False, gamma=0,\n",
      "                                            gpu...\n",
      "                                            interaction_constraints='',\n",
      "                                            learning_rate=0.300000012,\n",
      "                                            max_delta_step=0, max_depth=12,\n",
      "                                            max_leaf_nodes=None,\n",
      "                                            min_child_weight=1,\n",
      "                                            min_samples_split=1, missing=nan,\n",
      "                                            monotone_constraints='()',\n",
      "                                            n_estimators=10, n_jobs=8,\n",
      "                                            num_parallel_tree=1,\n",
      "                                            predictor='auto', random_state=3,\n",
      "                                            reg_alpha=0, reg_lambda=1,\n",
      "                                            scale_pos_weight=1, subsample=1,\n",
      "                                            tree_method='exact',\n",
      "                                            validate_parameters=1,\n",
      "                                            verbosity=None))])\n",
      "kfold  1  score: 95.57%\n",
      "kfold  1  score: 99.31%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21:46:40] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:46:40] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "VotingClassifier(estimators=[('gb',\n",
      "                              GradientBoostingClassifier(max_depth=12,\n",
      "                                                         min_samples_split=12,\n",
      "                                                         n_estimators=5,\n",
      "                                                         random_state=3)),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(max_depth=12,\n",
      "                                                     n_estimators=10,\n",
      "                                                     random_state=3)),\n",
      "                             ('xgb',\n",
      "                              XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "                                            colsample_bylevel=1,\n",
      "                                            colsample_bynode=1,\n",
      "                                            colsample_bytree=1,\n",
      "                                            enable_categorical=False, gamma=0,\n",
      "                                            gpu...\n",
      "                                            interaction_constraints='',\n",
      "                                            learning_rate=0.300000012,\n",
      "                                            max_delta_step=0, max_depth=12,\n",
      "                                            max_leaf_nodes=None,\n",
      "                                            min_child_weight=1,\n",
      "                                            min_samples_split=1, missing=nan,\n",
      "                                            monotone_constraints='()',\n",
      "                                            n_estimators=10, n_jobs=8,\n",
      "                                            num_parallel_tree=1,\n",
      "                                            predictor='auto', random_state=3,\n",
      "                                            reg_alpha=0, reg_lambda=1,\n",
      "                                            scale_pos_weight=1, subsample=1,\n",
      "                                            tree_method='exact',\n",
      "                                            validate_parameters=1,\n",
      "                                            verbosity=None))])\n",
      "kfold  1  score: 93.76%\n",
      "kfold  1  score: 98.99%\n",
      "[21:46:40] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:46:40] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "VotingClassifier(estimators=[('gb',\n",
      "                              GradientBoostingClassifier(max_depth=12,\n",
      "                                                         min_samples_split=12,\n",
      "                                                         n_estimators=5,\n",
      "                                                         random_state=3)),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(max_depth=12,\n",
      "                                                     n_estimators=10,\n",
      "                                                     random_state=3)),\n",
      "                             ('xgb',\n",
      "                              XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "                                            colsample_bylevel=1,\n",
      "                                            colsample_bynode=1,\n",
      "                                            colsample_bytree=1,\n",
      "                                            enable_categorical=False, gamma=0,\n",
      "                                            gpu...\n",
      "                                            interaction_constraints='',\n",
      "                                            learning_rate=0.300000012,\n",
      "                                            max_delta_step=0, max_depth=12,\n",
      "                                            max_leaf_nodes=None,\n",
      "                                            min_child_weight=1,\n",
      "                                            min_samples_split=1, missing=nan,\n",
      "                                            monotone_constraints='()',\n",
      "                                            n_estimators=10, n_jobs=8,\n",
      "                                            num_parallel_tree=1,\n",
      "                                            predictor='auto', random_state=3,\n",
      "                                            reg_alpha=0, reg_lambda=1,\n",
      "                                            scale_pos_weight=1, subsample=1,\n",
      "                                            tree_method='exact',\n",
      "                                            validate_parameters=1,\n",
      "                                            verbosity=None))])\n",
      "kfold  1  score: 94.16%\n",
      "kfold  1  score: 99.46%\n",
      "[21:46:40] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:46:40] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "VotingClassifier(estimators=[('gb',\n",
      "                              GradientBoostingClassifier(max_depth=12,\n",
      "                                                         min_samples_split=12,\n",
      "                                                         n_estimators=5,\n",
      "                                                         random_state=3)),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(max_depth=12,\n",
      "                                                     n_estimators=10,\n",
      "                                                     random_state=3)),\n",
      "                             ('xgb',\n",
      "                              XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "                                            colsample_bylevel=1,\n",
      "                                            colsample_bynode=1,\n",
      "                                            colsample_bytree=1,\n",
      "                                            enable_categorical=False, gamma=0,\n",
      "                                            gpu...\n",
      "                                            interaction_constraints='',\n",
      "                                            learning_rate=0.300000012,\n",
      "                                            max_delta_step=0, max_depth=12,\n",
      "                                            max_leaf_nodes=None,\n",
      "                                            min_child_weight=1,\n",
      "                                            min_samples_split=1, missing=nan,\n",
      "                                            monotone_constraints='()',\n",
      "                                            n_estimators=10, n_jobs=8,\n",
      "                                            num_parallel_tree=1,\n",
      "                                            predictor='auto', random_state=3,\n",
      "                                            reg_alpha=0, reg_lambda=1,\n",
      "                                            scale_pos_weight=1, subsample=1,\n",
      "                                            tree_method='exact',\n",
      "                                            validate_parameters=1,\n",
      "                                            verbosity=None))])\n",
      "kfold  1  score: 96.98%\n",
      "kfold  1  score: 98.97%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21:46:41] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:46:41] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "VotingClassifier(estimators=[('gb',\n",
      "                              GradientBoostingClassifier(max_depth=12,\n",
      "                                                         min_samples_split=12,\n",
      "                                                         n_estimators=5,\n",
      "                                                         random_state=3)),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(max_depth=12,\n",
      "                                                     n_estimators=10,\n",
      "                                                     random_state=3)),\n",
      "                             ('xgb',\n",
      "                              XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "                                            colsample_bylevel=1,\n",
      "                                            colsample_bynode=1,\n",
      "                                            colsample_bytree=1,\n",
      "                                            enable_categorical=False, gamma=0,\n",
      "                                            gpu...\n",
      "                                            interaction_constraints='',\n",
      "                                            learning_rate=0.300000012,\n",
      "                                            max_delta_step=0, max_depth=12,\n",
      "                                            max_leaf_nodes=None,\n",
      "                                            min_child_weight=1,\n",
      "                                            min_samples_split=1, missing=nan,\n",
      "                                            monotone_constraints='()',\n",
      "                                            n_estimators=10, n_jobs=8,\n",
      "                                            num_parallel_tree=1,\n",
      "                                            predictor='auto', random_state=3,\n",
      "                                            reg_alpha=0, reg_lambda=1,\n",
      "                                            scale_pos_weight=1, subsample=1,\n",
      "                                            tree_method='exact',\n",
      "                                            validate_parameters=1,\n",
      "                                            verbosity=None))])\n",
      "kfold  1  score: 95.77%\n",
      "kfold  1  score: 99.08%\n",
      "[21:46:41] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:46:41] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "VotingClassifier(estimators=[('gb',\n",
      "                              GradientBoostingClassifier(max_depth=12,\n",
      "                                                         min_samples_split=12,\n",
      "                                                         n_estimators=5,\n",
      "                                                         random_state=3)),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(max_depth=12,\n",
      "                                                     n_estimators=10,\n",
      "                                                     random_state=3)),\n",
      "                             ('xgb',\n",
      "                              XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "                                            colsample_bylevel=1,\n",
      "                                            colsample_bynode=1,\n",
      "                                            colsample_bytree=1,\n",
      "                                            enable_categorical=False, gamma=0,\n",
      "                                            gpu...\n",
      "                                            interaction_constraints='',\n",
      "                                            learning_rate=0.300000012,\n",
      "                                            max_delta_step=0, max_depth=12,\n",
      "                                            max_leaf_nodes=None,\n",
      "                                            min_child_weight=1,\n",
      "                                            min_samples_split=1, missing=nan,\n",
      "                                            monotone_constraints='()',\n",
      "                                            n_estimators=10, n_jobs=8,\n",
      "                                            num_parallel_tree=1,\n",
      "                                            predictor='auto', random_state=3,\n",
      "                                            reg_alpha=0, reg_lambda=1,\n",
      "                                            scale_pos_weight=1, subsample=1,\n",
      "                                            tree_method='exact',\n",
      "                                            validate_parameters=1,\n",
      "                                            verbosity=None))])\n",
      "kfold  1  score: 95.36%\n",
      "kfold  1  score: 98.79%\n",
      "[21:46:42] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:46:42] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "VotingClassifier(estimators=[('gb',\n",
      "                              GradientBoostingClassifier(max_depth=12,\n",
      "                                                         min_samples_split=12,\n",
      "                                                         n_estimators=5,\n",
      "                                                         random_state=3)),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(max_depth=12,\n",
      "                                                     n_estimators=10,\n",
      "                                                     random_state=3)),\n",
      "                             ('xgb',\n",
      "                              XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "                                            colsample_bylevel=1,\n",
      "                                            colsample_bynode=1,\n",
      "                                            colsample_bytree=1,\n",
      "                                            enable_categorical=False, gamma=0,\n",
      "                                            gpu...\n",
      "                                            interaction_constraints='',\n",
      "                                            learning_rate=0.300000012,\n",
      "                                            max_delta_step=0, max_depth=12,\n",
      "                                            max_leaf_nodes=None,\n",
      "                                            min_child_weight=1,\n",
      "                                            min_samples_split=1, missing=nan,\n",
      "                                            monotone_constraints='()',\n",
      "                                            n_estimators=10, n_jobs=8,\n",
      "                                            num_parallel_tree=1,\n",
      "                                            predictor='auto', random_state=3,\n",
      "                                            reg_alpha=0, reg_lambda=1,\n",
      "                                            scale_pos_weight=1, subsample=1,\n",
      "                                            tree_method='exact',\n",
      "                                            validate_parameters=1,\n",
      "                                            verbosity=None))])\n",
      "kfold  1  score: 95.16%\n",
      "kfold  1  score: 99.06%\n",
      "Average score over k-fold 95.37% (+/-0.91%) [Ensemble Majority]\n",
      "./Files_Majority_vote/128_features/filter2.csv\n",
      "n_estimators = 100 \n",
      "\n",
      "10-fold cross validation for file :  2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GradientBoostingClassifier(max_depth=12, min_samples_split=12, n_estimators=5,\n",
      "                           random_state=3)\n",
      "kfold  2  score: 89.74%\n",
      "kfold  2  score: 95.57%\n",
      "GradientBoostingClassifier(max_depth=12, min_samples_split=12, n_estimators=5,\n",
      "                           random_state=3)\n",
      "kfold  2  score: 93.89%\n",
      "kfold  2  score: 94.65%\n",
      "GradientBoostingClassifier(max_depth=12, min_samples_split=12, n_estimators=5,\n",
      "                           random_state=3)\n",
      "kfold  2  score: 90.93%\n",
      "kfold  2  score: 96.10%\n",
      "GradientBoostingClassifier(max_depth=12, min_samples_split=12, n_estimators=5,\n",
      "                           random_state=3)\n",
      "kfold  2  score: 91.91%\n",
      "kfold  2  score: 95.40%\n",
      "GradientBoostingClassifier(max_depth=12, min_samples_split=12, n_estimators=5,\n",
      "                           random_state=3)\n",
      "kfold  2  score: 90.14%\n",
      "kfold  2  score: 95.84%\n",
      "GradientBoostingClassifier(max_depth=12, min_samples_split=12, n_estimators=5,\n",
      "                           random_state=3)\n",
      "kfold  2  score: 90.93%\n",
      "kfold  2  score: 94.74%\n",
      "GradientBoostingClassifier(max_depth=12, min_samples_split=12, n_estimators=5,\n",
      "                           random_state=3)\n",
      "kfold  2  score: 92.50%\n",
      "kfold  2  score: 95.42%\n",
      "GradientBoostingClassifier(max_depth=12, min_samples_split=12, n_estimators=5,\n",
      "                           random_state=3)\n",
      "kfold  2  score: 89.55%\n",
      "kfold  2  score: 95.27%\n",
      "GradientBoostingClassifier(max_depth=12, min_samples_split=12, n_estimators=5,\n",
      "                           random_state=3)\n",
      "kfold  2  score: 92.11%\n",
      "kfold  2  score: 97.00%\n",
      "GradientBoostingClassifier(max_depth=12, min_samples_split=12, n_estimators=5,\n",
      "                           random_state=3)\n",
      "kfold  2  score: 89.92%\n",
      "kfold  2  score: 96.23%\n",
      "Average score over k-fold 91.16% (+/-1.34%) [Gradient Boosting]\n",
      "RandomForestClassifier(max_depth=12, n_estimators=10, random_state=3)\n",
      "kfold  2  score: 94.08%\n",
      "kfold  2  score: 98.93%\n",
      "RandomForestClassifier(max_depth=12, n_estimators=10, random_state=3)\n",
      "kfold  2  score: 96.65%\n",
      "kfold  2  score: 99.04%\n",
      "RandomForestClassifier(max_depth=12, n_estimators=10, random_state=3)\n",
      "kfold  2  score: 92.70%\n",
      "kfold  2  score: 99.21%\n",
      "RandomForestClassifier(max_depth=12, n_estimators=10, random_state=3)\n",
      "kfold  2  score: 93.69%\n",
      "kfold  2  score: 98.99%\n",
      "RandomForestClassifier(max_depth=12, n_estimators=10, random_state=3)\n",
      "kfold  2  score: 92.90%\n",
      "kfold  2  score: 98.73%\n",
      "RandomForestClassifier(max_depth=12, n_estimators=10, random_state=3)\n",
      "kfold  2  score: 93.10%\n",
      "kfold  2  score: 99.12%\n",
      "RandomForestClassifier(max_depth=12, n_estimators=10, random_state=3)\n",
      "kfold  2  score: 93.89%\n",
      "kfold  2  score: 98.64%\n",
      "RandomForestClassifier(max_depth=12, n_estimators=10, random_state=3)\n",
      "kfold  2  score: 95.07%\n",
      "kfold  2  score: 99.04%\n",
      "RandomForestClassifier(max_depth=12, n_estimators=10, random_state=3)\n",
      "kfold  2  score: 93.10%\n",
      "kfold  2  score: 99.17%\n",
      "RandomForestClassifier(max_depth=12, n_estimators=10, random_state=3)\n",
      "kfold  2  score: 91.90%\n",
      "kfold  2  score: 98.84%\n",
      "Average score over k-fold 93.71% (+/-1.28%) [Random Forest]\n",
      "[21:46:45] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:46:45] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
      "              gamma=0, gpu_id=-1, importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.300000012,\n",
      "              max_delta_step=0, max_depth=12, max_leaf_nodes=None,\n",
      "              min_child_weight=1, min_samples_split=1, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=10, n_jobs=8,\n",
      "              num_parallel_tree=1, predictor='auto', random_state=3,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)\n",
      "kfold  2  score: 93.89%\n",
      "kfold  2  score: 99.19%\n",
      "[21:46:45] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:46:45] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
      "              gamma=0, gpu_id=-1, importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.300000012,\n",
      "              max_delta_step=0, max_depth=12, max_leaf_nodes=None,\n",
      "              min_child_weight=1, min_samples_split=1, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=10, n_jobs=8,\n",
      "              num_parallel_tree=1, predictor='auto', random_state=3,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)\n",
      "kfold  2  score: 96.06%\n",
      "kfold  2  score: 99.17%\n",
      "[21:46:45] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:46:45] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
      "              gamma=0, gpu_id=-1, importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.300000012,\n",
      "              max_delta_step=0, max_depth=12, max_leaf_nodes=None,\n",
      "              min_child_weight=1, min_samples_split=1, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=10, n_jobs=8,\n",
      "              num_parallel_tree=1, predictor='auto', random_state=3,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)\n",
      "kfold  2  score: 92.31%\n",
      "kfold  2  score: 99.25%\n",
      "[21:46:45] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:46:45] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
      "              gamma=0, gpu_id=-1, importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.300000012,\n",
      "              max_delta_step=0, max_depth=12, max_leaf_nodes=None,\n",
      "              min_child_weight=1, min_samples_split=1, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=10, n_jobs=8,\n",
      "              num_parallel_tree=1, predictor='auto', random_state=3,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)\n",
      "kfold  2  score: 92.90%\n",
      "kfold  2  score: 99.32%\n",
      "[21:46:45] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:46:45] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
      "              gamma=0, gpu_id=-1, importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.300000012,\n",
      "              max_delta_step=0, max_depth=12, max_leaf_nodes=None,\n",
      "              min_child_weight=1, min_samples_split=1, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=10, n_jobs=8,\n",
      "              num_parallel_tree=1, predictor='auto', random_state=3,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)\n",
      "kfold  2  score: 92.31%\n",
      "kfold  2  score: 99.01%\n",
      "[21:46:45] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:46:45] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
      "              gamma=0, gpu_id=-1, importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.300000012,\n",
      "              max_delta_step=0, max_depth=12, max_leaf_nodes=None,\n",
      "              min_child_weight=1, min_samples_split=1, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=10, n_jobs=8,\n",
      "              num_parallel_tree=1, predictor='auto', random_state=3,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)\n",
      "kfold  2  score: 94.48%\n",
      "kfold  2  score: 99.08%\n",
      "[21:46:46] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:46:46] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
      "              gamma=0, gpu_id=-1, importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.300000012,\n",
      "              max_delta_step=0, max_depth=12, max_leaf_nodes=None,\n",
      "              min_child_weight=1, min_samples_split=1, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=10, n_jobs=8,\n",
      "              num_parallel_tree=1, predictor='auto', random_state=3,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)\n",
      "kfold  2  score: 93.29%\n",
      "kfold  2  score: 99.04%\n",
      "[21:46:46] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:46:46] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
      "              gamma=0, gpu_id=-1, importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.300000012,\n",
      "              max_delta_step=0, max_depth=12, max_leaf_nodes=None,\n",
      "              min_child_weight=1, min_samples_split=1, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=10, n_jobs=8,\n",
      "              num_parallel_tree=1, predictor='auto', random_state=3,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)\n",
      "kfold  2  score: 93.69%\n",
      "kfold  2  score: 99.19%\n",
      "[21:46:46] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:46:46] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
      "              gamma=0, gpu_id=-1, importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.300000012,\n",
      "              max_delta_step=0, max_depth=12, max_leaf_nodes=None,\n",
      "              min_child_weight=1, min_samples_split=1, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=10, n_jobs=8,\n",
      "              num_parallel_tree=1, predictor='auto', random_state=3,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)\n",
      "kfold  2  score: 95.46%\n",
      "kfold  2  score: 99.47%\n",
      "[21:46:46] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:46:46] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
      "              gamma=0, gpu_id=-1, importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.300000012,\n",
      "              max_delta_step=0, max_depth=12, max_leaf_nodes=None,\n",
      "              min_child_weight=1, min_samples_split=1, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=10, n_jobs=8,\n",
      "              num_parallel_tree=1, predictor='auto', random_state=3,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)\n",
      "kfold  2  score: 92.09%\n",
      "kfold  2  score: 99.25%\n",
      "Average score over k-fold 93.65% (+/-1.29%) [XGBClassifier]\n",
      "[21:46:47] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:46:47] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "VotingClassifier(estimators=[('gb',\n",
      "                              GradientBoostingClassifier(max_depth=12,\n",
      "                                                         min_samples_split=12,\n",
      "                                                         n_estimators=5,\n",
      "                                                         random_state=3)),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(max_depth=12,\n",
      "                                                     n_estimators=10,\n",
      "                                                     random_state=3)),\n",
      "                             ('xgb',\n",
      "                              XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "                                            colsample_bylevel=1,\n",
      "                                            colsample_bynode=1,\n",
      "                                            colsample_bytree=1,\n",
      "                                            enable_categorical=False, gamma=0,\n",
      "                                            gpu...\n",
      "                                            interaction_constraints='',\n",
      "                                            learning_rate=0.300000012,\n",
      "                                            max_delta_step=0, max_depth=12,\n",
      "                                            max_leaf_nodes=None,\n",
      "                                            min_child_weight=1,\n",
      "                                            min_samples_split=1, missing=nan,\n",
      "                                            monotone_constraints='()',\n",
      "                                            n_estimators=10, n_jobs=8,\n",
      "                                            num_parallel_tree=1,\n",
      "                                            predictor='auto', random_state=3,\n",
      "                                            reg_alpha=0, reg_lambda=1,\n",
      "                                            scale_pos_weight=1, subsample=1,\n",
      "                                            tree_method='exact',\n",
      "                                            validate_parameters=1,\n",
      "                                            verbosity=None))])\n",
      "kfold  2  score: 93.69%\n",
      "kfold  2  score: 99.01%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21:46:47] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:46:47] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "VotingClassifier(estimators=[('gb',\n",
      "                              GradientBoostingClassifier(max_depth=12,\n",
      "                                                         min_samples_split=12,\n",
      "                                                         n_estimators=5,\n",
      "                                                         random_state=3)),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(max_depth=12,\n",
      "                                                     n_estimators=10,\n",
      "                                                     random_state=3)),\n",
      "                             ('xgb',\n",
      "                              XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "                                            colsample_bylevel=1,\n",
      "                                            colsample_bynode=1,\n",
      "                                            colsample_bytree=1,\n",
      "                                            enable_categorical=False, gamma=0,\n",
      "                                            gpu...\n",
      "                                            interaction_constraints='',\n",
      "                                            learning_rate=0.300000012,\n",
      "                                            max_delta_step=0, max_depth=12,\n",
      "                                            max_leaf_nodes=None,\n",
      "                                            min_child_weight=1,\n",
      "                                            min_samples_split=1, missing=nan,\n",
      "                                            monotone_constraints='()',\n",
      "                                            n_estimators=10, n_jobs=8,\n",
      "                                            num_parallel_tree=1,\n",
      "                                            predictor='auto', random_state=3,\n",
      "                                            reg_alpha=0, reg_lambda=1,\n",
      "                                            scale_pos_weight=1, subsample=1,\n",
      "                                            tree_method='exact',\n",
      "                                            validate_parameters=1,\n",
      "                                            verbosity=None))])\n",
      "kfold  2  score: 96.65%\n",
      "kfold  2  score: 98.86%\n",
      "[21:46:48] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:46:48] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "VotingClassifier(estimators=[('gb',\n",
      "                              GradientBoostingClassifier(max_depth=12,\n",
      "                                                         min_samples_split=12,\n",
      "                                                         n_estimators=5,\n",
      "                                                         random_state=3)),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(max_depth=12,\n",
      "                                                     n_estimators=10,\n",
      "                                                     random_state=3)),\n",
      "                             ('xgb',\n",
      "                              XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "                                            colsample_bylevel=1,\n",
      "                                            colsample_bynode=1,\n",
      "                                            colsample_bytree=1,\n",
      "                                            enable_categorical=False, gamma=0,\n",
      "                                            gpu...\n",
      "                                            interaction_constraints='',\n",
      "                                            learning_rate=0.300000012,\n",
      "                                            max_delta_step=0, max_depth=12,\n",
      "                                            max_leaf_nodes=None,\n",
      "                                            min_child_weight=1,\n",
      "                                            min_samples_split=1, missing=nan,\n",
      "                                            monotone_constraints='()',\n",
      "                                            n_estimators=10, n_jobs=8,\n",
      "                                            num_parallel_tree=1,\n",
      "                                            predictor='auto', random_state=3,\n",
      "                                            reg_alpha=0, reg_lambda=1,\n",
      "                                            scale_pos_weight=1, subsample=1,\n",
      "                                            tree_method='exact',\n",
      "                                            validate_parameters=1,\n",
      "                                            verbosity=None))])\n",
      "kfold  2  score: 92.50%\n",
      "kfold  2  score: 99.28%\n",
      "[21:46:48] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:46:48] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "VotingClassifier(estimators=[('gb',\n",
      "                              GradientBoostingClassifier(max_depth=12,\n",
      "                                                         min_samples_split=12,\n",
      "                                                         n_estimators=5,\n",
      "                                                         random_state=3)),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(max_depth=12,\n",
      "                                                     n_estimators=10,\n",
      "                                                     random_state=3)),\n",
      "                             ('xgb',\n",
      "                              XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "                                            colsample_bylevel=1,\n",
      "                                            colsample_bynode=1,\n",
      "                                            colsample_bytree=1,\n",
      "                                            enable_categorical=False, gamma=0,\n",
      "                                            gpu...\n",
      "                                            interaction_constraints='',\n",
      "                                            learning_rate=0.300000012,\n",
      "                                            max_delta_step=0, max_depth=12,\n",
      "                                            max_leaf_nodes=None,\n",
      "                                            min_child_weight=1,\n",
      "                                            min_samples_split=1, missing=nan,\n",
      "                                            monotone_constraints='()',\n",
      "                                            n_estimators=10, n_jobs=8,\n",
      "                                            num_parallel_tree=1,\n",
      "                                            predictor='auto', random_state=3,\n",
      "                                            reg_alpha=0, reg_lambda=1,\n",
      "                                            scale_pos_weight=1, subsample=1,\n",
      "                                            tree_method='exact',\n",
      "                                            validate_parameters=1,\n",
      "                                            verbosity=None))])\n",
      "kfold  2  score: 93.29%\n",
      "kfold  2  score: 98.97%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21:46:49] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:46:49] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "VotingClassifier(estimators=[('gb',\n",
      "                              GradientBoostingClassifier(max_depth=12,\n",
      "                                                         min_samples_split=12,\n",
      "                                                         n_estimators=5,\n",
      "                                                         random_state=3)),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(max_depth=12,\n",
      "                                                     n_estimators=10,\n",
      "                                                     random_state=3)),\n",
      "                             ('xgb',\n",
      "                              XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "                                            colsample_bylevel=1,\n",
      "                                            colsample_bynode=1,\n",
      "                                            colsample_bytree=1,\n",
      "                                            enable_categorical=False, gamma=0,\n",
      "                                            gpu...\n",
      "                                            interaction_constraints='',\n",
      "                                            learning_rate=0.300000012,\n",
      "                                            max_delta_step=0, max_depth=12,\n",
      "                                            max_leaf_nodes=None,\n",
      "                                            min_child_weight=1,\n",
      "                                            min_samples_split=1, missing=nan,\n",
      "                                            monotone_constraints='()',\n",
      "                                            n_estimators=10, n_jobs=8,\n",
      "                                            num_parallel_tree=1,\n",
      "                                            predictor='auto', random_state=3,\n",
      "                                            reg_alpha=0, reg_lambda=1,\n",
      "                                            scale_pos_weight=1, subsample=1,\n",
      "                                            tree_method='exact',\n",
      "                                            validate_parameters=1,\n",
      "                                            verbosity=None))])\n",
      "kfold  2  score: 92.90%\n",
      "kfold  2  score: 98.73%\n",
      "[21:46:49] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:46:49] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "VotingClassifier(estimators=[('gb',\n",
      "                              GradientBoostingClassifier(max_depth=12,\n",
      "                                                         min_samples_split=12,\n",
      "                                                         n_estimators=5,\n",
      "                                                         random_state=3)),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(max_depth=12,\n",
      "                                                     n_estimators=10,\n",
      "                                                     random_state=3)),\n",
      "                             ('xgb',\n",
      "                              XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "                                            colsample_bylevel=1,\n",
      "                                            colsample_bynode=1,\n",
      "                                            colsample_bytree=1,\n",
      "                                            enable_categorical=False, gamma=0,\n",
      "                                            gpu...\n",
      "                                            interaction_constraints='',\n",
      "                                            learning_rate=0.300000012,\n",
      "                                            max_delta_step=0, max_depth=12,\n",
      "                                            max_leaf_nodes=None,\n",
      "                                            min_child_weight=1,\n",
      "                                            min_samples_split=1, missing=nan,\n",
      "                                            monotone_constraints='()',\n",
      "                                            n_estimators=10, n_jobs=8,\n",
      "                                            num_parallel_tree=1,\n",
      "                                            predictor='auto', random_state=3,\n",
      "                                            reg_alpha=0, reg_lambda=1,\n",
      "                                            scale_pos_weight=1, subsample=1,\n",
      "                                            tree_method='exact',\n",
      "                                            validate_parameters=1,\n",
      "                                            verbosity=None))])\n",
      "kfold  2  score: 94.48%\n",
      "kfold  2  score: 98.79%\n",
      "[21:46:50] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:46:50] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "VotingClassifier(estimators=[('gb',\n",
      "                              GradientBoostingClassifier(max_depth=12,\n",
      "                                                         min_samples_split=12,\n",
      "                                                         n_estimators=5,\n",
      "                                                         random_state=3)),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(max_depth=12,\n",
      "                                                     n_estimators=10,\n",
      "                                                     random_state=3)),\n",
      "                             ('xgb',\n",
      "                              XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "                                            colsample_bylevel=1,\n",
      "                                            colsample_bynode=1,\n",
      "                                            colsample_bytree=1,\n",
      "                                            enable_categorical=False, gamma=0,\n",
      "                                            gpu...\n",
      "                                            interaction_constraints='',\n",
      "                                            learning_rate=0.300000012,\n",
      "                                            max_delta_step=0, max_depth=12,\n",
      "                                            max_leaf_nodes=None,\n",
      "                                            min_child_weight=1,\n",
      "                                            min_samples_split=1, missing=nan,\n",
      "                                            monotone_constraints='()',\n",
      "                                            n_estimators=10, n_jobs=8,\n",
      "                                            num_parallel_tree=1,\n",
      "                                            predictor='auto', random_state=3,\n",
      "                                            reg_alpha=0, reg_lambda=1,\n",
      "                                            scale_pos_weight=1, subsample=1,\n",
      "                                            tree_method='exact',\n",
      "                                            validate_parameters=1,\n",
      "                                            verbosity=None))])\n",
      "kfold  2  score: 93.29%\n",
      "kfold  2  score: 98.68%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21:46:50] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:46:50] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "VotingClassifier(estimators=[('gb',\n",
      "                              GradientBoostingClassifier(max_depth=12,\n",
      "                                                         min_samples_split=12,\n",
      "                                                         n_estimators=5,\n",
      "                                                         random_state=3)),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(max_depth=12,\n",
      "                                                     n_estimators=10,\n",
      "                                                     random_state=3)),\n",
      "                             ('xgb',\n",
      "                              XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "                                            colsample_bylevel=1,\n",
      "                                            colsample_bynode=1,\n",
      "                                            colsample_bytree=1,\n",
      "                                            enable_categorical=False, gamma=0,\n",
      "                                            gpu...\n",
      "                                            interaction_constraints='',\n",
      "                                            learning_rate=0.300000012,\n",
      "                                            max_delta_step=0, max_depth=12,\n",
      "                                            max_leaf_nodes=None,\n",
      "                                            min_child_weight=1,\n",
      "                                            min_samples_split=1, missing=nan,\n",
      "                                            monotone_constraints='()',\n",
      "                                            n_estimators=10, n_jobs=8,\n",
      "                                            num_parallel_tree=1,\n",
      "                                            predictor='auto', random_state=3,\n",
      "                                            reg_alpha=0, reg_lambda=1,\n",
      "                                            scale_pos_weight=1, subsample=1,\n",
      "                                            tree_method='exact',\n",
      "                                            validate_parameters=1,\n",
      "                                            verbosity=None))])\n",
      "kfold  2  score: 94.08%\n",
      "kfold  2  score: 98.95%\n",
      "[21:46:51] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:46:51] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "VotingClassifier(estimators=[('gb',\n",
      "                              GradientBoostingClassifier(max_depth=12,\n",
      "                                                         min_samples_split=12,\n",
      "                                                         n_estimators=5,\n",
      "                                                         random_state=3)),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(max_depth=12,\n",
      "                                                     n_estimators=10,\n",
      "                                                     random_state=3)),\n",
      "                             ('xgb',\n",
      "                              XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "                                            colsample_bylevel=1,\n",
      "                                            colsample_bynode=1,\n",
      "                                            colsample_bytree=1,\n",
      "                                            enable_categorical=False, gamma=0,\n",
      "                                            gpu...\n",
      "                                            interaction_constraints='',\n",
      "                                            learning_rate=0.300000012,\n",
      "                                            max_delta_step=0, max_depth=12,\n",
      "                                            max_leaf_nodes=None,\n",
      "                                            min_child_weight=1,\n",
      "                                            min_samples_split=1, missing=nan,\n",
      "                                            monotone_constraints='()',\n",
      "                                            n_estimators=10, n_jobs=8,\n",
      "                                            num_parallel_tree=1,\n",
      "                                            predictor='auto', random_state=3,\n",
      "                                            reg_alpha=0, reg_lambda=1,\n",
      "                                            scale_pos_weight=1, subsample=1,\n",
      "                                            tree_method='exact',\n",
      "                                            validate_parameters=1,\n",
      "                                            verbosity=None))])\n",
      "kfold  2  score: 94.28%\n",
      "kfold  2  score: 99.21%\n",
      "[21:46:51] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:46:51] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "VotingClassifier(estimators=[('gb',\n",
      "                              GradientBoostingClassifier(max_depth=12,\n",
      "                                                         min_samples_split=12,\n",
      "                                                         n_estimators=5,\n",
      "                                                         random_state=3)),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(max_depth=12,\n",
      "                                                     n_estimators=10,\n",
      "                                                     random_state=3)),\n",
      "                             ('xgb',\n",
      "                              XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "                                            colsample_bylevel=1,\n",
      "                                            colsample_bynode=1,\n",
      "                                            colsample_bytree=1,\n",
      "                                            enable_categorical=False, gamma=0,\n",
      "                                            gpu...\n",
      "                                            interaction_constraints='',\n",
      "                                            learning_rate=0.300000012,\n",
      "                                            max_delta_step=0, max_depth=12,\n",
      "                                            max_leaf_nodes=None,\n",
      "                                            min_child_weight=1,\n",
      "                                            min_samples_split=1, missing=nan,\n",
      "                                            monotone_constraints='()',\n",
      "                                            n_estimators=10, n_jobs=8,\n",
      "                                            num_parallel_tree=1,\n",
      "                                            predictor='auto', random_state=3,\n",
      "                                            reg_alpha=0, reg_lambda=1,\n",
      "                                            scale_pos_weight=1, subsample=1,\n",
      "                                            tree_method='exact',\n",
      "                                            validate_parameters=1,\n",
      "                                            verbosity=None))])\n",
      "kfold  2  score: 92.49%\n",
      "kfold  2  score: 98.84%\n",
      "Average score over k-fold 93.77% (+/-1.17%) [Ensemble Majority]\n",
      "./Files_Majority_vote/128_features/filter3.csv\n",
      "n_estimators = 100 \n",
      "\n",
      "10-fold cross validation for file :  3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GradientBoostingClassifier(max_depth=12, min_samples_split=12, n_estimators=5,\n",
      "                           random_state=3)\n",
      "kfold  3  score: 90.41%\n",
      "kfold  3  score: 93.10%\n",
      "GradientBoostingClassifier(max_depth=12, min_samples_split=12, n_estimators=5,\n",
      "                           random_state=3)\n",
      "kfold  3  score: 88.38%\n",
      "kfold  3  score: 92.49%\n",
      "GradientBoostingClassifier(max_depth=12, min_samples_split=12, n_estimators=5,\n",
      "                           random_state=3)\n",
      "kfold  3  score: 90.77%\n",
      "kfold  3  score: 92.57%\n",
      "GradientBoostingClassifier(max_depth=12, min_samples_split=12, n_estimators=5,\n",
      "                           random_state=3)\n",
      "kfold  3  score: 88.19%\n",
      "kfold  3  score: 92.55%\n",
      "GradientBoostingClassifier(max_depth=12, min_samples_split=12, n_estimators=5,\n",
      "                           random_state=3)\n",
      "kfold  3  score: 89.11%\n",
      "kfold  3  score: 92.45%\n",
      "GradientBoostingClassifier(max_depth=12, min_samples_split=12, n_estimators=5,\n",
      "                           random_state=3)\n",
      "kfold  3  score: 91.68%\n",
      "kfold  3  score: 92.98%\n",
      "GradientBoostingClassifier(max_depth=12, min_samples_split=12, n_estimators=5,\n",
      "                           random_state=3)\n",
      "kfold  3  score: 88.17%\n",
      "kfold  3  score: 93.00%\n",
      "GradientBoostingClassifier(max_depth=12, min_samples_split=12, n_estimators=5,\n",
      "                           random_state=3)\n",
      "kfold  3  score: 89.65%\n",
      "kfold  3  score: 93.07%\n",
      "GradientBoostingClassifier(max_depth=12, min_samples_split=12, n_estimators=5,\n",
      "                           random_state=3)\n",
      "kfold  3  score: 88.35%\n",
      "kfold  3  score: 92.16%\n",
      "GradientBoostingClassifier(max_depth=12, min_samples_split=12, n_estimators=5,\n",
      "                           random_state=3)\n",
      "kfold  3  score: 89.46%\n",
      "kfold  3  score: 91.42%\n",
      "Average score over k-fold 89.42% (+/-1.16%) [Gradient Boosting]\n",
      "RandomForestClassifier(max_depth=12, n_estimators=10, random_state=3)\n",
      "kfold  3  score: 92.62%\n",
      "kfold  3  score: 95.88%\n",
      "RandomForestClassifier(max_depth=12, n_estimators=10, random_state=3)\n",
      "kfold  3  score: 91.51%\n",
      "kfold  3  score: 96.88%\n",
      "RandomForestClassifier(max_depth=12, n_estimators=10, random_state=3)\n",
      "kfold  3  score: 93.54%\n",
      "kfold  3  score: 96.53%\n",
      "RandomForestClassifier(max_depth=12, n_estimators=10, random_state=3)\n",
      "kfold  3  score: 89.85%\n",
      "kfold  3  score: 95.42%\n",
      "RandomForestClassifier(max_depth=12, n_estimators=10, random_state=3)\n",
      "kfold  3  score: 94.28%\n",
      "kfold  3  score: 96.63%\n",
      "RandomForestClassifier(max_depth=12, n_estimators=10, random_state=3)\n",
      "kfold  3  score: 93.90%\n",
      "kfold  3  score: 96.37%\n",
      "RandomForestClassifier(max_depth=12, n_estimators=10, random_state=3)\n",
      "kfold  3  score: 92.05%\n",
      "kfold  3  score: 96.86%\n",
      "RandomForestClassifier(max_depth=12, n_estimators=10, random_state=3)\n",
      "kfold  3  score: 93.16%\n",
      "kfold  3  score: 96.20%\n",
      "RandomForestClassifier(max_depth=12, n_estimators=10, random_state=3)\n",
      "kfold  3  score: 93.16%\n",
      "kfold  3  score: 96.76%\n",
      "RandomForestClassifier(max_depth=12, n_estimators=10, random_state=3)\n",
      "kfold  3  score: 93.53%\n",
      "kfold  3  score: 95.84%\n",
      "Average score over k-fold 92.76% (+/-1.25%) [Random Forest]\n",
      "[21:46:55] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:46:55] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
      "              gamma=0, gpu_id=-1, importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.300000012,\n",
      "              max_delta_step=0, max_depth=12, max_leaf_nodes=None,\n",
      "              min_child_weight=1, min_samples_split=1, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=10, n_jobs=8,\n",
      "              num_parallel_tree=1, predictor='auto', random_state=3,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)\n",
      "kfold  3  score: 95.20%\n",
      "kfold  3  score: 99.49%\n",
      "[21:46:55] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:46:55] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
      "              gamma=0, gpu_id=-1, importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.300000012,\n",
      "              max_delta_step=0, max_depth=12, max_leaf_nodes=None,\n",
      "              min_child_weight=1, min_samples_split=1, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=10, n_jobs=8,\n",
      "              num_parallel_tree=1, predictor='auto', random_state=3,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)\n",
      "kfold  3  score: 95.39%\n",
      "kfold  3  score: 99.55%\n",
      "[21:46:55] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:46:55] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
      "              gamma=0, gpu_id=-1, importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.300000012,\n",
      "              max_delta_step=0, max_depth=12, max_leaf_nodes=None,\n",
      "              min_child_weight=1, min_samples_split=1, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=10, n_jobs=8,\n",
      "              num_parallel_tree=1, predictor='auto', random_state=3,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)\n",
      "kfold  3  score: 95.39%\n",
      "kfold  3  score: 99.51%\n",
      "[21:46:55] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:46:55] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
      "              gamma=0, gpu_id=-1, importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.300000012,\n",
      "              max_delta_step=0, max_depth=12, max_leaf_nodes=None,\n",
      "              min_child_weight=1, min_samples_split=1, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=10, n_jobs=8,\n",
      "              num_parallel_tree=1, predictor='auto', random_state=3,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)\n",
      "kfold  3  score: 92.99%\n",
      "kfold  3  score: 99.22%\n",
      "[21:46:55] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:46:55] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
      "              gamma=0, gpu_id=-1, importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.300000012,\n",
      "              max_delta_step=0, max_depth=12, max_leaf_nodes=None,\n",
      "              min_child_weight=1, min_samples_split=1, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=10, n_jobs=8,\n",
      "              num_parallel_tree=1, predictor='auto', random_state=3,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)\n",
      "kfold  3  score: 95.76%\n",
      "kfold  3  score: 99.53%\n",
      "[21:46:56] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:46:56] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
      "              gamma=0, gpu_id=-1, importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.300000012,\n",
      "              max_delta_step=0, max_depth=12, max_leaf_nodes=None,\n",
      "              min_child_weight=1, min_samples_split=1, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=10, n_jobs=8,\n",
      "              num_parallel_tree=1, predictor='auto', random_state=3,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)\n",
      "kfold  3  score: 96.12%\n",
      "kfold  3  score: 99.38%\n",
      "[21:46:56] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:46:56] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
      "              gamma=0, gpu_id=-1, importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.300000012,\n",
      "              max_delta_step=0, max_depth=12, max_leaf_nodes=None,\n",
      "              min_child_weight=1, min_samples_split=1, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=10, n_jobs=8,\n",
      "              num_parallel_tree=1, predictor='auto', random_state=3,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)\n",
      "kfold  3  score: 94.45%\n",
      "kfold  3  score: 99.55%\n",
      "[21:46:56] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:46:56] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
      "              gamma=0, gpu_id=-1, importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.300000012,\n",
      "              max_delta_step=0, max_depth=12, max_leaf_nodes=None,\n",
      "              min_child_weight=1, min_samples_split=1, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=10, n_jobs=8,\n",
      "              num_parallel_tree=1, predictor='auto', random_state=3,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)\n",
      "kfold  3  score: 95.38%\n",
      "kfold  3  score: 99.41%\n",
      "[21:46:56] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:46:56] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
      "              gamma=0, gpu_id=-1, importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.300000012,\n",
      "              max_delta_step=0, max_depth=12, max_leaf_nodes=None,\n",
      "              min_child_weight=1, min_samples_split=1, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=10, n_jobs=8,\n",
      "              num_parallel_tree=1, predictor='auto', random_state=3,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)\n",
      "kfold  3  score: 95.19%\n",
      "kfold  3  score: 99.06%\n",
      "[21:46:56] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:46:56] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
      "              gamma=0, gpu_id=-1, importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.300000012,\n",
      "              max_delta_step=0, max_depth=12, max_leaf_nodes=None,\n",
      "              min_child_weight=1, min_samples_split=1, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=10, n_jobs=8,\n",
      "              num_parallel_tree=1, predictor='auto', random_state=3,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)\n",
      "kfold  3  score: 96.30%\n",
      "kfold  3  score: 98.87%\n",
      "Average score over k-fold 95.22% (+/-0.89%) [XGBClassifier]\n",
      "[21:46:57] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:46:57] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "VotingClassifier(estimators=[('gb',\n",
      "                              GradientBoostingClassifier(max_depth=12,\n",
      "                                                         min_samples_split=12,\n",
      "                                                         n_estimators=5,\n",
      "                                                         random_state=3)),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(max_depth=12,\n",
      "                                                     n_estimators=10,\n",
      "                                                     random_state=3)),\n",
      "                             ('xgb',\n",
      "                              XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "                                            colsample_bylevel=1,\n",
      "                                            colsample_bynode=1,\n",
      "                                            colsample_bytree=1,\n",
      "                                            enable_categorical=False, gamma=0,\n",
      "                                            gpu...\n",
      "                                            interaction_constraints='',\n",
      "                                            learning_rate=0.300000012,\n",
      "                                            max_delta_step=0, max_depth=12,\n",
      "                                            max_leaf_nodes=None,\n",
      "                                            min_child_weight=1,\n",
      "                                            min_samples_split=1, missing=nan,\n",
      "                                            monotone_constraints='()',\n",
      "                                            n_estimators=10, n_jobs=8,\n",
      "                                            num_parallel_tree=1,\n",
      "                                            predictor='auto', random_state=3,\n",
      "                                            reg_alpha=0, reg_lambda=1,\n",
      "                                            scale_pos_weight=1, subsample=1,\n",
      "                                            tree_method='exact',\n",
      "                                            validate_parameters=1,\n",
      "                                            verbosity=None))])\n",
      "kfold  3  score: 93.17%\n",
      "kfold  3  score: 96.45%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21:46:57] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:46:57] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "VotingClassifier(estimators=[('gb',\n",
      "                              GradientBoostingClassifier(max_depth=12,\n",
      "                                                         min_samples_split=12,\n",
      "                                                         n_estimators=5,\n",
      "                                                         random_state=3)),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(max_depth=12,\n",
      "                                                     n_estimators=10,\n",
      "                                                     random_state=3)),\n",
      "                             ('xgb',\n",
      "                              XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "                                            colsample_bylevel=1,\n",
      "                                            colsample_bynode=1,\n",
      "                                            colsample_bytree=1,\n",
      "                                            enable_categorical=False, gamma=0,\n",
      "                                            gpu...\n",
      "                                            interaction_constraints='',\n",
      "                                            learning_rate=0.300000012,\n",
      "                                            max_delta_step=0, max_depth=12,\n",
      "                                            max_leaf_nodes=None,\n",
      "                                            min_child_weight=1,\n",
      "                                            min_samples_split=1, missing=nan,\n",
      "                                            monotone_constraints='()',\n",
      "                                            n_estimators=10, n_jobs=8,\n",
      "                                            num_parallel_tree=1,\n",
      "                                            predictor='auto', random_state=3,\n",
      "                                            reg_alpha=0, reg_lambda=1,\n",
      "                                            scale_pos_weight=1, subsample=1,\n",
      "                                            tree_method='exact',\n",
      "                                            validate_parameters=1,\n",
      "                                            verbosity=None))])\n",
      "kfold  3  score: 92.99%\n",
      "kfold  3  score: 97.25%\n",
      "[21:46:58] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:46:58] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "VotingClassifier(estimators=[('gb',\n",
      "                              GradientBoostingClassifier(max_depth=12,\n",
      "                                                         min_samples_split=12,\n",
      "                                                         n_estimators=5,\n",
      "                                                         random_state=3)),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(max_depth=12,\n",
      "                                                     n_estimators=10,\n",
      "                                                     random_state=3)),\n",
      "                             ('xgb',\n",
      "                              XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "                                            colsample_bylevel=1,\n",
      "                                            colsample_bynode=1,\n",
      "                                            colsample_bytree=1,\n",
      "                                            enable_categorical=False, gamma=0,\n",
      "                                            gpu...\n",
      "                                            interaction_constraints='',\n",
      "                                            learning_rate=0.300000012,\n",
      "                                            max_delta_step=0, max_depth=12,\n",
      "                                            max_leaf_nodes=None,\n",
      "                                            min_child_weight=1,\n",
      "                                            min_samples_split=1, missing=nan,\n",
      "                                            monotone_constraints='()',\n",
      "                                            n_estimators=10, n_jobs=8,\n",
      "                                            num_parallel_tree=1,\n",
      "                                            predictor='auto', random_state=3,\n",
      "                                            reg_alpha=0, reg_lambda=1,\n",
      "                                            scale_pos_weight=1, subsample=1,\n",
      "                                            tree_method='exact',\n",
      "                                            validate_parameters=1,\n",
      "                                            verbosity=None))])\n",
      "kfold  3  score: 94.46%\n",
      "kfold  3  score: 96.96%\n",
      "[21:46:58] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:46:58] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "VotingClassifier(estimators=[('gb',\n",
      "                              GradientBoostingClassifier(max_depth=12,\n",
      "                                                         min_samples_split=12,\n",
      "                                                         n_estimators=5,\n",
      "                                                         random_state=3)),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(max_depth=12,\n",
      "                                                     n_estimators=10,\n",
      "                                                     random_state=3)),\n",
      "                             ('xgb',\n",
      "                              XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "                                            colsample_bylevel=1,\n",
      "                                            colsample_bynode=1,\n",
      "                                            colsample_bytree=1,\n",
      "                                            enable_categorical=False, gamma=0,\n",
      "                                            gpu...\n",
      "                                            interaction_constraints='',\n",
      "                                            learning_rate=0.300000012,\n",
      "                                            max_delta_step=0, max_depth=12,\n",
      "                                            max_leaf_nodes=None,\n",
      "                                            min_child_weight=1,\n",
      "                                            min_samples_split=1, missing=nan,\n",
      "                                            monotone_constraints='()',\n",
      "                                            n_estimators=10, n_jobs=8,\n",
      "                                            num_parallel_tree=1,\n",
      "                                            predictor='auto', random_state=3,\n",
      "                                            reg_alpha=0, reg_lambda=1,\n",
      "                                            scale_pos_weight=1, subsample=1,\n",
      "                                            tree_method='exact',\n",
      "                                            validate_parameters=1,\n",
      "                                            verbosity=None))])\n",
      "kfold  3  score: 90.96%\n",
      "kfold  3  score: 96.08%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21:46:59] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:46:59] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "VotingClassifier(estimators=[('gb',\n",
      "                              GradientBoostingClassifier(max_depth=12,\n",
      "                                                         min_samples_split=12,\n",
      "                                                         n_estimators=5,\n",
      "                                                         random_state=3)),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(max_depth=12,\n",
      "                                                     n_estimators=10,\n",
      "                                                     random_state=3)),\n",
      "                             ('xgb',\n",
      "                              XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "                                            colsample_bylevel=1,\n",
      "                                            colsample_bynode=1,\n",
      "                                            colsample_bytree=1,\n",
      "                                            enable_categorical=False, gamma=0,\n",
      "                                            gpu...\n",
      "                                            interaction_constraints='',\n",
      "                                            learning_rate=0.300000012,\n",
      "                                            max_delta_step=0, max_depth=12,\n",
      "                                            max_leaf_nodes=None,\n",
      "                                            min_child_weight=1,\n",
      "                                            min_samples_split=1, missing=nan,\n",
      "                                            monotone_constraints='()',\n",
      "                                            n_estimators=10, n_jobs=8,\n",
      "                                            num_parallel_tree=1,\n",
      "                                            predictor='auto', random_state=3,\n",
      "                                            reg_alpha=0, reg_lambda=1,\n",
      "                                            scale_pos_weight=1, subsample=1,\n",
      "                                            tree_method='exact',\n",
      "                                            validate_parameters=1,\n",
      "                                            verbosity=None))])\n",
      "kfold  3  score: 93.73%\n",
      "kfold  3  score: 97.11%\n",
      "[21:46:59] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:46:59] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "VotingClassifier(estimators=[('gb',\n",
      "                              GradientBoostingClassifier(max_depth=12,\n",
      "                                                         min_samples_split=12,\n",
      "                                                         n_estimators=5,\n",
      "                                                         random_state=3)),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(max_depth=12,\n",
      "                                                     n_estimators=10,\n",
      "                                                     random_state=3)),\n",
      "                             ('xgb',\n",
      "                              XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "                                            colsample_bylevel=1,\n",
      "                                            colsample_bynode=1,\n",
      "                                            colsample_bytree=1,\n",
      "                                            enable_categorical=False, gamma=0,\n",
      "                                            gpu...\n",
      "                                            interaction_constraints='',\n",
      "                                            learning_rate=0.300000012,\n",
      "                                            max_delta_step=0, max_depth=12,\n",
      "                                            max_leaf_nodes=None,\n",
      "                                            min_child_weight=1,\n",
      "                                            min_samples_split=1, missing=nan,\n",
      "                                            monotone_constraints='()',\n",
      "                                            n_estimators=10, n_jobs=8,\n",
      "                                            num_parallel_tree=1,\n",
      "                                            predictor='auto', random_state=3,\n",
      "                                            reg_alpha=0, reg_lambda=1,\n",
      "                                            scale_pos_weight=1, subsample=1,\n",
      "                                            tree_method='exact',\n",
      "                                            validate_parameters=1,\n",
      "                                            verbosity=None))])\n",
      "kfold  3  score: 93.90%\n",
      "kfold  3  score: 96.53%\n",
      "[21:47:00] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:47:00] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "VotingClassifier(estimators=[('gb',\n",
      "                              GradientBoostingClassifier(max_depth=12,\n",
      "                                                         min_samples_split=12,\n",
      "                                                         n_estimators=5,\n",
      "                                                         random_state=3)),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(max_depth=12,\n",
      "                                                     n_estimators=10,\n",
      "                                                     random_state=3)),\n",
      "                             ('xgb',\n",
      "                              XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "                                            colsample_bylevel=1,\n",
      "                                            colsample_bynode=1,\n",
      "                                            colsample_bytree=1,\n",
      "                                            enable_categorical=False, gamma=0,\n",
      "                                            gpu...\n",
      "                                            interaction_constraints='',\n",
      "                                            learning_rate=0.300000012,\n",
      "                                            max_delta_step=0, max_depth=12,\n",
      "                                            max_leaf_nodes=None,\n",
      "                                            min_child_weight=1,\n",
      "                                            min_samples_split=1, missing=nan,\n",
      "                                            monotone_constraints='()',\n",
      "                                            n_estimators=10, n_jobs=8,\n",
      "                                            num_parallel_tree=1,\n",
      "                                            predictor='auto', random_state=3,\n",
      "                                            reg_alpha=0, reg_lambda=1,\n",
      "                                            scale_pos_weight=1, subsample=1,\n",
      "                                            tree_method='exact',\n",
      "                                            validate_parameters=1,\n",
      "                                            verbosity=None))])\n",
      "kfold  3  score: 92.79%\n",
      "kfold  3  score: 97.39%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21:47:00] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:47:00] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "VotingClassifier(estimators=[('gb',\n",
      "                              GradientBoostingClassifier(max_depth=12,\n",
      "                                                         min_samples_split=12,\n",
      "                                                         n_estimators=5,\n",
      "                                                         random_state=3)),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(max_depth=12,\n",
      "                                                     n_estimators=10,\n",
      "                                                     random_state=3)),\n",
      "                             ('xgb',\n",
      "                              XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "                                            colsample_bylevel=1,\n",
      "                                            colsample_bynode=1,\n",
      "                                            colsample_bytree=1,\n",
      "                                            enable_categorical=False, gamma=0,\n",
      "                                            gpu...\n",
      "                                            interaction_constraints='',\n",
      "                                            learning_rate=0.300000012,\n",
      "                                            max_delta_step=0, max_depth=12,\n",
      "                                            max_leaf_nodes=None,\n",
      "                                            min_child_weight=1,\n",
      "                                            min_samples_split=1, missing=nan,\n",
      "                                            monotone_constraints='()',\n",
      "                                            n_estimators=10, n_jobs=8,\n",
      "                                            num_parallel_tree=1,\n",
      "                                            predictor='auto', random_state=3,\n",
      "                                            reg_alpha=0, reg_lambda=1,\n",
      "                                            scale_pos_weight=1, subsample=1,\n",
      "                                            tree_method='exact',\n",
      "                                            validate_parameters=1,\n",
      "                                            verbosity=None))])\n",
      "kfold  3  score: 93.53%\n",
      "kfold  3  score: 97.17%\n",
      "[21:47:01] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:47:01] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "VotingClassifier(estimators=[('gb',\n",
      "                              GradientBoostingClassifier(max_depth=12,\n",
      "                                                         min_samples_split=12,\n",
      "                                                         n_estimators=5,\n",
      "                                                         random_state=3)),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(max_depth=12,\n",
      "                                                     n_estimators=10,\n",
      "                                                     random_state=3)),\n",
      "                             ('xgb',\n",
      "                              XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "                                            colsample_bylevel=1,\n",
      "                                            colsample_bynode=1,\n",
      "                                            colsample_bytree=1,\n",
      "                                            enable_categorical=False, gamma=0,\n",
      "                                            gpu...\n",
      "                                            interaction_constraints='',\n",
      "                                            learning_rate=0.300000012,\n",
      "                                            max_delta_step=0, max_depth=12,\n",
      "                                            max_leaf_nodes=None,\n",
      "                                            min_child_weight=1,\n",
      "                                            min_samples_split=1, missing=nan,\n",
      "                                            monotone_constraints='()',\n",
      "                                            n_estimators=10, n_jobs=8,\n",
      "                                            num_parallel_tree=1,\n",
      "                                            predictor='auto', random_state=3,\n",
      "                                            reg_alpha=0, reg_lambda=1,\n",
      "                                            scale_pos_weight=1, subsample=1,\n",
      "                                            tree_method='exact',\n",
      "                                            validate_parameters=1,\n",
      "                                            verbosity=None))])\n",
      "kfold  3  score: 93.90%\n",
      "kfold  3  score: 97.56%\n",
      "[21:47:01] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:47:01] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "VotingClassifier(estimators=[('gb',\n",
      "                              GradientBoostingClassifier(max_depth=12,\n",
      "                                                         min_samples_split=12,\n",
      "                                                         n_estimators=5,\n",
      "                                                         random_state=3)),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(max_depth=12,\n",
      "                                                     n_estimators=10,\n",
      "                                                     random_state=3)),\n",
      "                             ('xgb',\n",
      "                              XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "                                            colsample_bylevel=1,\n",
      "                                            colsample_bynode=1,\n",
      "                                            colsample_bytree=1,\n",
      "                                            enable_categorical=False, gamma=0,\n",
      "                                            gpu...\n",
      "                                            interaction_constraints='',\n",
      "                                            learning_rate=0.300000012,\n",
      "                                            max_delta_step=0, max_depth=12,\n",
      "                                            max_leaf_nodes=None,\n",
      "                                            min_child_weight=1,\n",
      "                                            min_samples_split=1, missing=nan,\n",
      "                                            monotone_constraints='()',\n",
      "                                            n_estimators=10, n_jobs=8,\n",
      "                                            num_parallel_tree=1,\n",
      "                                            predictor='auto', random_state=3,\n",
      "                                            reg_alpha=0, reg_lambda=1,\n",
      "                                            scale_pos_weight=1, subsample=1,\n",
      "                                            tree_method='exact',\n",
      "                                            validate_parameters=1,\n",
      "                                            verbosity=None))])\n",
      "kfold  3  score: 94.64%\n",
      "kfold  3  score: 96.29%\n",
      "Average score over k-fold 93.41% (+/-0.99%) [Ensemble Majority]\n",
      "./Files_Majority_vote/128_features/filter4.csv\n",
      "n_estimators = 100 \n",
      "\n",
      "10-fold cross validation for file :  4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GradientBoostingClassifier(max_depth=12, min_samples_split=12, n_estimators=5,\n",
      "                           random_state=3)\n",
      "kfold  4  score: 85.80%\n",
      "kfold  4  score: 92.03%\n",
      "GradientBoostingClassifier(max_depth=12, min_samples_split=12, n_estimators=5,\n",
      "                           random_state=3)\n",
      "kfold  4  score: 94.24%\n",
      "kfold  4  score: 97.35%\n",
      "GradientBoostingClassifier(max_depth=12, min_samples_split=12, n_estimators=5,\n",
      "                           random_state=3)\n",
      "kfold  4  score: 91.73%\n",
      "kfold  4  score: 97.69%\n",
      "GradientBoostingClassifier(max_depth=12, min_samples_split=12, n_estimators=5,\n",
      "                           random_state=3)\n",
      "kfold  4  score: 93.08%\n",
      "kfold  4  score: 97.27%\n",
      "GradientBoostingClassifier(max_depth=12, min_samples_split=12, n_estimators=5,\n",
      "                           random_state=3)\n",
      "kfold  4  score: 93.46%\n",
      "kfold  4  score: 97.63%\n",
      "GradientBoostingClassifier(max_depth=12, min_samples_split=12, n_estimators=5,\n",
      "                           random_state=3)\n",
      "kfold  4  score: 91.92%\n",
      "kfold  4  score: 97.69%\n",
      "GradientBoostingClassifier(max_depth=12, min_samples_split=12, n_estimators=5,\n",
      "                           random_state=3)\n",
      "kfold  4  score: 91.54%\n",
      "kfold  4  score: 96.90%\n",
      "GradientBoostingClassifier(max_depth=12, min_samples_split=12, n_estimators=5,\n",
      "                           random_state=3)\n",
      "kfold  4  score: 91.54%\n",
      "kfold  4  score: 97.63%\n",
      "GradientBoostingClassifier(max_depth=12, min_samples_split=12, n_estimators=5,\n",
      "                           random_state=3)\n",
      "kfold  4  score: 95.00%\n",
      "kfold  4  score: 97.48%\n",
      "GradientBoostingClassifier(max_depth=12, min_samples_split=12, n_estimators=5,\n",
      "                           random_state=3)\n",
      "kfold  4  score: 91.35%\n",
      "kfold  4  score: 97.48%\n",
      "Average score over k-fold 91.97% (+/-2.38%) [Gradient Boosting]\n",
      "RandomForestClassifier(max_depth=12, n_estimators=10, random_state=3)\n",
      "kfold  4  score: 93.09%\n",
      "kfold  4  score: 99.19%\n",
      "RandomForestClassifier(max_depth=12, n_estimators=10, random_state=3)\n",
      "kfold  4  score: 96.35%\n",
      "kfold  4  score: 99.06%\n",
      "RandomForestClassifier(max_depth=12, n_estimators=10, random_state=3)\n",
      "kfold  4  score: 92.31%\n",
      "kfold  4  score: 99.17%\n",
      "RandomForestClassifier(max_depth=12, n_estimators=10, random_state=3)\n",
      "kfold  4  score: 94.62%\n",
      "kfold  4  score: 98.55%\n",
      "RandomForestClassifier(max_depth=12, n_estimators=10, random_state=3)\n",
      "kfold  4  score: 93.08%\n",
      "kfold  4  score: 99.10%\n",
      "RandomForestClassifier(max_depth=12, n_estimators=10, random_state=3)\n",
      "kfold  4  score: 93.08%\n",
      "kfold  4  score: 99.27%\n",
      "RandomForestClassifier(max_depth=12, n_estimators=10, random_state=3)\n",
      "kfold  4  score: 95.19%\n",
      "kfold  4  score: 99.02%\n",
      "RandomForestClassifier(max_depth=12, n_estimators=10, random_state=3)\n",
      "kfold  4  score: 94.62%\n",
      "kfold  4  score: 98.95%\n",
      "RandomForestClassifier(max_depth=12, n_estimators=10, random_state=3)\n",
      "kfold  4  score: 95.58%\n",
      "kfold  4  score: 98.72%\n",
      "RandomForestClassifier(max_depth=12, n_estimators=10, random_state=3)\n",
      "kfold  4  score: 93.08%\n",
      "kfold  4  score: 98.78%\n",
      "Average score over k-fold 94.10% (+/-1.28%) [Random Forest]\n",
      "[21:47:05] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:47:05] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
      "              gamma=0, gpu_id=-1, importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.300000012,\n",
      "              max_delta_step=0, max_depth=12, max_leaf_nodes=None,\n",
      "              min_child_weight=1, min_samples_split=1, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=10, n_jobs=8,\n",
      "              num_parallel_tree=1, predictor='auto', random_state=3,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)\n",
      "kfold  4  score: 94.05%\n",
      "kfold  4  score: 99.36%\n",
      "[21:47:05] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:47:05] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
      "              gamma=0, gpu_id=-1, importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.300000012,\n",
      "              max_delta_step=0, max_depth=12, max_leaf_nodes=None,\n",
      "              min_child_weight=1, min_samples_split=1, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=10, n_jobs=8,\n",
      "              num_parallel_tree=1, predictor='auto', random_state=3,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)\n",
      "kfold  4  score: 96.35%\n",
      "kfold  4  score: 99.47%\n",
      "[21:47:05] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:47:05] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
      "              gamma=0, gpu_id=-1, importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.300000012,\n",
      "              max_delta_step=0, max_depth=12, max_leaf_nodes=None,\n",
      "              min_child_weight=1, min_samples_split=1, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=10, n_jobs=8,\n",
      "              num_parallel_tree=1, predictor='auto', random_state=3,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)\n",
      "kfold  4  score: 93.65%\n",
      "kfold  4  score: 99.68%\n",
      "[21:47:05] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:47:05] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
      "              gamma=0, gpu_id=-1, importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.300000012,\n",
      "              max_delta_step=0, max_depth=12, max_leaf_nodes=None,\n",
      "              min_child_weight=1, min_samples_split=1, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=10, n_jobs=8,\n",
      "              num_parallel_tree=1, predictor='auto', random_state=3,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)\n",
      "kfold  4  score: 96.92%\n",
      "kfold  4  score: 99.59%\n",
      "[21:47:06] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:47:06] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
      "              gamma=0, gpu_id=-1, importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.300000012,\n",
      "              max_delta_step=0, max_depth=12, max_leaf_nodes=None,\n",
      "              min_child_weight=1, min_samples_split=1, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=10, n_jobs=8,\n",
      "              num_parallel_tree=1, predictor='auto', random_state=3,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)\n",
      "kfold  4  score: 95.00%\n",
      "kfold  4  score: 99.74%\n",
      "[21:47:06] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:47:06] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
      "              gamma=0, gpu_id=-1, importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.300000012,\n",
      "              max_delta_step=0, max_depth=12, max_leaf_nodes=None,\n",
      "              min_child_weight=1, min_samples_split=1, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=10, n_jobs=8,\n",
      "              num_parallel_tree=1, predictor='auto', random_state=3,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)\n",
      "kfold  4  score: 93.08%\n",
      "kfold  4  score: 99.59%\n",
      "[21:47:06] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:47:06] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
      "              gamma=0, gpu_id=-1, importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.300000012,\n",
      "              max_delta_step=0, max_depth=12, max_leaf_nodes=None,\n",
      "              min_child_weight=1, min_samples_split=1, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=10, n_jobs=8,\n",
      "              num_parallel_tree=1, predictor='auto', random_state=3,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)\n",
      "kfold  4  score: 94.62%\n",
      "kfold  4  score: 99.66%\n",
      "[21:47:06] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:47:06] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
      "              gamma=0, gpu_id=-1, importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.300000012,\n",
      "              max_delta_step=0, max_depth=12, max_leaf_nodes=None,\n",
      "              min_child_weight=1, min_samples_split=1, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=10, n_jobs=8,\n",
      "              num_parallel_tree=1, predictor='auto', random_state=3,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)\n",
      "kfold  4  score: 95.19%\n",
      "kfold  4  score: 99.66%\n",
      "[21:47:06] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:47:06] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
      "              gamma=0, gpu_id=-1, importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.300000012,\n",
      "              max_delta_step=0, max_depth=12, max_leaf_nodes=None,\n",
      "              min_child_weight=1, min_samples_split=1, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=10, n_jobs=8,\n",
      "              num_parallel_tree=1, predictor='auto', random_state=3,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)\n",
      "kfold  4  score: 95.77%\n",
      "kfold  4  score: 99.64%\n",
      "[21:47:06] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:47:06] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
      "              gamma=0, gpu_id=-1, importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.300000012,\n",
      "              max_delta_step=0, max_depth=12, max_leaf_nodes=None,\n",
      "              min_child_weight=1, min_samples_split=1, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=10, n_jobs=8,\n",
      "              num_parallel_tree=1, predictor='auto', random_state=3,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)\n",
      "kfold  4  score: 94.42%\n",
      "kfold  4  score: 99.08%\n",
      "Average score over k-fold 94.91% (+/-1.14%) [XGBClassifier]\n",
      "[21:47:07] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:47:07] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "VotingClassifier(estimators=[('gb',\n",
      "                              GradientBoostingClassifier(max_depth=12,\n",
      "                                                         min_samples_split=12,\n",
      "                                                         n_estimators=5,\n",
      "                                                         random_state=3)),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(max_depth=12,\n",
      "                                                     n_estimators=10,\n",
      "                                                     random_state=3)),\n",
      "                             ('xgb',\n",
      "                              XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "                                            colsample_bylevel=1,\n",
      "                                            colsample_bynode=1,\n",
      "                                            colsample_bytree=1,\n",
      "                                            enable_categorical=False, gamma=0,\n",
      "                                            gpu...\n",
      "                                            interaction_constraints='',\n",
      "                                            learning_rate=0.300000012,\n",
      "                                            max_delta_step=0, max_depth=12,\n",
      "                                            max_leaf_nodes=None,\n",
      "                                            min_child_weight=1,\n",
      "                                            min_samples_split=1, missing=nan,\n",
      "                                            monotone_constraints='()',\n",
      "                                            n_estimators=10, n_jobs=8,\n",
      "                                            num_parallel_tree=1,\n",
      "                                            predictor='auto', random_state=3,\n",
      "                                            reg_alpha=0, reg_lambda=1,\n",
      "                                            scale_pos_weight=1, subsample=1,\n",
      "                                            tree_method='exact',\n",
      "                                            validate_parameters=1,\n",
      "                                            verbosity=None))])\n",
      "kfold  4  score: 94.05%\n",
      "kfold  4  score: 99.19%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21:47:07] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:47:07] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "VotingClassifier(estimators=[('gb',\n",
      "                              GradientBoostingClassifier(max_depth=12,\n",
      "                                                         min_samples_split=12,\n",
      "                                                         n_estimators=5,\n",
      "                                                         random_state=3)),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(max_depth=12,\n",
      "                                                     n_estimators=10,\n",
      "                                                     random_state=3)),\n",
      "                             ('xgb',\n",
      "                              XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "                                            colsample_bylevel=1,\n",
      "                                            colsample_bynode=1,\n",
      "                                            colsample_bytree=1,\n",
      "                                            enable_categorical=False, gamma=0,\n",
      "                                            gpu...\n",
      "                                            interaction_constraints='',\n",
      "                                            learning_rate=0.300000012,\n",
      "                                            max_delta_step=0, max_depth=12,\n",
      "                                            max_leaf_nodes=None,\n",
      "                                            min_child_weight=1,\n",
      "                                            min_samples_split=1, missing=nan,\n",
      "                                            monotone_constraints='()',\n",
      "                                            n_estimators=10, n_jobs=8,\n",
      "                                            num_parallel_tree=1,\n",
      "                                            predictor='auto', random_state=3,\n",
      "                                            reg_alpha=0, reg_lambda=1,\n",
      "                                            scale_pos_weight=1, subsample=1,\n",
      "                                            tree_method='exact',\n",
      "                                            validate_parameters=1,\n",
      "                                            verbosity=None))])\n",
      "kfold  4  score: 96.93%\n",
      "kfold  4  score: 99.44%\n",
      "[21:47:08] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:47:08] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "VotingClassifier(estimators=[('gb',\n",
      "                              GradientBoostingClassifier(max_depth=12,\n",
      "                                                         min_samples_split=12,\n",
      "                                                         n_estimators=5,\n",
      "                                                         random_state=3)),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(max_depth=12,\n",
      "                                                     n_estimators=10,\n",
      "                                                     random_state=3)),\n",
      "                             ('xgb',\n",
      "                              XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "                                            colsample_bylevel=1,\n",
      "                                            colsample_bynode=1,\n",
      "                                            colsample_bytree=1,\n",
      "                                            enable_categorical=False, gamma=0,\n",
      "                                            gpu...\n",
      "                                            interaction_constraints='',\n",
      "                                            learning_rate=0.300000012,\n",
      "                                            max_delta_step=0, max_depth=12,\n",
      "                                            max_leaf_nodes=None,\n",
      "                                            min_child_weight=1,\n",
      "                                            min_samples_split=1, missing=nan,\n",
      "                                            monotone_constraints='()',\n",
      "                                            n_estimators=10, n_jobs=8,\n",
      "                                            num_parallel_tree=1,\n",
      "                                            predictor='auto', random_state=3,\n",
      "                                            reg_alpha=0, reg_lambda=1,\n",
      "                                            scale_pos_weight=1, subsample=1,\n",
      "                                            tree_method='exact',\n",
      "                                            validate_parameters=1,\n",
      "                                            verbosity=None))])\n",
      "kfold  4  score: 93.08%\n",
      "kfold  4  score: 99.57%\n",
      "[21:47:08] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:47:08] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "VotingClassifier(estimators=[('gb',\n",
      "                              GradientBoostingClassifier(max_depth=12,\n",
      "                                                         min_samples_split=12,\n",
      "                                                         n_estimators=5,\n",
      "                                                         random_state=3)),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(max_depth=12,\n",
      "                                                     n_estimators=10,\n",
      "                                                     random_state=3)),\n",
      "                             ('xgb',\n",
      "                              XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "                                            colsample_bylevel=1,\n",
      "                                            colsample_bynode=1,\n",
      "                                            colsample_bytree=1,\n",
      "                                            enable_categorical=False, gamma=0,\n",
      "                                            gpu...\n",
      "                                            interaction_constraints='',\n",
      "                                            learning_rate=0.300000012,\n",
      "                                            max_delta_step=0, max_depth=12,\n",
      "                                            max_leaf_nodes=None,\n",
      "                                            min_child_weight=1,\n",
      "                                            min_samples_split=1, missing=nan,\n",
      "                                            monotone_constraints='()',\n",
      "                                            n_estimators=10, n_jobs=8,\n",
      "                                            num_parallel_tree=1,\n",
      "                                            predictor='auto', random_state=3,\n",
      "                                            reg_alpha=0, reg_lambda=1,\n",
      "                                            scale_pos_weight=1, subsample=1,\n",
      "                                            tree_method='exact',\n",
      "                                            validate_parameters=1,\n",
      "                                            verbosity=None))])\n",
      "kfold  4  score: 95.77%\n",
      "kfold  4  score: 99.49%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21:47:09] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:47:09] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "VotingClassifier(estimators=[('gb',\n",
      "                              GradientBoostingClassifier(max_depth=12,\n",
      "                                                         min_samples_split=12,\n",
      "                                                         n_estimators=5,\n",
      "                                                         random_state=3)),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(max_depth=12,\n",
      "                                                     n_estimators=10,\n",
      "                                                     random_state=3)),\n",
      "                             ('xgb',\n",
      "                              XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "                                            colsample_bylevel=1,\n",
      "                                            colsample_bynode=1,\n",
      "                                            colsample_bytree=1,\n",
      "                                            enable_categorical=False, gamma=0,\n",
      "                                            gpu...\n",
      "                                            interaction_constraints='',\n",
      "                                            learning_rate=0.300000012,\n",
      "                                            max_delta_step=0, max_depth=12,\n",
      "                                            max_leaf_nodes=None,\n",
      "                                            min_child_weight=1,\n",
      "                                            min_samples_split=1, missing=nan,\n",
      "                                            monotone_constraints='()',\n",
      "                                            n_estimators=10, n_jobs=8,\n",
      "                                            num_parallel_tree=1,\n",
      "                                            predictor='auto', random_state=3,\n",
      "                                            reg_alpha=0, reg_lambda=1,\n",
      "                                            scale_pos_weight=1, subsample=1,\n",
      "                                            tree_method='exact',\n",
      "                                            validate_parameters=1,\n",
      "                                            verbosity=None))])\n",
      "kfold  4  score: 94.81%\n",
      "kfold  4  score: 99.57%\n",
      "[21:47:10] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:47:10] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "VotingClassifier(estimators=[('gb',\n",
      "                              GradientBoostingClassifier(max_depth=12,\n",
      "                                                         min_samples_split=12,\n",
      "                                                         n_estimators=5,\n",
      "                                                         random_state=3)),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(max_depth=12,\n",
      "                                                     n_estimators=10,\n",
      "                                                     random_state=3)),\n",
      "                             ('xgb',\n",
      "                              XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "                                            colsample_bylevel=1,\n",
      "                                            colsample_bynode=1,\n",
      "                                            colsample_bytree=1,\n",
      "                                            enable_categorical=False, gamma=0,\n",
      "                                            gpu...\n",
      "                                            interaction_constraints='',\n",
      "                                            learning_rate=0.300000012,\n",
      "                                            max_delta_step=0, max_depth=12,\n",
      "                                            max_leaf_nodes=None,\n",
      "                                            min_child_weight=1,\n",
      "                                            min_samples_split=1, missing=nan,\n",
      "                                            monotone_constraints='()',\n",
      "                                            n_estimators=10, n_jobs=8,\n",
      "                                            num_parallel_tree=1,\n",
      "                                            predictor='auto', random_state=3,\n",
      "                                            reg_alpha=0, reg_lambda=1,\n",
      "                                            scale_pos_weight=1, subsample=1,\n",
      "                                            tree_method='exact',\n",
      "                                            validate_parameters=1,\n",
      "                                            verbosity=None))])\n",
      "kfold  4  score: 92.88%\n",
      "kfold  4  score: 99.57%\n",
      "[21:47:10] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:47:10] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "VotingClassifier(estimators=[('gb',\n",
      "                              GradientBoostingClassifier(max_depth=12,\n",
      "                                                         min_samples_split=12,\n",
      "                                                         n_estimators=5,\n",
      "                                                         random_state=3)),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(max_depth=12,\n",
      "                                                     n_estimators=10,\n",
      "                                                     random_state=3)),\n",
      "                             ('xgb',\n",
      "                              XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "                                            colsample_bylevel=1,\n",
      "                                            colsample_bynode=1,\n",
      "                                            colsample_bytree=1,\n",
      "                                            enable_categorical=False, gamma=0,\n",
      "                                            gpu...\n",
      "                                            interaction_constraints='',\n",
      "                                            learning_rate=0.300000012,\n",
      "                                            max_delta_step=0, max_depth=12,\n",
      "                                            max_leaf_nodes=None,\n",
      "                                            min_child_weight=1,\n",
      "                                            min_samples_split=1, missing=nan,\n",
      "                                            monotone_constraints='()',\n",
      "                                            n_estimators=10, n_jobs=8,\n",
      "                                            num_parallel_tree=1,\n",
      "                                            predictor='auto', random_state=3,\n",
      "                                            reg_alpha=0, reg_lambda=1,\n",
      "                                            scale_pos_weight=1, subsample=1,\n",
      "                                            tree_method='exact',\n",
      "                                            validate_parameters=1,\n",
      "                                            verbosity=None))])\n",
      "kfold  4  score: 95.58%\n",
      "kfold  4  score: 99.57%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21:47:11] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:47:11] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "VotingClassifier(estimators=[('gb',\n",
      "                              GradientBoostingClassifier(max_depth=12,\n",
      "                                                         min_samples_split=12,\n",
      "                                                         n_estimators=5,\n",
      "                                                         random_state=3)),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(max_depth=12,\n",
      "                                                     n_estimators=10,\n",
      "                                                     random_state=3)),\n",
      "                             ('xgb',\n",
      "                              XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "                                            colsample_bylevel=1,\n",
      "                                            colsample_bynode=1,\n",
      "                                            colsample_bytree=1,\n",
      "                                            enable_categorical=False, gamma=0,\n",
      "                                            gpu...\n",
      "                                            interaction_constraints='',\n",
      "                                            learning_rate=0.300000012,\n",
      "                                            max_delta_step=0, max_depth=12,\n",
      "                                            max_leaf_nodes=None,\n",
      "                                            min_child_weight=1,\n",
      "                                            min_samples_split=1, missing=nan,\n",
      "                                            monotone_constraints='()',\n",
      "                                            n_estimators=10, n_jobs=8,\n",
      "                                            num_parallel_tree=1,\n",
      "                                            predictor='auto', random_state=3,\n",
      "                                            reg_alpha=0, reg_lambda=1,\n",
      "                                            scale_pos_weight=1, subsample=1,\n",
      "                                            tree_method='exact',\n",
      "                                            validate_parameters=1,\n",
      "                                            verbosity=None))])\n",
      "kfold  4  score: 93.85%\n",
      "kfold  4  score: 99.44%\n",
      "[21:47:11] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:47:11] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "VotingClassifier(estimators=[('gb',\n",
      "                              GradientBoostingClassifier(max_depth=12,\n",
      "                                                         min_samples_split=12,\n",
      "                                                         n_estimators=5,\n",
      "                                                         random_state=3)),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(max_depth=12,\n",
      "                                                     n_estimators=10,\n",
      "                                                     random_state=3)),\n",
      "                             ('xgb',\n",
      "                              XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "                                            colsample_bylevel=1,\n",
      "                                            colsample_bynode=1,\n",
      "                                            colsample_bytree=1,\n",
      "                                            enable_categorical=False, gamma=0,\n",
      "                                            gpu...\n",
      "                                            interaction_constraints='',\n",
      "                                            learning_rate=0.300000012,\n",
      "                                            max_delta_step=0, max_depth=12,\n",
      "                                            max_leaf_nodes=None,\n",
      "                                            min_child_weight=1,\n",
      "                                            min_samples_split=1, missing=nan,\n",
      "                                            monotone_constraints='()',\n",
      "                                            n_estimators=10, n_jobs=8,\n",
      "                                            num_parallel_tree=1,\n",
      "                                            predictor='auto', random_state=3,\n",
      "                                            reg_alpha=0, reg_lambda=1,\n",
      "                                            scale_pos_weight=1, subsample=1,\n",
      "                                            tree_method='exact',\n",
      "                                            validate_parameters=1,\n",
      "                                            verbosity=None))])\n",
      "kfold  4  score: 96.35%\n",
      "kfold  4  score: 99.53%\n",
      "[21:47:12] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:47:12] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "VotingClassifier(estimators=[('gb',\n",
      "                              GradientBoostingClassifier(max_depth=12,\n",
      "                                                         min_samples_split=12,\n",
      "                                                         n_estimators=5,\n",
      "                                                         random_state=3)),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(max_depth=12,\n",
      "                                                     n_estimators=10,\n",
      "                                                     random_state=3)),\n",
      "                             ('xgb',\n",
      "                              XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "                                            colsample_bylevel=1,\n",
      "                                            colsample_bynode=1,\n",
      "                                            colsample_bytree=1,\n",
      "                                            enable_categorical=False, gamma=0,\n",
      "                                            gpu...\n",
      "                                            interaction_constraints='',\n",
      "                                            learning_rate=0.300000012,\n",
      "                                            max_delta_step=0, max_depth=12,\n",
      "                                            max_leaf_nodes=None,\n",
      "                                            min_child_weight=1,\n",
      "                                            min_samples_split=1, missing=nan,\n",
      "                                            monotone_constraints='()',\n",
      "                                            n_estimators=10, n_jobs=8,\n",
      "                                            num_parallel_tree=1,\n",
      "                                            predictor='auto', random_state=3,\n",
      "                                            reg_alpha=0, reg_lambda=1,\n",
      "                                            scale_pos_weight=1, subsample=1,\n",
      "                                            tree_method='exact',\n",
      "                                            validate_parameters=1,\n",
      "                                            verbosity=None))])\n",
      "kfold  4  score: 94.23%\n",
      "kfold  4  score: 99.30%\n",
      "Average score over k-fold 94.75% (+/-1.30%) [Ensemble Majority]\n",
      "./Files_Majority_vote/128_features/filter5.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_estimators = 100 \n",
      "\n",
      "10-fold cross validation for file :  5\n",
      "GradientBoostingClassifier(max_depth=12, min_samples_split=12, n_estimators=5,\n",
      "                           random_state=3)\n",
      "kfold  5  score: 90.14%\n",
      "kfold  5  score: 93.54%\n",
      "GradientBoostingClassifier(max_depth=12, min_samples_split=12, n_estimators=5,\n",
      "                           random_state=3)\n",
      "kfold  5  score: 87.98%\n",
      "kfold  5  score: 90.05%\n",
      "GradientBoostingClassifier(max_depth=12, min_samples_split=12, n_estimators=5,\n",
      "                           random_state=3)\n",
      "kfold  5  score: 90.50%\n",
      "kfold  5  score: 94.49%\n",
      "GradientBoostingClassifier(max_depth=12, min_samples_split=12, n_estimators=5,\n",
      "                           random_state=3)\n",
      "kfold  5  score: 87.40%\n",
      "kfold  5  score: 91.13%\n",
      "GradientBoostingClassifier(max_depth=12, min_samples_split=12, n_estimators=5,\n",
      "                           random_state=3)\n",
      "kfold  5  score: 87.60%\n",
      "kfold  5  score: 90.46%\n",
      "GradientBoostingClassifier(max_depth=12, min_samples_split=12, n_estimators=5,\n",
      "                           random_state=3)\n",
      "kfold  5  score: 91.09%\n",
      "kfold  5  score: 94.12%\n",
      "GradientBoostingClassifier(max_depth=12, min_samples_split=12, n_estimators=5,\n",
      "                           random_state=3)\n",
      "kfold  5  score: 89.92%\n",
      "kfold  5  score: 91.97%\n",
      "GradientBoostingClassifier(max_depth=12, min_samples_split=12, n_estimators=5,\n",
      "                           random_state=3)\n",
      "kfold  5  score: 85.66%\n",
      "kfold  5  score: 87.45%\n",
      "GradientBoostingClassifier(max_depth=12, min_samples_split=12, n_estimators=5,\n",
      "                           random_state=3)\n",
      "kfold  5  score: 86.05%\n",
      "kfold  5  score: 89.95%\n",
      "GradientBoostingClassifier(max_depth=12, min_samples_split=12, n_estimators=5,\n",
      "                           random_state=3)\n",
      "kfold  5  score: 88.57%\n",
      "kfold  5  score: 92.38%\n",
      "Average score over k-fold 88.49% (+/-1.78%) [Gradient Boosting]\n",
      "RandomForestClassifier(max_depth=12, n_estimators=10, random_state=3)\n",
      "kfold  5  score: 94.78%\n",
      "kfold  5  score: 98.11%\n",
      "RandomForestClassifier(max_depth=12, n_estimators=10, random_state=3)\n",
      "kfold  5  score: 94.38%\n",
      "kfold  5  score: 97.50%\n",
      "RandomForestClassifier(max_depth=12, n_estimators=10, random_state=3)\n",
      "kfold  5  score: 93.60%\n",
      "kfold  5  score: 96.94%\n",
      "RandomForestClassifier(max_depth=12, n_estimators=10, random_state=3)\n",
      "kfold  5  score: 94.38%\n",
      "kfold  5  score: 97.42%\n",
      "RandomForestClassifier(max_depth=12, n_estimators=10, random_state=3)\n",
      "kfold  5  score: 94.19%\n",
      "kfold  5  score: 97.89%\n",
      "RandomForestClassifier(max_depth=12, n_estimators=10, random_state=3)\n",
      "kfold  5  score: 93.22%\n",
      "kfold  5  score: 98.15%\n",
      "RandomForestClassifier(max_depth=12, n_estimators=10, random_state=3)\n",
      "kfold  5  score: 92.83%\n",
      "kfold  5  score: 98.58%\n",
      "RandomForestClassifier(max_depth=12, n_estimators=10, random_state=3)\n",
      "kfold  5  score: 94.57%\n",
      "kfold  5  score: 98.67%\n",
      "RandomForestClassifier(max_depth=12, n_estimators=10, random_state=3)\n",
      "kfold  5  score: 93.99%\n",
      "kfold  5  score: 98.69%\n",
      "RandomForestClassifier(max_depth=12, n_estimators=10, random_state=3)\n",
      "kfold  5  score: 91.67%\n",
      "kfold  5  score: 97.50%\n",
      "Average score over k-fold 93.76% (+/-0.91%) [Random Forest]\n",
      "[21:47:15] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:47:15] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
      "              gamma=0, gpu_id=-1, importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.300000012,\n",
      "              max_delta_step=0, max_depth=12, max_leaf_nodes=None,\n",
      "              min_child_weight=1, min_samples_split=1, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=10, n_jobs=8,\n",
      "              num_parallel_tree=1, predictor='auto', random_state=3,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)\n",
      "kfold  5  score: 96.91%\n",
      "kfold  5  score: 99.31%\n",
      "[21:47:15] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:47:15] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
      "              gamma=0, gpu_id=-1, importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.300000012,\n",
      "              max_delta_step=0, max_depth=12, max_leaf_nodes=None,\n",
      "              min_child_weight=1, min_samples_split=1, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=10, n_jobs=8,\n",
      "              num_parallel_tree=1, predictor='auto', random_state=3,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)\n",
      "kfold  5  score: 94.57%\n",
      "kfold  5  score: 99.48%\n",
      "[21:47:16] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:47:16] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
      "              gamma=0, gpu_id=-1, importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.300000012,\n",
      "              max_delta_step=0, max_depth=12, max_leaf_nodes=None,\n",
      "              min_child_weight=1, min_samples_split=1, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=10, n_jobs=8,\n",
      "              num_parallel_tree=1, predictor='auto', random_state=3,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)\n",
      "kfold  5  score: 94.38%\n",
      "kfold  5  score: 99.20%\n",
      "[21:47:16] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:47:16] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
      "              gamma=0, gpu_id=-1, importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.300000012,\n",
      "              max_delta_step=0, max_depth=12, max_leaf_nodes=None,\n",
      "              min_child_weight=1, min_samples_split=1, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=10, n_jobs=8,\n",
      "              num_parallel_tree=1, predictor='auto', random_state=3,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)\n",
      "kfold  5  score: 95.35%\n",
      "kfold  5  score: 99.48%\n",
      "[21:47:16] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:47:16] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
      "              gamma=0, gpu_id=-1, importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.300000012,\n",
      "              max_delta_step=0, max_depth=12, max_leaf_nodes=None,\n",
      "              min_child_weight=1, min_samples_split=1, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=10, n_jobs=8,\n",
      "              num_parallel_tree=1, predictor='auto', random_state=3,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)\n",
      "kfold  5  score: 95.35%\n",
      "kfold  5  score: 99.50%\n",
      "[21:47:16] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:47:16] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
      "              gamma=0, gpu_id=-1, importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.300000012,\n",
      "              max_delta_step=0, max_depth=12, max_leaf_nodes=None,\n",
      "              min_child_weight=1, min_samples_split=1, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=10, n_jobs=8,\n",
      "              num_parallel_tree=1, predictor='auto', random_state=3,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)\n",
      "kfold  5  score: 94.38%\n",
      "kfold  5  score: 99.59%\n",
      "[21:47:16] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:47:16] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
      "              gamma=0, gpu_id=-1, importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.300000012,\n",
      "              max_delta_step=0, max_depth=12, max_leaf_nodes=None,\n",
      "              min_child_weight=1, min_samples_split=1, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=10, n_jobs=8,\n",
      "              num_parallel_tree=1, predictor='auto', random_state=3,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)\n",
      "kfold  5  score: 93.99%\n",
      "kfold  5  score: 99.35%\n",
      "[21:47:16] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:47:16] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
      "              gamma=0, gpu_id=-1, importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.300000012,\n",
      "              max_delta_step=0, max_depth=12, max_leaf_nodes=None,\n",
      "              min_child_weight=1, min_samples_split=1, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=10, n_jobs=8,\n",
      "              num_parallel_tree=1, predictor='auto', random_state=3,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)\n",
      "kfold  5  score: 95.93%\n",
      "kfold  5  score: 99.29%\n",
      "[21:47:16] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:47:16] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
      "              gamma=0, gpu_id=-1, importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.300000012,\n",
      "              max_delta_step=0, max_depth=12, max_leaf_nodes=None,\n",
      "              min_child_weight=1, min_samples_split=1, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=10, n_jobs=8,\n",
      "              num_parallel_tree=1, predictor='auto', random_state=3,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)\n",
      "kfold  5  score: 95.35%\n",
      "kfold  5  score: 99.61%\n",
      "[21:47:16] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:47:16] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
      "              gamma=0, gpu_id=-1, importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.300000012,\n",
      "              max_delta_step=0, max_depth=12, max_leaf_nodes=None,\n",
      "              min_child_weight=1, min_samples_split=1, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=10, n_jobs=8,\n",
      "              num_parallel_tree=1, predictor='auto', random_state=3,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)\n",
      "kfold  5  score: 93.99%\n",
      "kfold  5  score: 99.61%\n",
      "Average score over k-fold 95.02% (+/-0.89%) [XGBClassifier]\n",
      "[21:47:17] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:47:17] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "VotingClassifier(estimators=[('gb',\n",
      "                              GradientBoostingClassifier(max_depth=12,\n",
      "                                                         min_samples_split=12,\n",
      "                                                         n_estimators=5,\n",
      "                                                         random_state=3)),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(max_depth=12,\n",
      "                                                     n_estimators=10,\n",
      "                                                     random_state=3)),\n",
      "                             ('xgb',\n",
      "                              XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "                                            colsample_bylevel=1,\n",
      "                                            colsample_bynode=1,\n",
      "                                            colsample_bytree=1,\n",
      "                                            enable_categorical=False, gamma=0,\n",
      "                                            gpu...\n",
      "                                            interaction_constraints='',\n",
      "                                            learning_rate=0.300000012,\n",
      "                                            max_delta_step=0, max_depth=12,\n",
      "                                            max_leaf_nodes=None,\n",
      "                                            min_child_weight=1,\n",
      "                                            min_samples_split=1, missing=nan,\n",
      "                                            monotone_constraints='()',\n",
      "                                            n_estimators=10, n_jobs=8,\n",
      "                                            num_parallel_tree=1,\n",
      "                                            predictor='auto', random_state=3,\n",
      "                                            reg_alpha=0, reg_lambda=1,\n",
      "                                            scale_pos_weight=1, subsample=1,\n",
      "                                            tree_method='exact',\n",
      "                                            validate_parameters=1,\n",
      "                                            verbosity=None))])\n",
      "kfold  5  score: 95.55%\n",
      "kfold  5  score: 98.45%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21:47:17] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:47:17] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "VotingClassifier(estimators=[('gb',\n",
      "                              GradientBoostingClassifier(max_depth=12,\n",
      "                                                         min_samples_split=12,\n",
      "                                                         n_estimators=5,\n",
      "                                                         random_state=3)),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(max_depth=12,\n",
      "                                                     n_estimators=10,\n",
      "                                                     random_state=3)),\n",
      "                             ('xgb',\n",
      "                              XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "                                            colsample_bylevel=1,\n",
      "                                            colsample_bynode=1,\n",
      "                                            colsample_bytree=1,\n",
      "                                            enable_categorical=False, gamma=0,\n",
      "                                            gpu...\n",
      "                                            interaction_constraints='',\n",
      "                                            learning_rate=0.300000012,\n",
      "                                            max_delta_step=0, max_depth=12,\n",
      "                                            max_leaf_nodes=None,\n",
      "                                            min_child_weight=1,\n",
      "                                            min_samples_split=1, missing=nan,\n",
      "                                            monotone_constraints='()',\n",
      "                                            n_estimators=10, n_jobs=8,\n",
      "                                            num_parallel_tree=1,\n",
      "                                            predictor='auto', random_state=3,\n",
      "                                            reg_alpha=0, reg_lambda=1,\n",
      "                                            scale_pos_weight=1, subsample=1,\n",
      "                                            tree_method='exact',\n",
      "                                            validate_parameters=1,\n",
      "                                            verbosity=None))])\n",
      "kfold  5  score: 94.77%\n",
      "kfold  5  score: 97.93%\n",
      "[21:47:18] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:47:18] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "VotingClassifier(estimators=[('gb',\n",
      "                              GradientBoostingClassifier(max_depth=12,\n",
      "                                                         min_samples_split=12,\n",
      "                                                         n_estimators=5,\n",
      "                                                         random_state=3)),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(max_depth=12,\n",
      "                                                     n_estimators=10,\n",
      "                                                     random_state=3)),\n",
      "                             ('xgb',\n",
      "                              XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "                                            colsample_bylevel=1,\n",
      "                                            colsample_bynode=1,\n",
      "                                            colsample_bytree=1,\n",
      "                                            enable_categorical=False, gamma=0,\n",
      "                                            gpu...\n",
      "                                            interaction_constraints='',\n",
      "                                            learning_rate=0.300000012,\n",
      "                                            max_delta_step=0, max_depth=12,\n",
      "                                            max_leaf_nodes=None,\n",
      "                                            min_child_weight=1,\n",
      "                                            min_samples_split=1, missing=nan,\n",
      "                                            monotone_constraints='()',\n",
      "                                            n_estimators=10, n_jobs=8,\n",
      "                                            num_parallel_tree=1,\n",
      "                                            predictor='auto', random_state=3,\n",
      "                                            reg_alpha=0, reg_lambda=1,\n",
      "                                            scale_pos_weight=1, subsample=1,\n",
      "                                            tree_method='exact',\n",
      "                                            validate_parameters=1,\n",
      "                                            verbosity=None))])\n",
      "kfold  5  score: 93.80%\n",
      "kfold  5  score: 98.04%\n",
      "[21:47:18] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:47:18] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "VotingClassifier(estimators=[('gb',\n",
      "                              GradientBoostingClassifier(max_depth=12,\n",
      "                                                         min_samples_split=12,\n",
      "                                                         n_estimators=5,\n",
      "                                                         random_state=3)),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(max_depth=12,\n",
      "                                                     n_estimators=10,\n",
      "                                                     random_state=3)),\n",
      "                             ('xgb',\n",
      "                              XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "                                            colsample_bylevel=1,\n",
      "                                            colsample_bynode=1,\n",
      "                                            colsample_bytree=1,\n",
      "                                            enable_categorical=False, gamma=0,\n",
      "                                            gpu...\n",
      "                                            interaction_constraints='',\n",
      "                                            learning_rate=0.300000012,\n",
      "                                            max_delta_step=0, max_depth=12,\n",
      "                                            max_leaf_nodes=None,\n",
      "                                            min_child_weight=1,\n",
      "                                            min_samples_split=1, missing=nan,\n",
      "                                            monotone_constraints='()',\n",
      "                                            n_estimators=10, n_jobs=8,\n",
      "                                            num_parallel_tree=1,\n",
      "                                            predictor='auto', random_state=3,\n",
      "                                            reg_alpha=0, reg_lambda=1,\n",
      "                                            scale_pos_weight=1, subsample=1,\n",
      "                                            tree_method='exact',\n",
      "                                            validate_parameters=1,\n",
      "                                            verbosity=None))])\n",
      "kfold  5  score: 94.38%\n",
      "kfold  5  score: 97.55%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21:47:19] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:47:19] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "VotingClassifier(estimators=[('gb',\n",
      "                              GradientBoostingClassifier(max_depth=12,\n",
      "                                                         min_samples_split=12,\n",
      "                                                         n_estimators=5,\n",
      "                                                         random_state=3)),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(max_depth=12,\n",
      "                                                     n_estimators=10,\n",
      "                                                     random_state=3)),\n",
      "                             ('xgb',\n",
      "                              XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "                                            colsample_bylevel=1,\n",
      "                                            colsample_bynode=1,\n",
      "                                            colsample_bytree=1,\n",
      "                                            enable_categorical=False, gamma=0,\n",
      "                                            gpu...\n",
      "                                            interaction_constraints='',\n",
      "                                            learning_rate=0.300000012,\n",
      "                                            max_delta_step=0, max_depth=12,\n",
      "                                            max_leaf_nodes=None,\n",
      "                                            min_child_weight=1,\n",
      "                                            min_samples_split=1, missing=nan,\n",
      "                                            monotone_constraints='()',\n",
      "                                            n_estimators=10, n_jobs=8,\n",
      "                                            num_parallel_tree=1,\n",
      "                                            predictor='auto', random_state=3,\n",
      "                                            reg_alpha=0, reg_lambda=1,\n",
      "                                            scale_pos_weight=1, subsample=1,\n",
      "                                            tree_method='exact',\n",
      "                                            validate_parameters=1,\n",
      "                                            verbosity=None))])\n",
      "kfold  5  score: 94.19%\n",
      "kfold  5  score: 98.11%\n",
      "[21:47:19] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:47:19] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "VotingClassifier(estimators=[('gb',\n",
      "                              GradientBoostingClassifier(max_depth=12,\n",
      "                                                         min_samples_split=12,\n",
      "                                                         n_estimators=5,\n",
      "                                                         random_state=3)),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(max_depth=12,\n",
      "                                                     n_estimators=10,\n",
      "                                                     random_state=3)),\n",
      "                             ('xgb',\n",
      "                              XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "                                            colsample_bylevel=1,\n",
      "                                            colsample_bynode=1,\n",
      "                                            colsample_bytree=1,\n",
      "                                            enable_categorical=False, gamma=0,\n",
      "                                            gpu...\n",
      "                                            interaction_constraints='',\n",
      "                                            learning_rate=0.300000012,\n",
      "                                            max_delta_step=0, max_depth=12,\n",
      "                                            max_leaf_nodes=None,\n",
      "                                            min_child_weight=1,\n",
      "                                            min_samples_split=1, missing=nan,\n",
      "                                            monotone_constraints='()',\n",
      "                                            n_estimators=10, n_jobs=8,\n",
      "                                            num_parallel_tree=1,\n",
      "                                            predictor='auto', random_state=3,\n",
      "                                            reg_alpha=0, reg_lambda=1,\n",
      "                                            scale_pos_weight=1, subsample=1,\n",
      "                                            tree_method='exact',\n",
      "                                            validate_parameters=1,\n",
      "                                            verbosity=None))])\n",
      "kfold  5  score: 94.19%\n",
      "kfold  5  score: 98.69%\n",
      "[21:47:20] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:47:20] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "VotingClassifier(estimators=[('gb',\n",
      "                              GradientBoostingClassifier(max_depth=12,\n",
      "                                                         min_samples_split=12,\n",
      "                                                         n_estimators=5,\n",
      "                                                         random_state=3)),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(max_depth=12,\n",
      "                                                     n_estimators=10,\n",
      "                                                     random_state=3)),\n",
      "                             ('xgb',\n",
      "                              XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "                                            colsample_bylevel=1,\n",
      "                                            colsample_bynode=1,\n",
      "                                            colsample_bytree=1,\n",
      "                                            enable_categorical=False, gamma=0,\n",
      "                                            gpu...\n",
      "                                            interaction_constraints='',\n",
      "                                            learning_rate=0.300000012,\n",
      "                                            max_delta_step=0, max_depth=12,\n",
      "                                            max_leaf_nodes=None,\n",
      "                                            min_child_weight=1,\n",
      "                                            min_samples_split=1, missing=nan,\n",
      "                                            monotone_constraints='()',\n",
      "                                            n_estimators=10, n_jobs=8,\n",
      "                                            num_parallel_tree=1,\n",
      "                                            predictor='auto', random_state=3,\n",
      "                                            reg_alpha=0, reg_lambda=1,\n",
      "                                            scale_pos_weight=1, subsample=1,\n",
      "                                            tree_method='exact',\n",
      "                                            validate_parameters=1,\n",
      "                                            verbosity=None))])\n",
      "kfold  5  score: 93.22%\n",
      "kfold  5  score: 98.69%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21:47:20] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:47:20] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "VotingClassifier(estimators=[('gb',\n",
      "                              GradientBoostingClassifier(max_depth=12,\n",
      "                                                         min_samples_split=12,\n",
      "                                                         n_estimators=5,\n",
      "                                                         random_state=3)),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(max_depth=12,\n",
      "                                                     n_estimators=10,\n",
      "                                                     random_state=3)),\n",
      "                             ('xgb',\n",
      "                              XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "                                            colsample_bylevel=1,\n",
      "                                            colsample_bynode=1,\n",
      "                                            colsample_bytree=1,\n",
      "                                            enable_categorical=False, gamma=0,\n",
      "                                            gpu...\n",
      "                                            interaction_constraints='',\n",
      "                                            learning_rate=0.300000012,\n",
      "                                            max_delta_step=0, max_depth=12,\n",
      "                                            max_leaf_nodes=None,\n",
      "                                            min_child_weight=1,\n",
      "                                            min_samples_split=1, missing=nan,\n",
      "                                            monotone_constraints='()',\n",
      "                                            n_estimators=10, n_jobs=8,\n",
      "                                            num_parallel_tree=1,\n",
      "                                            predictor='auto', random_state=3,\n",
      "                                            reg_alpha=0, reg_lambda=1,\n",
      "                                            scale_pos_weight=1, subsample=1,\n",
      "                                            tree_method='exact',\n",
      "                                            validate_parameters=1,\n",
      "                                            verbosity=None))])\n",
      "kfold  5  score: 94.57%\n",
      "kfold  5  score: 98.67%\n",
      "[21:47:20] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:47:20] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "VotingClassifier(estimators=[('gb',\n",
      "                              GradientBoostingClassifier(max_depth=12,\n",
      "                                                         min_samples_split=12,\n",
      "                                                         n_estimators=5,\n",
      "                                                         random_state=3)),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(max_depth=12,\n",
      "                                                     n_estimators=10,\n",
      "                                                     random_state=3)),\n",
      "                             ('xgb',\n",
      "                              XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "                                            colsample_bylevel=1,\n",
      "                                            colsample_bynode=1,\n",
      "                                            colsample_bytree=1,\n",
      "                                            enable_categorical=False, gamma=0,\n",
      "                                            gpu...\n",
      "                                            interaction_constraints='',\n",
      "                                            learning_rate=0.300000012,\n",
      "                                            max_delta_step=0, max_depth=12,\n",
      "                                            max_leaf_nodes=None,\n",
      "                                            min_child_weight=1,\n",
      "                                            min_samples_split=1, missing=nan,\n",
      "                                            monotone_constraints='()',\n",
      "                                            n_estimators=10, n_jobs=8,\n",
      "                                            num_parallel_tree=1,\n",
      "                                            predictor='auto', random_state=3,\n",
      "                                            reg_alpha=0, reg_lambda=1,\n",
      "                                            scale_pos_weight=1, subsample=1,\n",
      "                                            tree_method='exact',\n",
      "                                            validate_parameters=1,\n",
      "                                            verbosity=None))])\n",
      "kfold  5  score: 94.57%\n",
      "kfold  5  score: 98.67%\n",
      "[21:47:21] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:47:21] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "VotingClassifier(estimators=[('gb',\n",
      "                              GradientBoostingClassifier(max_depth=12,\n",
      "                                                         min_samples_split=12,\n",
      "                                                         n_estimators=5,\n",
      "                                                         random_state=3)),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(max_depth=12,\n",
      "                                                     n_estimators=10,\n",
      "                                                     random_state=3)),\n",
      "                             ('xgb',\n",
      "                              XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "                                            colsample_bylevel=1,\n",
      "                                            colsample_bynode=1,\n",
      "                                            colsample_bytree=1,\n",
      "                                            enable_categorical=False, gamma=0,\n",
      "                                            gpu...\n",
      "                                            interaction_constraints='',\n",
      "                                            learning_rate=0.300000012,\n",
      "                                            max_delta_step=0, max_depth=12,\n",
      "                                            max_leaf_nodes=None,\n",
      "                                            min_child_weight=1,\n",
      "                                            min_samples_split=1, missing=nan,\n",
      "                                            monotone_constraints='()',\n",
      "                                            n_estimators=10, n_jobs=8,\n",
      "                                            num_parallel_tree=1,\n",
      "                                            predictor='auto', random_state=3,\n",
      "                                            reg_alpha=0, reg_lambda=1,\n",
      "                                            scale_pos_weight=1, subsample=1,\n",
      "                                            tree_method='exact',\n",
      "                                            validate_parameters=1,\n",
      "                                            verbosity=None))])\n",
      "kfold  5  score: 92.44%\n",
      "kfold  5  score: 98.30%\n",
      "Average score over k-fold 94.17% (+/-0.82%) [Ensemble Majority]\n",
      "./Files_Majority_vote/128_features/filter6.csv\n",
      "n_estimators = 100 \n",
      "\n",
      "10-fold cross validation for file :  6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GradientBoostingClassifier(max_depth=12, min_samples_split=12, n_estimators=5,\n",
      "                           random_state=3)\n",
      "kfold  6  score: 81.49%\n",
      "kfold  6  score: 83.33%\n",
      "GradientBoostingClassifier(max_depth=12, min_samples_split=12, n_estimators=5,\n",
      "                           random_state=3)\n",
      "kfold  6  score: 85.31%\n",
      "kfold  6  score: 89.98%\n",
      "GradientBoostingClassifier(max_depth=12, min_samples_split=12, n_estimators=5,\n",
      "                           random_state=3)\n",
      "kfold  6  score: 83.90%\n",
      "kfold  6  score: 85.35%\n",
      "GradientBoostingClassifier(max_depth=12, min_samples_split=12, n_estimators=5,\n",
      "                           random_state=3)\n",
      "kfold  6  score: 79.88%\n",
      "kfold  6  score: 84.27%\n",
      "GradientBoostingClassifier(max_depth=12, min_samples_split=12, n_estimators=5,\n",
      "                           random_state=3)\n",
      "kfold  6  score: 84.71%\n",
      "kfold  6  score: 88.61%\n",
      "GradientBoostingClassifier(max_depth=12, min_samples_split=12, n_estimators=5,\n",
      "                           random_state=3)\n",
      "kfold  6  score: 80.48%\n",
      "kfold  6  score: 82.93%\n",
      "GradientBoostingClassifier(max_depth=12, min_samples_split=12, n_estimators=5,\n",
      "                           random_state=3)\n",
      "kfold  6  score: 83.50%\n",
      "kfold  6  score: 84.07%\n",
      "GradientBoostingClassifier(max_depth=12, min_samples_split=12, n_estimators=5,\n",
      "                           random_state=3)\n",
      "kfold  6  score: 81.65%\n",
      "kfold  6  score: 84.72%\n",
      "GradientBoostingClassifier(max_depth=12, min_samples_split=12, n_estimators=5,\n",
      "                           random_state=3)\n",
      "kfold  6  score: 83.27%\n",
      "kfold  6  score: 88.57%\n",
      "GradientBoostingClassifier(max_depth=12, min_samples_split=12, n_estimators=5,\n",
      "                           random_state=3)\n",
      "kfold  6  score: 81.25%\n",
      "kfold  6  score: 85.10%\n",
      "Average score over k-fold 82.54% (+/-1.75%) [Gradient Boosting]\n",
      "RandomForestClassifier(max_depth=12, n_estimators=10, random_state=3)\n",
      "kfold  6  score: 92.15%\n",
      "kfold  6  score: 96.55%\n",
      "RandomForestClassifier(max_depth=12, n_estimators=10, random_state=3)\n",
      "kfold  6  score: 90.34%\n",
      "kfold  6  score: 95.64%\n",
      "RandomForestClassifier(max_depth=12, n_estimators=10, random_state=3)\n",
      "kfold  6  score: 89.74%\n",
      "kfold  6  score: 93.58%\n",
      "RandomForestClassifier(max_depth=12, n_estimators=10, random_state=3)\n",
      "kfold  6  score: 91.15%\n",
      "kfold  6  score: 95.88%\n",
      "RandomForestClassifier(max_depth=12, n_estimators=10, random_state=3)\n",
      "kfold  6  score: 90.95%\n",
      "kfold  6  score: 95.68%\n",
      "RandomForestClassifier(max_depth=12, n_estimators=10, random_state=3)\n",
      "kfold  6  score: 91.55%\n",
      "kfold  6  score: 96.20%\n",
      "RandomForestClassifier(max_depth=12, n_estimators=10, random_state=3)\n",
      "kfold  6  score: 90.54%\n",
      "kfold  6  score: 94.92%\n",
      "RandomForestClassifier(max_depth=12, n_estimators=10, random_state=3)\n",
      "kfold  6  score: 92.74%\n",
      "kfold  6  score: 97.81%\n",
      "RandomForestClassifier(max_depth=12, n_estimators=10, random_state=3)\n",
      "kfold  6  score: 88.71%\n",
      "kfold  6  score: 95.24%\n",
      "RandomForestClassifier(max_depth=12, n_estimators=10, random_state=3)\n",
      "kfold  6  score: 91.94%\n",
      "kfold  6  score: 96.31%\n",
      "Average score over k-fold 90.98% (+/-1.14%) [Random Forest]\n",
      "[21:47:25] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:47:25] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
      "              gamma=0, gpu_id=-1, importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.300000012,\n",
      "              max_delta_step=0, max_depth=12, max_leaf_nodes=None,\n",
      "              min_child_weight=1, min_samples_split=1, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=10, n_jobs=8,\n",
      "              num_parallel_tree=1, predictor='auto', random_state=3,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)\n",
      "kfold  6  score: 95.17%\n",
      "kfold  6  score: 99.04%\n",
      "[21:47:25] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:47:25] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
      "              gamma=0, gpu_id=-1, importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.300000012,\n",
      "              max_delta_step=0, max_depth=12, max_leaf_nodes=None,\n",
      "              min_child_weight=1, min_samples_split=1, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=10, n_jobs=8,\n",
      "              num_parallel_tree=1, predictor='auto', random_state=3,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)\n",
      "kfold  6  score: 94.16%\n",
      "kfold  6  score: 98.66%\n",
      "[21:47:25] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:47:25] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
      "              gamma=0, gpu_id=-1, importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.300000012,\n",
      "              max_delta_step=0, max_depth=12, max_leaf_nodes=None,\n",
      "              min_child_weight=1, min_samples_split=1, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=10, n_jobs=8,\n",
      "              num_parallel_tree=1, predictor='auto', random_state=3,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)\n",
      "kfold  6  score: 92.56%\n",
      "kfold  6  score: 97.45%\n",
      "[21:47:25] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:47:25] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
      "              gamma=0, gpu_id=-1, importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.300000012,\n",
      "              max_delta_step=0, max_depth=12, max_leaf_nodes=None,\n",
      "              min_child_weight=1, min_samples_split=1, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=10, n_jobs=8,\n",
      "              num_parallel_tree=1, predictor='auto', random_state=3,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)\n",
      "kfold  6  score: 91.95%\n",
      "kfold  6  score: 98.26%\n",
      "[21:47:25] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:47:25] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
      "              gamma=0, gpu_id=-1, importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.300000012,\n",
      "              max_delta_step=0, max_depth=12, max_leaf_nodes=None,\n",
      "              min_child_weight=1, min_samples_split=1, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=10, n_jobs=8,\n",
      "              num_parallel_tree=1, predictor='auto', random_state=3,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)\n",
      "kfold  6  score: 93.56%\n",
      "kfold  6  score: 98.19%\n",
      "[21:47:25] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:47:25] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
      "              gamma=0, gpu_id=-1, importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.300000012,\n",
      "              max_delta_step=0, max_depth=12, max_leaf_nodes=None,\n",
      "              min_child_weight=1, min_samples_split=1, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=10, n_jobs=8,\n",
      "              num_parallel_tree=1, predictor='auto', random_state=3,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)\n",
      "kfold  6  score: 93.96%\n",
      "kfold  6  score: 98.88%\n",
      "[21:47:25] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:47:25] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
      "              gamma=0, gpu_id=-1, importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.300000012,\n",
      "              max_delta_step=0, max_depth=12, max_leaf_nodes=None,\n",
      "              min_child_weight=1, min_samples_split=1, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=10, n_jobs=8,\n",
      "              num_parallel_tree=1, predictor='auto', random_state=3,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)\n",
      "kfold  6  score: 95.57%\n",
      "kfold  6  score: 98.84%\n",
      "[21:47:25] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:47:25] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
      "              gamma=0, gpu_id=-1, importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.300000012,\n",
      "              max_delta_step=0, max_depth=12, max_leaf_nodes=None,\n",
      "              min_child_weight=1, min_samples_split=1, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=10, n_jobs=8,\n",
      "              num_parallel_tree=1, predictor='auto', random_state=3,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)\n",
      "kfold  6  score: 92.54%\n",
      "kfold  6  score: 97.47%\n",
      "[21:47:25] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:47:25] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
      "              gamma=0, gpu_id=-1, importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.300000012,\n",
      "              max_delta_step=0, max_depth=12, max_leaf_nodes=None,\n",
      "              min_child_weight=1, min_samples_split=1, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=10, n_jobs=8,\n",
      "              num_parallel_tree=1, predictor='auto', random_state=3,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)\n",
      "kfold  6  score: 92.54%\n",
      "kfold  6  score: 98.37%\n",
      "[21:47:26] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:47:26] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
      "              gamma=0, gpu_id=-1, importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.300000012,\n",
      "              max_delta_step=0, max_depth=12, max_leaf_nodes=None,\n",
      "              min_child_weight=1, min_samples_split=1, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=10, n_jobs=8,\n",
      "              num_parallel_tree=1, predictor='auto', random_state=3,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)\n",
      "kfold  6  score: 94.76%\n",
      "kfold  6  score: 98.41%\n",
      "Average score over k-fold 93.68% (+/-1.19%) [XGBClassifier]\n",
      "[21:47:26] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:47:26] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "VotingClassifier(estimators=[('gb',\n",
      "                              GradientBoostingClassifier(max_depth=12,\n",
      "                                                         min_samples_split=12,\n",
      "                                                         n_estimators=5,\n",
      "                                                         random_state=3)),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(max_depth=12,\n",
      "                                                     n_estimators=10,\n",
      "                                                     random_state=3)),\n",
      "                             ('xgb',\n",
      "                              XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "                                            colsample_bylevel=1,\n",
      "                                            colsample_bynode=1,\n",
      "                                            colsample_bytree=1,\n",
      "                                            enable_categorical=False, gamma=0,\n",
      "                                            gpu...\n",
      "                                            interaction_constraints='',\n",
      "                                            learning_rate=0.300000012,\n",
      "                                            max_delta_step=0, max_depth=12,\n",
      "                                            max_leaf_nodes=None,\n",
      "                                            min_child_weight=1,\n",
      "                                            min_samples_split=1, missing=nan,\n",
      "                                            monotone_constraints='()',\n",
      "                                            n_estimators=10, n_jobs=8,\n",
      "                                            num_parallel_tree=1,\n",
      "                                            predictor='auto', random_state=3,\n",
      "                                            reg_alpha=0, reg_lambda=1,\n",
      "                                            scale_pos_weight=1, subsample=1,\n",
      "                                            tree_method='exact',\n",
      "                                            validate_parameters=1,\n",
      "                                            verbosity=None))])\n",
      "kfold  6  score: 93.56%\n",
      "kfold  6  score: 97.16%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21:47:27] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:47:27] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "VotingClassifier(estimators=[('gb',\n",
      "                              GradientBoostingClassifier(max_depth=12,\n",
      "                                                         min_samples_split=12,\n",
      "                                                         n_estimators=5,\n",
      "                                                         random_state=3)),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(max_depth=12,\n",
      "                                                     n_estimators=10,\n",
      "                                                     random_state=3)),\n",
      "                             ('xgb',\n",
      "                              XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "                                            colsample_bylevel=1,\n",
      "                                            colsample_bynode=1,\n",
      "                                            colsample_bytree=1,\n",
      "                                            enable_categorical=False, gamma=0,\n",
      "                                            gpu...\n",
      "                                            interaction_constraints='',\n",
      "                                            learning_rate=0.300000012,\n",
      "                                            max_delta_step=0, max_depth=12,\n",
      "                                            max_leaf_nodes=None,\n",
      "                                            min_child_weight=1,\n",
      "                                            min_samples_split=1, missing=nan,\n",
      "                                            monotone_constraints='()',\n",
      "                                            n_estimators=10, n_jobs=8,\n",
      "                                            num_parallel_tree=1,\n",
      "                                            predictor='auto', random_state=3,\n",
      "                                            reg_alpha=0, reg_lambda=1,\n",
      "                                            scale_pos_weight=1, subsample=1,\n",
      "                                            tree_method='exact',\n",
      "                                            validate_parameters=1,\n",
      "                                            verbosity=None))])\n",
      "kfold  6  score: 91.15%\n",
      "kfold  6  score: 96.73%\n",
      "[21:47:27] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:47:27] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "VotingClassifier(estimators=[('gb',\n",
      "                              GradientBoostingClassifier(max_depth=12,\n",
      "                                                         min_samples_split=12,\n",
      "                                                         n_estimators=5,\n",
      "                                                         random_state=3)),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(max_depth=12,\n",
      "                                                     n_estimators=10,\n",
      "                                                     random_state=3)),\n",
      "                             ('xgb',\n",
      "                              XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "                                            colsample_bylevel=1,\n",
      "                                            colsample_bynode=1,\n",
      "                                            colsample_bytree=1,\n",
      "                                            enable_categorical=False, gamma=0,\n",
      "                                            gpu...\n",
      "                                            interaction_constraints='',\n",
      "                                            learning_rate=0.300000012,\n",
      "                                            max_delta_step=0, max_depth=12,\n",
      "                                            max_leaf_nodes=None,\n",
      "                                            min_child_weight=1,\n",
      "                                            min_samples_split=1, missing=nan,\n",
      "                                            monotone_constraints='()',\n",
      "                                            n_estimators=10, n_jobs=8,\n",
      "                                            num_parallel_tree=1,\n",
      "                                            predictor='auto', random_state=3,\n",
      "                                            reg_alpha=0, reg_lambda=1,\n",
      "                                            scale_pos_weight=1, subsample=1,\n",
      "                                            tree_method='exact',\n",
      "                                            validate_parameters=1,\n",
      "                                            verbosity=None))])\n",
      "kfold  6  score: 90.34%\n",
      "kfold  6  score: 93.96%\n",
      "[21:47:27] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:47:28] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "VotingClassifier(estimators=[('gb',\n",
      "                              GradientBoostingClassifier(max_depth=12,\n",
      "                                                         min_samples_split=12,\n",
      "                                                         n_estimators=5,\n",
      "                                                         random_state=3)),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(max_depth=12,\n",
      "                                                     n_estimators=10,\n",
      "                                                     random_state=3)),\n",
      "                             ('xgb',\n",
      "                              XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "                                            colsample_bylevel=1,\n",
      "                                            colsample_bynode=1,\n",
      "                                            colsample_bytree=1,\n",
      "                                            enable_categorical=False, gamma=0,\n",
      "                                            gpu...\n",
      "                                            interaction_constraints='',\n",
      "                                            learning_rate=0.300000012,\n",
      "                                            max_delta_step=0, max_depth=12,\n",
      "                                            max_leaf_nodes=None,\n",
      "                                            min_child_weight=1,\n",
      "                                            min_samples_split=1, missing=nan,\n",
      "                                            monotone_constraints='()',\n",
      "                                            n_estimators=10, n_jobs=8,\n",
      "                                            num_parallel_tree=1,\n",
      "                                            predictor='auto', random_state=3,\n",
      "                                            reg_alpha=0, reg_lambda=1,\n",
      "                                            scale_pos_weight=1, subsample=1,\n",
      "                                            tree_method='exact',\n",
      "                                            validate_parameters=1,\n",
      "                                            verbosity=None))])\n",
      "kfold  6  score: 90.14%\n",
      "kfold  6  score: 95.88%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21:47:28] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:47:28] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "VotingClassifier(estimators=[('gb',\n",
      "                              GradientBoostingClassifier(max_depth=12,\n",
      "                                                         min_samples_split=12,\n",
      "                                                         n_estimators=5,\n",
      "                                                         random_state=3)),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(max_depth=12,\n",
      "                                                     n_estimators=10,\n",
      "                                                     random_state=3)),\n",
      "                             ('xgb',\n",
      "                              XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "                                            colsample_bylevel=1,\n",
      "                                            colsample_bynode=1,\n",
      "                                            colsample_bytree=1,\n",
      "                                            enable_categorical=False, gamma=0,\n",
      "                                            gpu...\n",
      "                                            interaction_constraints='',\n",
      "                                            learning_rate=0.300000012,\n",
      "                                            max_delta_step=0, max_depth=12,\n",
      "                                            max_leaf_nodes=None,\n",
      "                                            min_child_weight=1,\n",
      "                                            min_samples_split=1, missing=nan,\n",
      "                                            monotone_constraints='()',\n",
      "                                            n_estimators=10, n_jobs=8,\n",
      "                                            num_parallel_tree=1,\n",
      "                                            predictor='auto', random_state=3,\n",
      "                                            reg_alpha=0, reg_lambda=1,\n",
      "                                            scale_pos_weight=1, subsample=1,\n",
      "                                            tree_method='exact',\n",
      "                                            validate_parameters=1,\n",
      "                                            verbosity=None))])\n",
      "kfold  6  score: 90.74%\n",
      "kfold  6  score: 96.06%\n",
      "[21:47:28] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:47:28] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "VotingClassifier(estimators=[('gb',\n",
      "                              GradientBoostingClassifier(max_depth=12,\n",
      "                                                         min_samples_split=12,\n",
      "                                                         n_estimators=5,\n",
      "                                                         random_state=3)),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(max_depth=12,\n",
      "                                                     n_estimators=10,\n",
      "                                                     random_state=3)),\n",
      "                             ('xgb',\n",
      "                              XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "                                            colsample_bylevel=1,\n",
      "                                            colsample_bynode=1,\n",
      "                                            colsample_bytree=1,\n",
      "                                            enable_categorical=False, gamma=0,\n",
      "                                            gpu...\n",
      "                                            interaction_constraints='',\n",
      "                                            learning_rate=0.300000012,\n",
      "                                            max_delta_step=0, max_depth=12,\n",
      "                                            max_leaf_nodes=None,\n",
      "                                            min_child_weight=1,\n",
      "                                            min_samples_split=1, missing=nan,\n",
      "                                            monotone_constraints='()',\n",
      "                                            n_estimators=10, n_jobs=8,\n",
      "                                            num_parallel_tree=1,\n",
      "                                            predictor='auto', random_state=3,\n",
      "                                            reg_alpha=0, reg_lambda=1,\n",
      "                                            scale_pos_weight=1, subsample=1,\n",
      "                                            tree_method='exact',\n",
      "                                            validate_parameters=1,\n",
      "                                            verbosity=None))])\n",
      "kfold  6  score: 91.15%\n",
      "kfold  6  score: 96.71%\n",
      "[21:47:29] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:47:29] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "VotingClassifier(estimators=[('gb',\n",
      "                              GradientBoostingClassifier(max_depth=12,\n",
      "                                                         min_samples_split=12,\n",
      "                                                         n_estimators=5,\n",
      "                                                         random_state=3)),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(max_depth=12,\n",
      "                                                     n_estimators=10,\n",
      "                                                     random_state=3)),\n",
      "                             ('xgb',\n",
      "                              XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "                                            colsample_bylevel=1,\n",
      "                                            colsample_bynode=1,\n",
      "                                            colsample_bytree=1,\n",
      "                                            enable_categorical=False, gamma=0,\n",
      "                                            gpu...\n",
      "                                            interaction_constraints='',\n",
      "                                            learning_rate=0.300000012,\n",
      "                                            max_delta_step=0, max_depth=12,\n",
      "                                            max_leaf_nodes=None,\n",
      "                                            min_child_weight=1,\n",
      "                                            min_samples_split=1, missing=nan,\n",
      "                                            monotone_constraints='()',\n",
      "                                            n_estimators=10, n_jobs=8,\n",
      "                                            num_parallel_tree=1,\n",
      "                                            predictor='auto', random_state=3,\n",
      "                                            reg_alpha=0, reg_lambda=1,\n",
      "                                            scale_pos_weight=1, subsample=1,\n",
      "                                            tree_method='exact',\n",
      "                                            validate_parameters=1,\n",
      "                                            verbosity=None))])\n",
      "kfold  6  score: 92.56%\n",
      "kfold  6  score: 95.55%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21:47:29] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:47:29] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "VotingClassifier(estimators=[('gb',\n",
      "                              GradientBoostingClassifier(max_depth=12,\n",
      "                                                         min_samples_split=12,\n",
      "                                                         n_estimators=5,\n",
      "                                                         random_state=3)),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(max_depth=12,\n",
      "                                                     n_estimators=10,\n",
      "                                                     random_state=3)),\n",
      "                             ('xgb',\n",
      "                              XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "                                            colsample_bylevel=1,\n",
      "                                            colsample_bynode=1,\n",
      "                                            colsample_bytree=1,\n",
      "                                            enable_categorical=False, gamma=0,\n",
      "                                            gpu...\n",
      "                                            interaction_constraints='',\n",
      "                                            learning_rate=0.300000012,\n",
      "                                            max_delta_step=0, max_depth=12,\n",
      "                                            max_leaf_nodes=None,\n",
      "                                            min_child_weight=1,\n",
      "                                            min_samples_split=1, missing=nan,\n",
      "                                            monotone_constraints='()',\n",
      "                                            n_estimators=10, n_jobs=8,\n",
      "                                            num_parallel_tree=1,\n",
      "                                            predictor='auto', random_state=3,\n",
      "                                            reg_alpha=0, reg_lambda=1,\n",
      "                                            scale_pos_weight=1, subsample=1,\n",
      "                                            tree_method='exact',\n",
      "                                            validate_parameters=1,\n",
      "                                            verbosity=None))])\n",
      "kfold  6  score: 91.73%\n",
      "kfold  6  score: 96.73%\n",
      "[21:47:30] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:47:30] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "VotingClassifier(estimators=[('gb',\n",
      "                              GradientBoostingClassifier(max_depth=12,\n",
      "                                                         min_samples_split=12,\n",
      "                                                         n_estimators=5,\n",
      "                                                         random_state=3)),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(max_depth=12,\n",
      "                                                     n_estimators=10,\n",
      "                                                     random_state=3)),\n",
      "                             ('xgb',\n",
      "                              XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "                                            colsample_bylevel=1,\n",
      "                                            colsample_bynode=1,\n",
      "                                            colsample_bytree=1,\n",
      "                                            enable_categorical=False, gamma=0,\n",
      "                                            gpu...\n",
      "                                            interaction_constraints='',\n",
      "                                            learning_rate=0.300000012,\n",
      "                                            max_delta_step=0, max_depth=12,\n",
      "                                            max_leaf_nodes=None,\n",
      "                                            min_child_weight=1,\n",
      "                                            min_samples_split=1, missing=nan,\n",
      "                                            monotone_constraints='()',\n",
      "                                            n_estimators=10, n_jobs=8,\n",
      "                                            num_parallel_tree=1,\n",
      "                                            predictor='auto', random_state=3,\n",
      "                                            reg_alpha=0, reg_lambda=1,\n",
      "                                            scale_pos_weight=1, subsample=1,\n",
      "                                            tree_method='exact',\n",
      "                                            validate_parameters=1,\n",
      "                                            verbosity=None))])\n",
      "kfold  6  score: 89.52%\n",
      "kfold  6  score: 95.88%\n",
      "[21:47:30] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:47:30] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "VotingClassifier(estimators=[('gb',\n",
      "                              GradientBoostingClassifier(max_depth=12,\n",
      "                                                         min_samples_split=12,\n",
      "                                                         n_estimators=5,\n",
      "                                                         random_state=3)),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(max_depth=12,\n",
      "                                                     n_estimators=10,\n",
      "                                                     random_state=3)),\n",
      "                             ('xgb',\n",
      "                              XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "                                            colsample_bylevel=1,\n",
      "                                            colsample_bynode=1,\n",
      "                                            colsample_bytree=1,\n",
      "                                            enable_categorical=False, gamma=0,\n",
      "                                            gpu...\n",
      "                                            interaction_constraints='',\n",
      "                                            learning_rate=0.300000012,\n",
      "                                            max_delta_step=0, max_depth=12,\n",
      "                                            max_leaf_nodes=None,\n",
      "                                            min_child_weight=1,\n",
      "                                            min_samples_split=1, missing=nan,\n",
      "                                            monotone_constraints='()',\n",
      "                                            n_estimators=10, n_jobs=8,\n",
      "                                            num_parallel_tree=1,\n",
      "                                            predictor='auto', random_state=3,\n",
      "                                            reg_alpha=0, reg_lambda=1,\n",
      "                                            scale_pos_weight=1, subsample=1,\n",
      "                                            tree_method='exact',\n",
      "                                            validate_parameters=1,\n",
      "                                            verbosity=None))])\n",
      "kfold  6  score: 91.94%\n",
      "kfold  6  score: 96.22%\n",
      "Average score over k-fold 91.28% (+/-1.14%) [Ensemble Majority]\n",
      "./Files_Majority_vote/128_features/filter7.csv\n",
      "n_estimators = 100 \n",
      "\n",
      "10-fold cross validation for file :  7\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GradientBoostingClassifier(max_depth=12, min_samples_split=12, n_estimators=5,\n",
      "                           random_state=3)\n",
      "kfold  7  score: 89.12%\n",
      "kfold  7  score: 93.89%\n",
      "GradientBoostingClassifier(max_depth=12, min_samples_split=12, n_estimators=5,\n",
      "                           random_state=3)\n",
      "kfold  7  score: 91.79%\n",
      "kfold  7  score: 94.42%\n",
      "GradientBoostingClassifier(max_depth=12, min_samples_split=12, n_estimators=5,\n",
      "                           random_state=3)\n",
      "kfold  7  score: 92.56%\n",
      "kfold  7  score: 93.95%\n",
      "GradientBoostingClassifier(max_depth=12, min_samples_split=12, n_estimators=5,\n",
      "                           random_state=3)\n",
      "kfold  7  score: 87.60%\n",
      "kfold  7  score: 92.42%\n",
      "GradientBoostingClassifier(max_depth=12, min_samples_split=12, n_estimators=5,\n",
      "                           random_state=3)\n",
      "kfold  7  score: 87.79%\n",
      "kfold  7  score: 93.55%\n",
      "GradientBoostingClassifier(max_depth=12, min_samples_split=12, n_estimators=5,\n",
      "                           random_state=3)\n",
      "kfold  7  score: 91.79%\n",
      "kfold  7  score: 94.40%\n",
      "GradientBoostingClassifier(max_depth=12, min_samples_split=12, n_estimators=5,\n",
      "                           random_state=3)\n",
      "kfold  7  score: 91.78%\n",
      "kfold  7  score: 92.83%\n",
      "GradientBoostingClassifier(max_depth=12, min_samples_split=12, n_estimators=5,\n",
      "                           random_state=3)\n",
      "kfold  7  score: 90.25%\n",
      "kfold  7  score: 94.44%\n",
      "GradientBoostingClassifier(max_depth=12, min_samples_split=12, n_estimators=5,\n",
      "                           random_state=3)\n",
      "kfold  7  score: 90.25%\n",
      "kfold  7  score: 93.89%\n",
      "GradientBoostingClassifier(max_depth=12, min_samples_split=12, n_estimators=5,\n",
      "                           random_state=3)\n",
      "kfold  7  score: 88.34%\n",
      "kfold  7  score: 92.93%\n",
      "Average score over k-fold 90.13% (+/-1.74%) [Gradient Boosting]\n",
      "RandomForestClassifier(max_depth=12, n_estimators=10, random_state=3)\n",
      "kfold  7  score: 91.98%\n",
      "kfold  7  score: 97.30%\n",
      "RandomForestClassifier(max_depth=12, n_estimators=10, random_state=3)\n",
      "kfold  7  score: 94.08%\n",
      "kfold  7  score: 98.49%\n",
      "RandomForestClassifier(max_depth=12, n_estimators=10, random_state=3)\n",
      "kfold  7  score: 92.56%\n",
      "kfold  7  score: 96.48%\n",
      "RandomForestClassifier(max_depth=12, n_estimators=10, random_state=3)\n",
      "kfold  7  score: 91.98%\n",
      "kfold  7  score: 97.30%\n",
      "RandomForestClassifier(max_depth=12, n_estimators=10, random_state=3)\n",
      "kfold  7  score: 93.13%\n",
      "kfold  7  score: 98.49%\n",
      "RandomForestClassifier(max_depth=12, n_estimators=10, random_state=3)\n",
      "kfold  7  score: 94.08%\n",
      "kfold  7  score: 96.97%\n",
      "RandomForestClassifier(max_depth=12, n_estimators=10, random_state=3)\n",
      "kfold  7  score: 94.65%\n",
      "kfold  7  score: 97.52%\n",
      "RandomForestClassifier(max_depth=12, n_estimators=10, random_state=3)\n",
      "kfold  7  score: 91.97%\n",
      "kfold  7  score: 96.97%\n",
      "RandomForestClassifier(max_depth=12, n_estimators=10, random_state=3)\n",
      "kfold  7  score: 92.54%\n",
      "kfold  7  score: 98.75%\n",
      "RandomForestClassifier(max_depth=12, n_estimators=10, random_state=3)\n",
      "kfold  7  score: 93.12%\n",
      "kfold  7  score: 97.67%\n",
      "Average score over k-fold 93.01% (+/-0.93%) [Random Forest]\n",
      "[21:47:34] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:47:34] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
      "              gamma=0, gpu_id=-1, importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.300000012,\n",
      "              max_delta_step=0, max_depth=12, max_leaf_nodes=None,\n",
      "              min_child_weight=1, min_samples_split=1, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=10, n_jobs=8,\n",
      "              num_parallel_tree=1, predictor='auto', random_state=3,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)\n",
      "kfold  7  score: 93.51%\n",
      "kfold  7  score: 99.13%\n",
      "[21:47:34] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:47:34] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
      "              gamma=0, gpu_id=-1, importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.300000012,\n",
      "              max_delta_step=0, max_depth=12, max_leaf_nodes=None,\n",
      "              min_child_weight=1, min_samples_split=1, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=10, n_jobs=8,\n",
      "              num_parallel_tree=1, predictor='auto', random_state=3,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)\n",
      "kfold  7  score: 95.04%\n",
      "kfold  7  score: 99.28%\n",
      "[21:47:34] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:47:34] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
      "              gamma=0, gpu_id=-1, importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.300000012,\n",
      "              max_delta_step=0, max_depth=12, max_leaf_nodes=None,\n",
      "              min_child_weight=1, min_samples_split=1, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=10, n_jobs=8,\n",
      "              num_parallel_tree=1, predictor='auto', random_state=3,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)\n",
      "kfold  7  score: 94.85%\n",
      "kfold  7  score: 99.32%\n",
      "[21:47:34] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:47:34] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
      "              gamma=0, gpu_id=-1, importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.300000012,\n",
      "              max_delta_step=0, max_depth=12, max_leaf_nodes=None,\n",
      "              min_child_weight=1, min_samples_split=1, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=10, n_jobs=8,\n",
      "              num_parallel_tree=1, predictor='auto', random_state=3,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)\n",
      "kfold  7  score: 92.94%\n",
      "kfold  7  score: 99.45%\n",
      "[21:47:34] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:47:34] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
      "              gamma=0, gpu_id=-1, importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.300000012,\n",
      "              max_delta_step=0, max_depth=12, max_leaf_nodes=None,\n",
      "              min_child_weight=1, min_samples_split=1, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=10, n_jobs=8,\n",
      "              num_parallel_tree=1, predictor='auto', random_state=3,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)\n",
      "kfold  7  score: 92.75%\n",
      "kfold  7  score: 99.49%\n",
      "[21:47:34] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:47:34] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
      "              gamma=0, gpu_id=-1, importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.300000012,\n",
      "              max_delta_step=0, max_depth=12, max_leaf_nodes=None,\n",
      "              min_child_weight=1, min_samples_split=1, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=10, n_jobs=8,\n",
      "              num_parallel_tree=1, predictor='auto', random_state=3,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)\n",
      "kfold  7  score: 94.85%\n",
      "kfold  7  score: 99.51%\n",
      "[21:47:34] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:47:34] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
      "              gamma=0, gpu_id=-1, importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.300000012,\n",
      "              max_delta_step=0, max_depth=12, max_leaf_nodes=None,\n",
      "              min_child_weight=1, min_samples_split=1, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=10, n_jobs=8,\n",
      "              num_parallel_tree=1, predictor='auto', random_state=3,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)\n",
      "kfold  7  score: 96.56%\n",
      "kfold  7  score: 99.19%\n",
      "[21:47:35] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:47:35] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
      "              gamma=0, gpu_id=-1, importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.300000012,\n",
      "              max_delta_step=0, max_depth=12, max_leaf_nodes=None,\n",
      "              min_child_weight=1, min_samples_split=1, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=10, n_jobs=8,\n",
      "              num_parallel_tree=1, predictor='auto', random_state=3,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)\n",
      "kfold  7  score: 95.22%\n",
      "kfold  7  score: 99.30%\n",
      "[21:47:35] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:47:35] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
      "              gamma=0, gpu_id=-1, importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.300000012,\n",
      "              max_delta_step=0, max_depth=12, max_leaf_nodes=None,\n",
      "              min_child_weight=1, min_samples_split=1, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=10, n_jobs=8,\n",
      "              num_parallel_tree=1, predictor='auto', random_state=3,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)\n",
      "kfold  7  score: 92.93%\n",
      "kfold  7  score: 99.53%\n",
      "[21:47:35] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:47:35] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
      "              gamma=0, gpu_id=-1, importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.300000012,\n",
      "              max_delta_step=0, max_depth=12, max_leaf_nodes=None,\n",
      "              min_child_weight=1, min_samples_split=1, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=10, n_jobs=8,\n",
      "              num_parallel_tree=1, predictor='auto', random_state=3,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)\n",
      "kfold  7  score: 93.88%\n",
      "kfold  7  score: 99.43%\n",
      "Average score over k-fold 94.25% (+/-1.18%) [XGBClassifier]\n",
      "[21:47:35] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:47:35] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "VotingClassifier(estimators=[('gb',\n",
      "                              GradientBoostingClassifier(max_depth=12,\n",
      "                                                         min_samples_split=12,\n",
      "                                                         n_estimators=5,\n",
      "                                                         random_state=3)),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(max_depth=12,\n",
      "                                                     n_estimators=10,\n",
      "                                                     random_state=3)),\n",
      "                             ('xgb',\n",
      "                              XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "                                            colsample_bylevel=1,\n",
      "                                            colsample_bynode=1,\n",
      "                                            colsample_bytree=1,\n",
      "                                            enable_categorical=False, gamma=0,\n",
      "                                            gpu...\n",
      "                                            interaction_constraints='',\n",
      "                                            learning_rate=0.300000012,\n",
      "                                            max_delta_step=0, max_depth=12,\n",
      "                                            max_leaf_nodes=None,\n",
      "                                            min_child_weight=1,\n",
      "                                            min_samples_split=1, missing=nan,\n",
      "                                            monotone_constraints='()',\n",
      "                                            n_estimators=10, n_jobs=8,\n",
      "                                            num_parallel_tree=1,\n",
      "                                            predictor='auto', random_state=3,\n",
      "                                            reg_alpha=0, reg_lambda=1,\n",
      "                                            scale_pos_weight=1, subsample=1,\n",
      "                                            tree_method='exact',\n",
      "                                            validate_parameters=1,\n",
      "                                            verbosity=None))])\n",
      "kfold  7  score: 92.18%\n",
      "kfold  7  score: 97.62%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21:47:36] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:47:36] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "VotingClassifier(estimators=[('gb',\n",
      "                              GradientBoostingClassifier(max_depth=12,\n",
      "                                                         min_samples_split=12,\n",
      "                                                         n_estimators=5,\n",
      "                                                         random_state=3)),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(max_depth=12,\n",
      "                                                     n_estimators=10,\n",
      "                                                     random_state=3)),\n",
      "                             ('xgb',\n",
      "                              XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "                                            colsample_bylevel=1,\n",
      "                                            colsample_bynode=1,\n",
      "                                            colsample_bytree=1,\n",
      "                                            enable_categorical=False, gamma=0,\n",
      "                                            gpu...\n",
      "                                            interaction_constraints='',\n",
      "                                            learning_rate=0.300000012,\n",
      "                                            max_delta_step=0, max_depth=12,\n",
      "                                            max_leaf_nodes=None,\n",
      "                                            min_child_weight=1,\n",
      "                                            min_samples_split=1, missing=nan,\n",
      "                                            monotone_constraints='()',\n",
      "                                            n_estimators=10, n_jobs=8,\n",
      "                                            num_parallel_tree=1,\n",
      "                                            predictor='auto', random_state=3,\n",
      "                                            reg_alpha=0, reg_lambda=1,\n",
      "                                            scale_pos_weight=1, subsample=1,\n",
      "                                            tree_method='exact',\n",
      "                                            validate_parameters=1,\n",
      "                                            verbosity=None))])\n",
      "kfold  7  score: 95.23%\n",
      "kfold  7  score: 98.68%\n",
      "[21:47:36] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:47:36] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "VotingClassifier(estimators=[('gb',\n",
      "                              GradientBoostingClassifier(max_depth=12,\n",
      "                                                         min_samples_split=12,\n",
      "                                                         n_estimators=5,\n",
      "                                                         random_state=3)),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(max_depth=12,\n",
      "                                                     n_estimators=10,\n",
      "                                                     random_state=3)),\n",
      "                             ('xgb',\n",
      "                              XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "                                            colsample_bylevel=1,\n",
      "                                            colsample_bynode=1,\n",
      "                                            colsample_bytree=1,\n",
      "                                            enable_categorical=False, gamma=0,\n",
      "                                            gpu...\n",
      "                                            interaction_constraints='',\n",
      "                                            learning_rate=0.300000012,\n",
      "                                            max_delta_step=0, max_depth=12,\n",
      "                                            max_leaf_nodes=None,\n",
      "                                            min_child_weight=1,\n",
      "                                            min_samples_split=1, missing=nan,\n",
      "                                            monotone_constraints='()',\n",
      "                                            n_estimators=10, n_jobs=8,\n",
      "                                            num_parallel_tree=1,\n",
      "                                            predictor='auto', random_state=3,\n",
      "                                            reg_alpha=0, reg_lambda=1,\n",
      "                                            scale_pos_weight=1, subsample=1,\n",
      "                                            tree_method='exact',\n",
      "                                            validate_parameters=1,\n",
      "                                            verbosity=None))])\n",
      "kfold  7  score: 95.23%\n",
      "kfold  7  score: 97.84%\n",
      "[21:47:37] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:47:37] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "VotingClassifier(estimators=[('gb',\n",
      "                              GradientBoostingClassifier(max_depth=12,\n",
      "                                                         min_samples_split=12,\n",
      "                                                         n_estimators=5,\n",
      "                                                         random_state=3)),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(max_depth=12,\n",
      "                                                     n_estimators=10,\n",
      "                                                     random_state=3)),\n",
      "                             ('xgb',\n",
      "                              XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "                                            colsample_bylevel=1,\n",
      "                                            colsample_bynode=1,\n",
      "                                            colsample_bytree=1,\n",
      "                                            enable_categorical=False, gamma=0,\n",
      "                                            gpu...\n",
      "                                            interaction_constraints='',\n",
      "                                            learning_rate=0.300000012,\n",
      "                                            max_delta_step=0, max_depth=12,\n",
      "                                            max_leaf_nodes=None,\n",
      "                                            min_child_weight=1,\n",
      "                                            min_samples_split=1, missing=nan,\n",
      "                                            monotone_constraints='()',\n",
      "                                            n_estimators=10, n_jobs=8,\n",
      "                                            num_parallel_tree=1,\n",
      "                                            predictor='auto', random_state=3,\n",
      "                                            reg_alpha=0, reg_lambda=1,\n",
      "                                            scale_pos_weight=1, subsample=1,\n",
      "                                            tree_method='exact',\n",
      "                                            validate_parameters=1,\n",
      "                                            verbosity=None))])\n",
      "kfold  7  score: 91.22%\n",
      "kfold  7  score: 97.28%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21:47:37] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:47:37] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "VotingClassifier(estimators=[('gb',\n",
      "                              GradientBoostingClassifier(max_depth=12,\n",
      "                                                         min_samples_split=12,\n",
      "                                                         n_estimators=5,\n",
      "                                                         random_state=3)),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(max_depth=12,\n",
      "                                                     n_estimators=10,\n",
      "                                                     random_state=3)),\n",
      "                             ('xgb',\n",
      "                              XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "                                            colsample_bylevel=1,\n",
      "                                            colsample_bynode=1,\n",
      "                                            colsample_bytree=1,\n",
      "                                            enable_categorical=False, gamma=0,\n",
      "                                            gpu...\n",
      "                                            interaction_constraints='',\n",
      "                                            learning_rate=0.300000012,\n",
      "                                            max_delta_step=0, max_depth=12,\n",
      "                                            max_leaf_nodes=None,\n",
      "                                            min_child_weight=1,\n",
      "                                            min_samples_split=1, missing=nan,\n",
      "                                            monotone_constraints='()',\n",
      "                                            n_estimators=10, n_jobs=8,\n",
      "                                            num_parallel_tree=1,\n",
      "                                            predictor='auto', random_state=3,\n",
      "                                            reg_alpha=0, reg_lambda=1,\n",
      "                                            scale_pos_weight=1, subsample=1,\n",
      "                                            tree_method='exact',\n",
      "                                            validate_parameters=1,\n",
      "                                            verbosity=None))])\n",
      "kfold  7  score: 92.75%\n",
      "kfold  7  score: 98.60%\n",
      "[21:47:38] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:47:38] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "VotingClassifier(estimators=[('gb',\n",
      "                              GradientBoostingClassifier(max_depth=12,\n",
      "                                                         min_samples_split=12,\n",
      "                                                         n_estimators=5,\n",
      "                                                         random_state=3)),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(max_depth=12,\n",
      "                                                     n_estimators=10,\n",
      "                                                     random_state=3)),\n",
      "                             ('xgb',\n",
      "                              XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "                                            colsample_bylevel=1,\n",
      "                                            colsample_bynode=1,\n",
      "                                            colsample_bytree=1,\n",
      "                                            enable_categorical=False, gamma=0,\n",
      "                                            gpu...\n",
      "                                            interaction_constraints='',\n",
      "                                            learning_rate=0.300000012,\n",
      "                                            max_delta_step=0, max_depth=12,\n",
      "                                            max_leaf_nodes=None,\n",
      "                                            min_child_weight=1,\n",
      "                                            min_samples_split=1, missing=nan,\n",
      "                                            monotone_constraints='()',\n",
      "                                            n_estimators=10, n_jobs=8,\n",
      "                                            num_parallel_tree=1,\n",
      "                                            predictor='auto', random_state=3,\n",
      "                                            reg_alpha=0, reg_lambda=1,\n",
      "                                            scale_pos_weight=1, subsample=1,\n",
      "                                            tree_method='exact',\n",
      "                                            validate_parameters=1,\n",
      "                                            verbosity=None))])\n",
      "kfold  7  score: 94.27%\n",
      "kfold  7  score: 97.77%\n",
      "[21:47:38] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:47:38] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "VotingClassifier(estimators=[('gb',\n",
      "                              GradientBoostingClassifier(max_depth=12,\n",
      "                                                         min_samples_split=12,\n",
      "                                                         n_estimators=5,\n",
      "                                                         random_state=3)),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(max_depth=12,\n",
      "                                                     n_estimators=10,\n",
      "                                                     random_state=3)),\n",
      "                             ('xgb',\n",
      "                              XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "                                            colsample_bylevel=1,\n",
      "                                            colsample_bynode=1,\n",
      "                                            colsample_bytree=1,\n",
      "                                            enable_categorical=False, gamma=0,\n",
      "                                            gpu...\n",
      "                                            interaction_constraints='',\n",
      "                                            learning_rate=0.300000012,\n",
      "                                            max_delta_step=0, max_depth=12,\n",
      "                                            max_leaf_nodes=None,\n",
      "                                            min_child_weight=1,\n",
      "                                            min_samples_split=1, missing=nan,\n",
      "                                            monotone_constraints='()',\n",
      "                                            n_estimators=10, n_jobs=8,\n",
      "                                            num_parallel_tree=1,\n",
      "                                            predictor='auto', random_state=3,\n",
      "                                            reg_alpha=0, reg_lambda=1,\n",
      "                                            scale_pos_weight=1, subsample=1,\n",
      "                                            tree_method='exact',\n",
      "                                            validate_parameters=1,\n",
      "                                            verbosity=None))])\n",
      "kfold  7  score: 94.65%\n",
      "kfold  7  score: 97.56%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21:47:39] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:47:39] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "VotingClassifier(estimators=[('gb',\n",
      "                              GradientBoostingClassifier(max_depth=12,\n",
      "                                                         min_samples_split=12,\n",
      "                                                         n_estimators=5,\n",
      "                                                         random_state=3)),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(max_depth=12,\n",
      "                                                     n_estimators=10,\n",
      "                                                     random_state=3)),\n",
      "                             ('xgb',\n",
      "                              XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "                                            colsample_bylevel=1,\n",
      "                                            colsample_bynode=1,\n",
      "                                            colsample_bytree=1,\n",
      "                                            enable_categorical=False, gamma=0,\n",
      "                                            gpu...\n",
      "                                            interaction_constraints='',\n",
      "                                            learning_rate=0.300000012,\n",
      "                                            max_delta_step=0, max_depth=12,\n",
      "                                            max_leaf_nodes=None,\n",
      "                                            min_child_weight=1,\n",
      "                                            min_samples_split=1, missing=nan,\n",
      "                                            monotone_constraints='()',\n",
      "                                            n_estimators=10, n_jobs=8,\n",
      "                                            num_parallel_tree=1,\n",
      "                                            predictor='auto', random_state=3,\n",
      "                                            reg_alpha=0, reg_lambda=1,\n",
      "                                            scale_pos_weight=1, subsample=1,\n",
      "                                            tree_method='exact',\n",
      "                                            validate_parameters=1,\n",
      "                                            verbosity=None))])\n",
      "kfold  7  score: 93.12%\n",
      "kfold  7  score: 97.69%\n",
      "[21:47:39] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:47:39] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "VotingClassifier(estimators=[('gb',\n",
      "                              GradientBoostingClassifier(max_depth=12,\n",
      "                                                         min_samples_split=12,\n",
      "                                                         n_estimators=5,\n",
      "                                                         random_state=3)),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(max_depth=12,\n",
      "                                                     n_estimators=10,\n",
      "                                                     random_state=3)),\n",
      "                             ('xgb',\n",
      "                              XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "                                            colsample_bylevel=1,\n",
      "                                            colsample_bynode=1,\n",
      "                                            colsample_bytree=1,\n",
      "                                            enable_categorical=False, gamma=0,\n",
      "                                            gpu...\n",
      "                                            interaction_constraints='',\n",
      "                                            learning_rate=0.300000012,\n",
      "                                            max_delta_step=0, max_depth=12,\n",
      "                                            max_leaf_nodes=None,\n",
      "                                            min_child_weight=1,\n",
      "                                            min_samples_split=1, missing=nan,\n",
      "                                            monotone_constraints='()',\n",
      "                                            n_estimators=10, n_jobs=8,\n",
      "                                            num_parallel_tree=1,\n",
      "                                            predictor='auto', random_state=3,\n",
      "                                            reg_alpha=0, reg_lambda=1,\n",
      "                                            scale_pos_weight=1, subsample=1,\n",
      "                                            tree_method='exact',\n",
      "                                            validate_parameters=1,\n",
      "                                            verbosity=None))])\n",
      "kfold  7  score: 92.93%\n",
      "kfold  7  score: 98.81%\n",
      "[21:47:40] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:47:40] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "VotingClassifier(estimators=[('gb',\n",
      "                              GradientBoostingClassifier(max_depth=12,\n",
      "                                                         min_samples_split=12,\n",
      "                                                         n_estimators=5,\n",
      "                                                         random_state=3)),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(max_depth=12,\n",
      "                                                     n_estimators=10,\n",
      "                                                     random_state=3)),\n",
      "                             ('xgb',\n",
      "                              XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "                                            colsample_bylevel=1,\n",
      "                                            colsample_bynode=1,\n",
      "                                            colsample_bytree=1,\n",
      "                                            enable_categorical=False, gamma=0,\n",
      "                                            gpu...\n",
      "                                            interaction_constraints='',\n",
      "                                            learning_rate=0.300000012,\n",
      "                                            max_delta_step=0, max_depth=12,\n",
      "                                            max_leaf_nodes=None,\n",
      "                                            min_child_weight=1,\n",
      "                                            min_samples_split=1, missing=nan,\n",
      "                                            monotone_constraints='()',\n",
      "                                            n_estimators=10, n_jobs=8,\n",
      "                                            num_parallel_tree=1,\n",
      "                                            predictor='auto', random_state=3,\n",
      "                                            reg_alpha=0, reg_lambda=1,\n",
      "                                            scale_pos_weight=1, subsample=1,\n",
      "                                            tree_method='exact',\n",
      "                                            validate_parameters=1,\n",
      "                                            verbosity=None))])\n",
      "kfold  7  score: 92.93%\n",
      "kfold  7  score: 97.71%\n",
      "Average score over k-fold 93.45% (+/-1.27%) [Ensemble Majority]\n",
      "./Files_Majority_vote/128_features/filter8.csv\n",
      "n_estimators = 100 \n",
      "\n",
      "10-fold cross validation for file :  8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GradientBoostingClassifier(max_depth=12, min_samples_split=12, n_estimators=5,\n",
      "                           random_state=3)\n",
      "kfold  8  score: 93.16%\n",
      "kfold  8  score: 97.76%\n",
      "GradientBoostingClassifier(max_depth=12, min_samples_split=12, n_estimators=5,\n",
      "                           random_state=3)\n",
      "kfold  8  score: 94.16%\n",
      "kfold  8  score: 97.83%\n",
      "GradientBoostingClassifier(max_depth=12, min_samples_split=12, n_estimators=5,\n",
      "                           random_state=3)\n",
      "kfold  8  score: 94.97%\n",
      "kfold  8  score: 97.67%\n",
      "GradientBoostingClassifier(max_depth=12, min_samples_split=12, n_estimators=5,\n",
      "                           random_state=3)\n",
      "kfold  8  score: 93.36%\n",
      "kfold  8  score: 97.76%\n",
      "GradientBoostingClassifier(max_depth=12, min_samples_split=12, n_estimators=5,\n",
      "                           random_state=3)\n",
      "kfold  8  score: 94.97%\n",
      "kfold  8  score: 98.26%\n",
      "GradientBoostingClassifier(max_depth=12, min_samples_split=12, n_estimators=5,\n",
      "                           random_state=3)\n",
      "kfold  8  score: 95.17%\n",
      "kfold  8  score: 97.18%\n",
      "GradientBoostingClassifier(max_depth=12, min_samples_split=12, n_estimators=5,\n",
      "                           random_state=3)\n",
      "kfold  8  score: 93.16%\n",
      "kfold  8  score: 97.99%\n",
      "GradientBoostingClassifier(max_depth=12, min_samples_split=12, n_estimators=5,\n",
      "                           random_state=3)\n",
      "kfold  8  score: 94.76%\n",
      "kfold  8  score: 98.28%\n",
      "GradientBoostingClassifier(max_depth=12, min_samples_split=12, n_estimators=5,\n",
      "                           random_state=3)\n",
      "kfold  8  score: 95.97%\n",
      "kfold  8  score: 97.74%\n",
      "GradientBoostingClassifier(max_depth=12, min_samples_split=12, n_estimators=5,\n",
      "                           random_state=3)\n",
      "kfold  8  score: 95.56%\n",
      "kfold  8  score: 97.14%\n",
      "Average score over k-fold 94.52% (+/-0.96%) [Gradient Boosting]\n",
      "RandomForestClassifier(max_depth=12, n_estimators=10, random_state=3)\n",
      "kfold  8  score: 95.77%\n",
      "kfold  8  score: 99.60%\n",
      "RandomForestClassifier(max_depth=12, n_estimators=10, random_state=3)\n",
      "kfold  8  score: 96.58%\n",
      "kfold  8  score: 99.49%\n",
      "RandomForestClassifier(max_depth=12, n_estimators=10, random_state=3)\n",
      "kfold  8  score: 96.98%\n",
      "kfold  8  score: 99.46%\n",
      "RandomForestClassifier(max_depth=12, n_estimators=10, random_state=3)\n",
      "kfold  8  score: 96.78%\n",
      "kfold  8  score: 99.49%\n",
      "RandomForestClassifier(max_depth=12, n_estimators=10, random_state=3)\n",
      "kfold  8  score: 95.57%\n",
      "kfold  8  score: 99.17%\n",
      "RandomForestClassifier(max_depth=12, n_estimators=10, random_state=3)\n",
      "kfold  8  score: 96.38%\n",
      "kfold  8  score: 99.19%\n",
      "RandomForestClassifier(max_depth=12, n_estimators=10, random_state=3)\n",
      "kfold  8  score: 95.77%\n",
      "kfold  8  score: 99.42%\n",
      "RandomForestClassifier(max_depth=12, n_estimators=10, random_state=3)\n",
      "kfold  8  score: 96.37%\n",
      "kfold  8  score: 99.37%\n",
      "RandomForestClassifier(max_depth=12, n_estimators=10, random_state=3)\n",
      "kfold  8  score: 97.98%\n",
      "kfold  8  score: 99.60%\n",
      "RandomForestClassifier(max_depth=12, n_estimators=10, random_state=3)\n",
      "kfold  8  score: 97.58%\n",
      "kfold  8  score: 99.55%\n",
      "Average score over k-fold 96.58% (+/-0.75%) [Random Forest]\n",
      "[21:47:43] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:47:43] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
      "              gamma=0, gpu_id=-1, importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.300000012,\n",
      "              max_delta_step=0, max_depth=12, max_leaf_nodes=None,\n",
      "              min_child_weight=1, min_samples_split=1, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=10, n_jobs=8,\n",
      "              num_parallel_tree=1, predictor='auto', random_state=3,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)\n",
      "kfold  8  score: 96.18%\n",
      "kfold  8  score: 99.62%\n",
      "[21:47:43] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:47:43] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
      "              gamma=0, gpu_id=-1, importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.300000012,\n",
      "              max_delta_step=0, max_depth=12, max_leaf_nodes=None,\n",
      "              min_child_weight=1, min_samples_split=1, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=10, n_jobs=8,\n",
      "              num_parallel_tree=1, predictor='auto', random_state=3,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)\n",
      "kfold  8  score: 96.98%\n",
      "kfold  8  score: 99.71%\n",
      "[21:47:43] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:47:43] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
      "              gamma=0, gpu_id=-1, importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.300000012,\n",
      "              max_delta_step=0, max_depth=12, max_leaf_nodes=None,\n",
      "              min_child_weight=1, min_samples_split=1, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=10, n_jobs=8,\n",
      "              num_parallel_tree=1, predictor='auto', random_state=3,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)\n",
      "kfold  8  score: 98.19%\n",
      "kfold  8  score: 99.66%\n",
      "[21:47:43] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:47:43] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
      "              gamma=0, gpu_id=-1, importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.300000012,\n",
      "              max_delta_step=0, max_depth=12, max_leaf_nodes=None,\n",
      "              min_child_weight=1, min_samples_split=1, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=10, n_jobs=8,\n",
      "              num_parallel_tree=1, predictor='auto', random_state=3,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)\n",
      "kfold  8  score: 97.59%\n",
      "kfold  8  score: 99.69%\n",
      "[21:47:43] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:47:43] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
      "              gamma=0, gpu_id=-1, importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.300000012,\n",
      "              max_delta_step=0, max_depth=12, max_leaf_nodes=None,\n",
      "              min_child_weight=1, min_samples_split=1, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=10, n_jobs=8,\n",
      "              num_parallel_tree=1, predictor='auto', random_state=3,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)\n",
      "kfold  8  score: 95.57%\n",
      "kfold  8  score: 99.66%\n",
      "[21:47:43] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:47:43] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
      "              gamma=0, gpu_id=-1, importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.300000012,\n",
      "              max_delta_step=0, max_depth=12, max_leaf_nodes=None,\n",
      "              min_child_weight=1, min_samples_split=1, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=10, n_jobs=8,\n",
      "              num_parallel_tree=1, predictor='auto', random_state=3,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)\n",
      "kfold  8  score: 97.38%\n",
      "kfold  8  score: 99.78%\n",
      "[21:47:44] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:47:44] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
      "              gamma=0, gpu_id=-1, importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.300000012,\n",
      "              max_delta_step=0, max_depth=12, max_leaf_nodes=None,\n",
      "              min_child_weight=1, min_samples_split=1, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=10, n_jobs=8,\n",
      "              num_parallel_tree=1, predictor='auto', random_state=3,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)\n",
      "kfold  8  score: 96.38%\n",
      "kfold  8  score: 99.69%\n",
      "[21:47:44] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:47:44] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
      "              gamma=0, gpu_id=-1, importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.300000012,\n",
      "              max_delta_step=0, max_depth=12, max_leaf_nodes=None,\n",
      "              min_child_weight=1, min_samples_split=1, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=10, n_jobs=8,\n",
      "              num_parallel_tree=1, predictor='auto', random_state=3,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)\n",
      "kfold  8  score: 96.57%\n",
      "kfold  8  score: 99.66%\n",
      "[21:47:44] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:47:44] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
      "              gamma=0, gpu_id=-1, importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.300000012,\n",
      "              max_delta_step=0, max_depth=12, max_leaf_nodes=None,\n",
      "              min_child_weight=1, min_samples_split=1, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=10, n_jobs=8,\n",
      "              num_parallel_tree=1, predictor='auto', random_state=3,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)\n",
      "kfold  8  score: 98.19%\n",
      "kfold  8  score: 99.82%\n",
      "[21:47:44] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:47:44] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
      "              gamma=0, gpu_id=-1, importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.300000012,\n",
      "              max_delta_step=0, max_depth=12, max_leaf_nodes=None,\n",
      "              min_child_weight=1, min_samples_split=1, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=10, n_jobs=8,\n",
      "              num_parallel_tree=1, predictor='auto', random_state=3,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)\n",
      "kfold  8  score: 98.79%\n",
      "kfold  8  score: 99.80%\n",
      "Average score over k-fold 97.18% (+/-0.97%) [XGBClassifier]\n",
      "[21:47:44] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:47:44] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "VotingClassifier(estimators=[('gb',\n",
      "                              GradientBoostingClassifier(max_depth=12,\n",
      "                                                         min_samples_split=12,\n",
      "                                                         n_estimators=5,\n",
      "                                                         random_state=3)),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(max_depth=12,\n",
      "                                                     n_estimators=10,\n",
      "                                                     random_state=3)),\n",
      "                             ('xgb',\n",
      "                              XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "                                            colsample_bylevel=1,\n",
      "                                            colsample_bynode=1,\n",
      "                                            colsample_bytree=1,\n",
      "                                            enable_categorical=False, gamma=0,\n",
      "                                            gpu...\n",
      "                                            interaction_constraints='',\n",
      "                                            learning_rate=0.300000012,\n",
      "                                            max_delta_step=0, max_depth=12,\n",
      "                                            max_leaf_nodes=None,\n",
      "                                            min_child_weight=1,\n",
      "                                            min_samples_split=1, missing=nan,\n",
      "                                            monotone_constraints='()',\n",
      "                                            n_estimators=10, n_jobs=8,\n",
      "                                            num_parallel_tree=1,\n",
      "                                            predictor='auto', random_state=3,\n",
      "                                            reg_alpha=0, reg_lambda=1,\n",
      "                                            scale_pos_weight=1, subsample=1,\n",
      "                                            tree_method='exact',\n",
      "                                            validate_parameters=1,\n",
      "                                            verbosity=None))])\n",
      "kfold  8  score: 95.98%\n",
      "kfold  8  score: 99.55%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21:47:45] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:47:45] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "VotingClassifier(estimators=[('gb',\n",
      "                              GradientBoostingClassifier(max_depth=12,\n",
      "                                                         min_samples_split=12,\n",
      "                                                         n_estimators=5,\n",
      "                                                         random_state=3)),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(max_depth=12,\n",
      "                                                     n_estimators=10,\n",
      "                                                     random_state=3)),\n",
      "                             ('xgb',\n",
      "                              XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "                                            colsample_bylevel=1,\n",
      "                                            colsample_bynode=1,\n",
      "                                            colsample_bytree=1,\n",
      "                                            enable_categorical=False, gamma=0,\n",
      "                                            gpu...\n",
      "                                            interaction_constraints='',\n",
      "                                            learning_rate=0.300000012,\n",
      "                                            max_delta_step=0, max_depth=12,\n",
      "                                            max_leaf_nodes=None,\n",
      "                                            min_child_weight=1,\n",
      "                                            min_samples_split=1, missing=nan,\n",
      "                                            monotone_constraints='()',\n",
      "                                            n_estimators=10, n_jobs=8,\n",
      "                                            num_parallel_tree=1,\n",
      "                                            predictor='auto', random_state=3,\n",
      "                                            reg_alpha=0, reg_lambda=1,\n",
      "                                            scale_pos_weight=1, subsample=1,\n",
      "                                            tree_method='exact',\n",
      "                                            validate_parameters=1,\n",
      "                                            verbosity=None))])\n",
      "kfold  8  score: 96.78%\n",
      "kfold  8  score: 99.62%\n",
      "[21:47:45] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:47:45] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "VotingClassifier(estimators=[('gb',\n",
      "                              GradientBoostingClassifier(max_depth=12,\n",
      "                                                         min_samples_split=12,\n",
      "                                                         n_estimators=5,\n",
      "                                                         random_state=3)),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(max_depth=12,\n",
      "                                                     n_estimators=10,\n",
      "                                                     random_state=3)),\n",
      "                             ('xgb',\n",
      "                              XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "                                            colsample_bylevel=1,\n",
      "                                            colsample_bynode=1,\n",
      "                                            colsample_bytree=1,\n",
      "                                            enable_categorical=False, gamma=0,\n",
      "                                            gpu...\n",
      "                                            interaction_constraints='',\n",
      "                                            learning_rate=0.300000012,\n",
      "                                            max_delta_step=0, max_depth=12,\n",
      "                                            max_leaf_nodes=None,\n",
      "                                            min_child_weight=1,\n",
      "                                            min_samples_split=1, missing=nan,\n",
      "                                            monotone_constraints='()',\n",
      "                                            n_estimators=10, n_jobs=8,\n",
      "                                            num_parallel_tree=1,\n",
      "                                            predictor='auto', random_state=3,\n",
      "                                            reg_alpha=0, reg_lambda=1,\n",
      "                                            scale_pos_weight=1, subsample=1,\n",
      "                                            tree_method='exact',\n",
      "                                            validate_parameters=1,\n",
      "                                            verbosity=None))])\n",
      "kfold  8  score: 97.38%\n",
      "kfold  8  score: 99.51%\n",
      "[21:47:46] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:47:46] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "VotingClassifier(estimators=[('gb',\n",
      "                              GradientBoostingClassifier(max_depth=12,\n",
      "                                                         min_samples_split=12,\n",
      "                                                         n_estimators=5,\n",
      "                                                         random_state=3)),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(max_depth=12,\n",
      "                                                     n_estimators=10,\n",
      "                                                     random_state=3)),\n",
      "                             ('xgb',\n",
      "                              XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "                                            colsample_bylevel=1,\n",
      "                                            colsample_bynode=1,\n",
      "                                            colsample_bytree=1,\n",
      "                                            enable_categorical=False, gamma=0,\n",
      "                                            gpu...\n",
      "                                            interaction_constraints='',\n",
      "                                            learning_rate=0.300000012,\n",
      "                                            max_delta_step=0, max_depth=12,\n",
      "                                            max_leaf_nodes=None,\n",
      "                                            min_child_weight=1,\n",
      "                                            min_samples_split=1, missing=nan,\n",
      "                                            monotone_constraints='()',\n",
      "                                            n_estimators=10, n_jobs=8,\n",
      "                                            num_parallel_tree=1,\n",
      "                                            predictor='auto', random_state=3,\n",
      "                                            reg_alpha=0, reg_lambda=1,\n",
      "                                            scale_pos_weight=1, subsample=1,\n",
      "                                            tree_method='exact',\n",
      "                                            validate_parameters=1,\n",
      "                                            verbosity=None))])\n",
      "kfold  8  score: 97.18%\n",
      "kfold  8  score: 99.57%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21:47:46] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:47:46] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "VotingClassifier(estimators=[('gb',\n",
      "                              GradientBoostingClassifier(max_depth=12,\n",
      "                                                         min_samples_split=12,\n",
      "                                                         n_estimators=5,\n",
      "                                                         random_state=3)),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(max_depth=12,\n",
      "                                                     n_estimators=10,\n",
      "                                                     random_state=3)),\n",
      "                             ('xgb',\n",
      "                              XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "                                            colsample_bylevel=1,\n",
      "                                            colsample_bynode=1,\n",
      "                                            colsample_bytree=1,\n",
      "                                            enable_categorical=False, gamma=0,\n",
      "                                            gpu...\n",
      "                                            interaction_constraints='',\n",
      "                                            learning_rate=0.300000012,\n",
      "                                            max_delta_step=0, max_depth=12,\n",
      "                                            max_leaf_nodes=None,\n",
      "                                            min_child_weight=1,\n",
      "                                            min_samples_split=1, missing=nan,\n",
      "                                            monotone_constraints='()',\n",
      "                                            n_estimators=10, n_jobs=8,\n",
      "                                            num_parallel_tree=1,\n",
      "                                            predictor='auto', random_state=3,\n",
      "                                            reg_alpha=0, reg_lambda=1,\n",
      "                                            scale_pos_weight=1, subsample=1,\n",
      "                                            tree_method='exact',\n",
      "                                            validate_parameters=1,\n",
      "                                            verbosity=None))])\n",
      "kfold  8  score: 95.98%\n",
      "kfold  8  score: 99.40%\n",
      "[21:47:47] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:47:47] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "VotingClassifier(estimators=[('gb',\n",
      "                              GradientBoostingClassifier(max_depth=12,\n",
      "                                                         min_samples_split=12,\n",
      "                                                         n_estimators=5,\n",
      "                                                         random_state=3)),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(max_depth=12,\n",
      "                                                     n_estimators=10,\n",
      "                                                     random_state=3)),\n",
      "                             ('xgb',\n",
      "                              XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "                                            colsample_bylevel=1,\n",
      "                                            colsample_bynode=1,\n",
      "                                            colsample_bytree=1,\n",
      "                                            enable_categorical=False, gamma=0,\n",
      "                                            gpu...\n",
      "                                            interaction_constraints='',\n",
      "                                            learning_rate=0.300000012,\n",
      "                                            max_delta_step=0, max_depth=12,\n",
      "                                            max_leaf_nodes=None,\n",
      "                                            min_child_weight=1,\n",
      "                                            min_samples_split=1, missing=nan,\n",
      "                                            monotone_constraints='()',\n",
      "                                            n_estimators=10, n_jobs=8,\n",
      "                                            num_parallel_tree=1,\n",
      "                                            predictor='auto', random_state=3,\n",
      "                                            reg_alpha=0, reg_lambda=1,\n",
      "                                            scale_pos_weight=1, subsample=1,\n",
      "                                            tree_method='exact',\n",
      "                                            validate_parameters=1,\n",
      "                                            verbosity=None))])\n",
      "kfold  8  score: 96.38%\n",
      "kfold  8  score: 99.40%\n",
      "[21:47:47] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:47:47] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "VotingClassifier(estimators=[('gb',\n",
      "                              GradientBoostingClassifier(max_depth=12,\n",
      "                                                         min_samples_split=12,\n",
      "                                                         n_estimators=5,\n",
      "                                                         random_state=3)),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(max_depth=12,\n",
      "                                                     n_estimators=10,\n",
      "                                                     random_state=3)),\n",
      "                             ('xgb',\n",
      "                              XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "                                            colsample_bylevel=1,\n",
      "                                            colsample_bynode=1,\n",
      "                                            colsample_bytree=1,\n",
      "                                            enable_categorical=False, gamma=0,\n",
      "                                            gpu...\n",
      "                                            interaction_constraints='',\n",
      "                                            learning_rate=0.300000012,\n",
      "                                            max_delta_step=0, max_depth=12,\n",
      "                                            max_leaf_nodes=None,\n",
      "                                            min_child_weight=1,\n",
      "                                            min_samples_split=1, missing=nan,\n",
      "                                            monotone_constraints='()',\n",
      "                                            n_estimators=10, n_jobs=8,\n",
      "                                            num_parallel_tree=1,\n",
      "                                            predictor='auto', random_state=3,\n",
      "                                            reg_alpha=0, reg_lambda=1,\n",
      "                                            scale_pos_weight=1, subsample=1,\n",
      "                                            tree_method='exact',\n",
      "                                            validate_parameters=1,\n",
      "                                            verbosity=None))])\n",
      "kfold  8  score: 95.57%\n",
      "kfold  8  score: 99.57%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21:47:48] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:47:48] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "VotingClassifier(estimators=[('gb',\n",
      "                              GradientBoostingClassifier(max_depth=12,\n",
      "                                                         min_samples_split=12,\n",
      "                                                         n_estimators=5,\n",
      "                                                         random_state=3)),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(max_depth=12,\n",
      "                                                     n_estimators=10,\n",
      "                                                     random_state=3)),\n",
      "                             ('xgb',\n",
      "                              XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "                                            colsample_bylevel=1,\n",
      "                                            colsample_bynode=1,\n",
      "                                            colsample_bytree=1,\n",
      "                                            enable_categorical=False, gamma=0,\n",
      "                                            gpu...\n",
      "                                            interaction_constraints='',\n",
      "                                            learning_rate=0.300000012,\n",
      "                                            max_delta_step=0, max_depth=12,\n",
      "                                            max_leaf_nodes=None,\n",
      "                                            min_child_weight=1,\n",
      "                                            min_samples_split=1, missing=nan,\n",
      "                                            monotone_constraints='()',\n",
      "                                            n_estimators=10, n_jobs=8,\n",
      "                                            num_parallel_tree=1,\n",
      "                                            predictor='auto', random_state=3,\n",
      "                                            reg_alpha=0, reg_lambda=1,\n",
      "                                            scale_pos_weight=1, subsample=1,\n",
      "                                            tree_method='exact',\n",
      "                                            validate_parameters=1,\n",
      "                                            verbosity=None))])\n",
      "kfold  8  score: 96.37%\n",
      "kfold  8  score: 99.60%\n",
      "[21:47:48] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:47:48] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "VotingClassifier(estimators=[('gb',\n",
      "                              GradientBoostingClassifier(max_depth=12,\n",
      "                                                         min_samples_split=12,\n",
      "                                                         n_estimators=5,\n",
      "                                                         random_state=3)),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(max_depth=12,\n",
      "                                                     n_estimators=10,\n",
      "                                                     random_state=3)),\n",
      "                             ('xgb',\n",
      "                              XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "                                            colsample_bylevel=1,\n",
      "                                            colsample_bynode=1,\n",
      "                                            colsample_bytree=1,\n",
      "                                            enable_categorical=False, gamma=0,\n",
      "                                            gpu...\n",
      "                                            interaction_constraints='',\n",
      "                                            learning_rate=0.300000012,\n",
      "                                            max_delta_step=0, max_depth=12,\n",
      "                                            max_leaf_nodes=None,\n",
      "                                            min_child_weight=1,\n",
      "                                            min_samples_split=1, missing=nan,\n",
      "                                            monotone_constraints='()',\n",
      "                                            n_estimators=10, n_jobs=8,\n",
      "                                            num_parallel_tree=1,\n",
      "                                            predictor='auto', random_state=3,\n",
      "                                            reg_alpha=0, reg_lambda=1,\n",
      "                                            scale_pos_weight=1, subsample=1,\n",
      "                                            tree_method='exact',\n",
      "                                            validate_parameters=1,\n",
      "                                            verbosity=None))])\n",
      "kfold  8  score: 98.19%\n",
      "kfold  8  score: 99.62%\n",
      "[21:47:48] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:47:48] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "VotingClassifier(estimators=[('gb',\n",
      "                              GradientBoostingClassifier(max_depth=12,\n",
      "                                                         min_samples_split=12,\n",
      "                                                         n_estimators=5,\n",
      "                                                         random_state=3)),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(max_depth=12,\n",
      "                                                     n_estimators=10,\n",
      "                                                     random_state=3)),\n",
      "                             ('xgb',\n",
      "                              XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "                                            colsample_bylevel=1,\n",
      "                                            colsample_bynode=1,\n",
      "                                            colsample_bytree=1,\n",
      "                                            enable_categorical=False, gamma=0,\n",
      "                                            gpu...\n",
      "                                            interaction_constraints='',\n",
      "                                            learning_rate=0.300000012,\n",
      "                                            max_delta_step=0, max_depth=12,\n",
      "                                            max_leaf_nodes=None,\n",
      "                                            min_child_weight=1,\n",
      "                                            min_samples_split=1, missing=nan,\n",
      "                                            monotone_constraints='()',\n",
      "                                            n_estimators=10, n_jobs=8,\n",
      "                                            num_parallel_tree=1,\n",
      "                                            predictor='auto', random_state=3,\n",
      "                                            reg_alpha=0, reg_lambda=1,\n",
      "                                            scale_pos_weight=1, subsample=1,\n",
      "                                            tree_method='exact',\n",
      "                                            validate_parameters=1,\n",
      "                                            verbosity=None))])\n",
      "kfold  8  score: 97.78%\n",
      "kfold  8  score: 99.58%\n",
      "Average score over k-fold 96.76% (+/-0.81%) [Ensemble Majority]\n",
      "./Files_Majority_vote/128_features/filter9.csv\n",
      "n_estimators = 100 \n",
      "\n",
      "10-fold cross validation for file :  9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GradientBoostingClassifier(max_depth=12, min_samples_split=12, n_estimators=5,\n",
      "                           random_state=3)\n",
      "kfold  9  score: 89.70%\n",
      "kfold  9  score: 92.16%\n",
      "GradientBoostingClassifier(max_depth=12, min_samples_split=12, n_estimators=5,\n",
      "                           random_state=3)\n",
      "kfold  9  score: 87.83%\n",
      "kfold  9  score: 90.76%\n",
      "GradientBoostingClassifier(max_depth=12, min_samples_split=12, n_estimators=5,\n",
      "                           random_state=3)\n",
      "kfold  9  score: 82.21%\n",
      "kfold  9  score: 86.37%\n",
      "GradientBoostingClassifier(max_depth=12, min_samples_split=12, n_estimators=5,\n",
      "                           random_state=3)\n",
      "kfold  9  score: 85.58%\n",
      "kfold  9  score: 90.20%\n",
      "GradientBoostingClassifier(max_depth=12, min_samples_split=12, n_estimators=5,\n",
      "                           random_state=3)\n",
      "kfold  9  score: 87.08%\n",
      "kfold  9  score: 90.03%\n",
      "GradientBoostingClassifier(max_depth=12, min_samples_split=12, n_estimators=5,\n",
      "                           random_state=3)\n",
      "kfold  9  score: 88.76%\n",
      "kfold  9  score: 92.88%\n",
      "GradientBoostingClassifier(max_depth=12, min_samples_split=12, n_estimators=5,\n",
      "                           random_state=3)\n",
      "kfold  9  score: 84.64%\n",
      "kfold  9  score: 87.08%\n",
      "GradientBoostingClassifier(max_depth=12, min_samples_split=12, n_estimators=5,\n",
      "                           random_state=3)\n",
      "kfold  9  score: 86.70%\n",
      "kfold  9  score: 90.57%\n",
      "GradientBoostingClassifier(max_depth=12, min_samples_split=12, n_estimators=5,\n",
      "                           random_state=3)\n",
      "kfold  9  score: 86.70%\n",
      "kfold  9  score: 89.06%\n",
      "GradientBoostingClassifier(max_depth=12, min_samples_split=12, n_estimators=5,\n",
      "                           random_state=3)\n",
      "kfold  9  score: 89.89%\n",
      "kfold  9  score: 92.41%\n",
      "Average score over k-fold 86.91% (+/-2.24%) [Gradient Boosting]\n",
      "RandomForestClassifier(max_depth=12, n_estimators=10, random_state=3)\n",
      "kfold  9  score: 89.51%\n",
      "kfold  9  score: 94.65%\n",
      "RandomForestClassifier(max_depth=12, n_estimators=10, random_state=3)\n",
      "kfold  9  score: 92.32%\n",
      "kfold  9  score: 96.84%\n",
      "RandomForestClassifier(max_depth=12, n_estimators=10, random_state=3)\n",
      "kfold  9  score: 87.27%\n",
      "kfold  9  score: 95.82%\n",
      "RandomForestClassifier(max_depth=12, n_estimators=10, random_state=3)\n",
      "kfold  9  score: 89.51%\n",
      "kfold  9  score: 96.09%\n",
      "RandomForestClassifier(max_depth=12, n_estimators=10, random_state=3)\n",
      "kfold  9  score: 91.76%\n",
      "kfold  9  score: 95.96%\n",
      "RandomForestClassifier(max_depth=12, n_estimators=10, random_state=3)\n",
      "kfold  9  score: 91.01%\n",
      "kfold  9  score: 96.03%\n",
      "RandomForestClassifier(max_depth=12, n_estimators=10, random_state=3)\n",
      "kfold  9  score: 89.33%\n",
      "kfold  9  score: 95.76%\n",
      "RandomForestClassifier(max_depth=12, n_estimators=10, random_state=3)\n",
      "kfold  9  score: 90.45%\n",
      "kfold  9  score: 96.42%\n",
      "RandomForestClassifier(max_depth=12, n_estimators=10, random_state=3)\n",
      "kfold  9  score: 89.89%\n",
      "kfold  9  score: 96.38%\n",
      "RandomForestClassifier(max_depth=12, n_estimators=10, random_state=3)\n",
      "kfold  9  score: 89.51%\n",
      "kfold  9  score: 95.76%\n",
      "Average score over k-fold 90.06% (+/-1.35%) [Random Forest]\n",
      "[21:47:52] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:47:52] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
      "              gamma=0, gpu_id=-1, importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.300000012,\n",
      "              max_delta_step=0, max_depth=12, max_leaf_nodes=None,\n",
      "              min_child_weight=1, min_samples_split=1, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=10, n_jobs=8,\n",
      "              num_parallel_tree=1, predictor='auto', random_state=3,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)\n",
      "kfold  9  score: 93.45%\n",
      "kfold  9  score: 98.88%\n",
      "[21:47:52] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:47:52] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
      "              gamma=0, gpu_id=-1, importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.300000012,\n",
      "              max_delta_step=0, max_depth=12, max_leaf_nodes=None,\n",
      "              min_child_weight=1, min_samples_split=1, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=10, n_jobs=8,\n",
      "              num_parallel_tree=1, predictor='auto', random_state=3,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)\n",
      "kfold  9  score: 92.88%\n",
      "kfold  9  score: 98.88%\n",
      "[21:47:53] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:47:53] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
      "              gamma=0, gpu_id=-1, importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.300000012,\n",
      "              max_delta_step=0, max_depth=12, max_leaf_nodes=None,\n",
      "              min_child_weight=1, min_samples_split=1, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=10, n_jobs=8,\n",
      "              num_parallel_tree=1, predictor='auto', random_state=3,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)\n",
      "kfold  9  score: 88.39%\n",
      "kfold  9  score: 97.96%\n",
      "[21:47:53] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:47:53] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
      "              gamma=0, gpu_id=-1, importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.300000012,\n",
      "              max_delta_step=0, max_depth=12, max_leaf_nodes=None,\n",
      "              min_child_weight=1, min_samples_split=1, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=10, n_jobs=8,\n",
      "              num_parallel_tree=1, predictor='auto', random_state=3,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)\n",
      "kfold  9  score: 91.39%\n",
      "kfold  9  score: 98.56%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21:47:53] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:47:53] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
      "              gamma=0, gpu_id=-1, importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.300000012,\n",
      "              max_delta_step=0, max_depth=12, max_leaf_nodes=None,\n",
      "              min_child_weight=1, min_samples_split=1, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=10, n_jobs=8,\n",
      "              num_parallel_tree=1, predictor='auto', random_state=3,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)\n",
      "kfold  9  score: 95.32%\n",
      "kfold  9  score: 98.77%\n",
      "[21:47:53] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:47:53] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
      "              gamma=0, gpu_id=-1, importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.300000012,\n",
      "              max_delta_step=0, max_depth=12, max_leaf_nodes=None,\n",
      "              min_child_weight=1, min_samples_split=1, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=10, n_jobs=8,\n",
      "              num_parallel_tree=1, predictor='auto', random_state=3,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)\n",
      "kfold  9  score: 91.39%\n",
      "kfold  9  score: 98.36%\n",
      "[21:47:53] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:47:53] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
      "              gamma=0, gpu_id=-1, importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.300000012,\n",
      "              max_delta_step=0, max_depth=12, max_leaf_nodes=None,\n",
      "              min_child_weight=1, min_samples_split=1, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=10, n_jobs=8,\n",
      "              num_parallel_tree=1, predictor='auto', random_state=3,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)\n",
      "kfold  9  score: 91.39%\n",
      "kfold  9  score: 98.73%\n",
      "[21:47:53] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:47:53] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
      "              gamma=0, gpu_id=-1, importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.300000012,\n",
      "              max_delta_step=0, max_depth=12, max_leaf_nodes=None,\n",
      "              min_child_weight=1, min_samples_split=1, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=10, n_jobs=8,\n",
      "              num_parallel_tree=1, predictor='auto', random_state=3,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)\n",
      "kfold  9  score: 92.32%\n",
      "kfold  9  score: 98.38%\n",
      "[21:47:53] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:47:53] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
      "              gamma=0, gpu_id=-1, importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.300000012,\n",
      "              max_delta_step=0, max_depth=12, max_leaf_nodes=None,\n",
      "              min_child_weight=1, min_samples_split=1, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=10, n_jobs=8,\n",
      "              num_parallel_tree=1, predictor='auto', random_state=3,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)\n",
      "kfold  9  score: 91.57%\n",
      "kfold  9  score: 98.98%\n",
      "[21:47:53] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:47:53] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
      "              gamma=0, gpu_id=-1, importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.300000012,\n",
      "              max_delta_step=0, max_depth=12, max_leaf_nodes=None,\n",
      "              min_child_weight=1, min_samples_split=1, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=10, n_jobs=8,\n",
      "              num_parallel_tree=1, predictor='auto', random_state=3,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)\n",
      "kfold  9  score: 91.39%\n",
      "kfold  9  score: 98.79%\n",
      "Average score over k-fold 91.95% (+/-1.69%) [XGBClassifier]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21:47:54] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:47:54] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "VotingClassifier(estimators=[('gb',\n",
      "                              GradientBoostingClassifier(max_depth=12,\n",
      "                                                         min_samples_split=12,\n",
      "                                                         n_estimators=5,\n",
      "                                                         random_state=3)),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(max_depth=12,\n",
      "                                                     n_estimators=10,\n",
      "                                                     random_state=3)),\n",
      "                             ('xgb',\n",
      "                              XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "                                            colsample_bylevel=1,\n",
      "                                            colsample_bynode=1,\n",
      "                                            colsample_bytree=1,\n",
      "                                            enable_categorical=False, gamma=0,\n",
      "                                            gpu...\n",
      "                                            interaction_constraints='',\n",
      "                                            learning_rate=0.300000012,\n",
      "                                            max_delta_step=0, max_depth=12,\n",
      "                                            max_leaf_nodes=None,\n",
      "                                            min_child_weight=1,\n",
      "                                            min_samples_split=1, missing=nan,\n",
      "                                            monotone_constraints='()',\n",
      "                                            n_estimators=10, n_jobs=8,\n",
      "                                            num_parallel_tree=1,\n",
      "                                            predictor='auto', random_state=3,\n",
      "                                            reg_alpha=0, reg_lambda=1,\n",
      "                                            scale_pos_weight=1, subsample=1,\n",
      "                                            tree_method='exact',\n",
      "                                            validate_parameters=1,\n",
      "                                            verbosity=None))])\n",
      "kfold  9  score: 92.13%\n",
      "kfold  9  score: 96.75%\n",
      "[21:47:54] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:47:54] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "VotingClassifier(estimators=[('gb',\n",
      "                              GradientBoostingClassifier(max_depth=12,\n",
      "                                                         min_samples_split=12,\n",
      "                                                         n_estimators=5,\n",
      "                                                         random_state=3)),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(max_depth=12,\n",
      "                                                     n_estimators=10,\n",
      "                                                     random_state=3)),\n",
      "                             ('xgb',\n",
      "                              XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "                                            colsample_bylevel=1,\n",
      "                                            colsample_bynode=1,\n",
      "                                            colsample_bytree=1,\n",
      "                                            enable_categorical=False, gamma=0,\n",
      "                                            gpu...\n",
      "                                            interaction_constraints='',\n",
      "                                            learning_rate=0.300000012,\n",
      "                                            max_delta_step=0, max_depth=12,\n",
      "                                            max_leaf_nodes=None,\n",
      "                                            min_child_weight=1,\n",
      "                                            min_samples_split=1, missing=nan,\n",
      "                                            monotone_constraints='()',\n",
      "                                            n_estimators=10, n_jobs=8,\n",
      "                                            num_parallel_tree=1,\n",
      "                                            predictor='auto', random_state=3,\n",
      "                                            reg_alpha=0, reg_lambda=1,\n",
      "                                            scale_pos_weight=1, subsample=1,\n",
      "                                            tree_method='exact',\n",
      "                                            validate_parameters=1,\n",
      "                                            verbosity=None))])\n",
      "kfold  9  score: 93.07%\n",
      "kfold  9  score: 97.69%\n",
      "[21:47:55] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:47:55] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "VotingClassifier(estimators=[('gb',\n",
      "                              GradientBoostingClassifier(max_depth=12,\n",
      "                                                         min_samples_split=12,\n",
      "                                                         n_estimators=5,\n",
      "                                                         random_state=3)),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(max_depth=12,\n",
      "                                                     n_estimators=10,\n",
      "                                                     random_state=3)),\n",
      "                             ('xgb',\n",
      "                              XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "                                            colsample_bylevel=1,\n",
      "                                            colsample_bynode=1,\n",
      "                                            colsample_bytree=1,\n",
      "                                            enable_categorical=False, gamma=0,\n",
      "                                            gpu...\n",
      "                                            interaction_constraints='',\n",
      "                                            learning_rate=0.300000012,\n",
      "                                            max_delta_step=0, max_depth=12,\n",
      "                                            max_leaf_nodes=None,\n",
      "                                            min_child_weight=1,\n",
      "                                            min_samples_split=1, missing=nan,\n",
      "                                            monotone_constraints='()',\n",
      "                                            n_estimators=10, n_jobs=8,\n",
      "                                            num_parallel_tree=1,\n",
      "                                            predictor='auto', random_state=3,\n",
      "                                            reg_alpha=0, reg_lambda=1,\n",
      "                                            scale_pos_weight=1, subsample=1,\n",
      "                                            tree_method='exact',\n",
      "                                            validate_parameters=1,\n",
      "                                            verbosity=None))])\n",
      "kfold  9  score: 87.64%\n",
      "kfold  9  score: 96.09%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21:47:56] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:47:56] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "VotingClassifier(estimators=[('gb',\n",
      "                              GradientBoostingClassifier(max_depth=12,\n",
      "                                                         min_samples_split=12,\n",
      "                                                         n_estimators=5,\n",
      "                                                         random_state=3)),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(max_depth=12,\n",
      "                                                     n_estimators=10,\n",
      "                                                     random_state=3)),\n",
      "                             ('xgb',\n",
      "                              XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "                                            colsample_bylevel=1,\n",
      "                                            colsample_bynode=1,\n",
      "                                            colsample_bytree=1,\n",
      "                                            enable_categorical=False, gamma=0,\n",
      "                                            gpu...\n",
      "                                            interaction_constraints='',\n",
      "                                            learning_rate=0.300000012,\n",
      "                                            max_delta_step=0, max_depth=12,\n",
      "                                            max_leaf_nodes=None,\n",
      "                                            min_child_weight=1,\n",
      "                                            min_samples_split=1, missing=nan,\n",
      "                                            monotone_constraints='()',\n",
      "                                            n_estimators=10, n_jobs=8,\n",
      "                                            num_parallel_tree=1,\n",
      "                                            predictor='auto', random_state=3,\n",
      "                                            reg_alpha=0, reg_lambda=1,\n",
      "                                            scale_pos_weight=1, subsample=1,\n",
      "                                            tree_method='exact',\n",
      "                                            validate_parameters=1,\n",
      "                                            verbosity=None))])\n",
      "kfold  9  score: 90.82%\n",
      "kfold  9  score: 97.25%\n",
      "[21:47:56] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:47:56] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "VotingClassifier(estimators=[('gb',\n",
      "                              GradientBoostingClassifier(max_depth=12,\n",
      "                                                         min_samples_split=12,\n",
      "                                                         n_estimators=5,\n",
      "                                                         random_state=3)),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(max_depth=12,\n",
      "                                                     n_estimators=10,\n",
      "                                                     random_state=3)),\n",
      "                             ('xgb',\n",
      "                              XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "                                            colsample_bylevel=1,\n",
      "                                            colsample_bynode=1,\n",
      "                                            colsample_bytree=1,\n",
      "                                            enable_categorical=False, gamma=0,\n",
      "                                            gpu...\n",
      "                                            interaction_constraints='',\n",
      "                                            learning_rate=0.300000012,\n",
      "                                            max_delta_step=0, max_depth=12,\n",
      "                                            max_leaf_nodes=None,\n",
      "                                            min_child_weight=1,\n",
      "                                            min_samples_split=1, missing=nan,\n",
      "                                            monotone_constraints='()',\n",
      "                                            n_estimators=10, n_jobs=8,\n",
      "                                            num_parallel_tree=1,\n",
      "                                            predictor='auto', random_state=3,\n",
      "                                            reg_alpha=0, reg_lambda=1,\n",
      "                                            scale_pos_weight=1, subsample=1,\n",
      "                                            tree_method='exact',\n",
      "                                            validate_parameters=1,\n",
      "                                            verbosity=None))])\n",
      "kfold  9  score: 93.63%\n",
      "kfold  9  score: 97.02%\n",
      "[21:47:57] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:47:57] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "VotingClassifier(estimators=[('gb',\n",
      "                              GradientBoostingClassifier(max_depth=12,\n",
      "                                                         min_samples_split=12,\n",
      "                                                         n_estimators=5,\n",
      "                                                         random_state=3)),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(max_depth=12,\n",
      "                                                     n_estimators=10,\n",
      "                                                     random_state=3)),\n",
      "                             ('xgb',\n",
      "                              XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "                                            colsample_bylevel=1,\n",
      "                                            colsample_bynode=1,\n",
      "                                            colsample_bytree=1,\n",
      "                                            enable_categorical=False, gamma=0,\n",
      "                                            gpu...\n",
      "                                            interaction_constraints='',\n",
      "                                            learning_rate=0.300000012,\n",
      "                                            max_delta_step=0, max_depth=12,\n",
      "                                            max_leaf_nodes=None,\n",
      "                                            min_child_weight=1,\n",
      "                                            min_samples_split=1, missing=nan,\n",
      "                                            monotone_constraints='()',\n",
      "                                            n_estimators=10, n_jobs=8,\n",
      "                                            num_parallel_tree=1,\n",
      "                                            predictor='auto', random_state=3,\n",
      "                                            reg_alpha=0, reg_lambda=1,\n",
      "                                            scale_pos_weight=1, subsample=1,\n",
      "                                            tree_method='exact',\n",
      "                                            validate_parameters=1,\n",
      "                                            verbosity=None))])\n",
      "kfold  9  score: 91.39%\n",
      "kfold  9  score: 97.27%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21:47:57] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:47:57] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "VotingClassifier(estimators=[('gb',\n",
      "                              GradientBoostingClassifier(max_depth=12,\n",
      "                                                         min_samples_split=12,\n",
      "                                                         n_estimators=5,\n",
      "                                                         random_state=3)),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(max_depth=12,\n",
      "                                                     n_estimators=10,\n",
      "                                                     random_state=3)),\n",
      "                             ('xgb',\n",
      "                              XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "                                            colsample_bylevel=1,\n",
      "                                            colsample_bynode=1,\n",
      "                                            colsample_bytree=1,\n",
      "                                            enable_categorical=False, gamma=0,\n",
      "                                            gpu...\n",
      "                                            interaction_constraints='',\n",
      "                                            learning_rate=0.300000012,\n",
      "                                            max_delta_step=0, max_depth=12,\n",
      "                                            max_leaf_nodes=None,\n",
      "                                            min_child_weight=1,\n",
      "                                            min_samples_split=1, missing=nan,\n",
      "                                            monotone_constraints='()',\n",
      "                                            n_estimators=10, n_jobs=8,\n",
      "                                            num_parallel_tree=1,\n",
      "                                            predictor='auto', random_state=3,\n",
      "                                            reg_alpha=0, reg_lambda=1,\n",
      "                                            scale_pos_weight=1, subsample=1,\n",
      "                                            tree_method='exact',\n",
      "                                            validate_parameters=1,\n",
      "                                            verbosity=None))])\n",
      "kfold  9  score: 90.26%\n",
      "kfold  9  score: 96.65%\n",
      "[21:47:58] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:47:58] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "VotingClassifier(estimators=[('gb',\n",
      "                              GradientBoostingClassifier(max_depth=12,\n",
      "                                                         min_samples_split=12,\n",
      "                                                         n_estimators=5,\n",
      "                                                         random_state=3)),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(max_depth=12,\n",
      "                                                     n_estimators=10,\n",
      "                                                     random_state=3)),\n",
      "                             ('xgb',\n",
      "                              XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "                                            colsample_bylevel=1,\n",
      "                                            colsample_bynode=1,\n",
      "                                            colsample_bytree=1,\n",
      "                                            enable_categorical=False, gamma=0,\n",
      "                                            gpu...\n",
      "                                            interaction_constraints='',\n",
      "                                            learning_rate=0.300000012,\n",
      "                                            max_delta_step=0, max_depth=12,\n",
      "                                            max_leaf_nodes=None,\n",
      "                                            min_child_weight=1,\n",
      "                                            min_samples_split=1, missing=nan,\n",
      "                                            monotone_constraints='()',\n",
      "                                            n_estimators=10, n_jobs=8,\n",
      "                                            num_parallel_tree=1,\n",
      "                                            predictor='auto', random_state=3,\n",
      "                                            reg_alpha=0, reg_lambda=1,\n",
      "                                            scale_pos_weight=1, subsample=1,\n",
      "                                            tree_method='exact',\n",
      "                                            validate_parameters=1,\n",
      "                                            verbosity=None))])\n",
      "kfold  9  score: 91.20%\n",
      "kfold  9  score: 97.46%\n",
      "[21:47:58] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:47:59] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "VotingClassifier(estimators=[('gb',\n",
      "                              GradientBoostingClassifier(max_depth=12,\n",
      "                                                         min_samples_split=12,\n",
      "                                                         n_estimators=5,\n",
      "                                                         random_state=3)),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(max_depth=12,\n",
      "                                                     n_estimators=10,\n",
      "                                                     random_state=3)),\n",
      "                             ('xgb',\n",
      "                              XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "                                            colsample_bylevel=1,\n",
      "                                            colsample_bynode=1,\n",
      "                                            colsample_bytree=1,\n",
      "                                            enable_categorical=False, gamma=0,\n",
      "                                            gpu...\n",
      "                                            interaction_constraints='',\n",
      "                                            learning_rate=0.300000012,\n",
      "                                            max_delta_step=0, max_depth=12,\n",
      "                                            max_leaf_nodes=None,\n",
      "                                            min_child_weight=1,\n",
      "                                            min_samples_split=1, missing=nan,\n",
      "                                            monotone_constraints='()',\n",
      "                                            n_estimators=10, n_jobs=8,\n",
      "                                            num_parallel_tree=1,\n",
      "                                            predictor='auto', random_state=3,\n",
      "                                            reg_alpha=0, reg_lambda=1,\n",
      "                                            scale_pos_weight=1, subsample=1,\n",
      "                                            tree_method='exact',\n",
      "                                            validate_parameters=1,\n",
      "                                            verbosity=None))])\n",
      "kfold  9  score: 91.57%\n",
      "kfold  9  score: 97.17%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21:47:59] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:47:59] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "VotingClassifier(estimators=[('gb',\n",
      "                              GradientBoostingClassifier(max_depth=12,\n",
      "                                                         min_samples_split=12,\n",
      "                                                         n_estimators=5,\n",
      "                                                         random_state=3)),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(max_depth=12,\n",
      "                                                     n_estimators=10,\n",
      "                                                     random_state=3)),\n",
      "                             ('xgb',\n",
      "                              XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "                                            colsample_bylevel=1,\n",
      "                                            colsample_bynode=1,\n",
      "                                            colsample_bytree=1,\n",
      "                                            enable_categorical=False, gamma=0,\n",
      "                                            gpu...\n",
      "                                            interaction_constraints='',\n",
      "                                            learning_rate=0.300000012,\n",
      "                                            max_delta_step=0, max_depth=12,\n",
      "                                            max_leaf_nodes=None,\n",
      "                                            min_child_weight=1,\n",
      "                                            min_samples_split=1, missing=nan,\n",
      "                                            monotone_constraints='()',\n",
      "                                            n_estimators=10, n_jobs=8,\n",
      "                                            num_parallel_tree=1,\n",
      "                                            predictor='auto', random_state=3,\n",
      "                                            reg_alpha=0, reg_lambda=1,\n",
      "                                            scale_pos_weight=1, subsample=1,\n",
      "                                            tree_method='exact',\n",
      "                                            validate_parameters=1,\n",
      "                                            verbosity=None))])\n",
      "kfold  9  score: 91.20%\n",
      "kfold  9  score: 97.17%\n",
      "Average score over k-fold 91.29% (+/-1.55%) [Ensemble Majority]\n",
      "./Files_Majority_vote/128_features/filter10.csv\n",
      "n_estimators = 100 \n",
      "\n",
      "10-fold cross validation for file :  10\n",
      "GradientBoostingClassifier(max_depth=12, min_samples_split=12, n_estimators=5,\n",
      "                           random_state=3)\n",
      "kfold  10  score: 86.18%\n",
      "kfold  10  score: 91.56%\n",
      "GradientBoostingClassifier(max_depth=12, min_samples_split=12, n_estimators=5,\n",
      "                           random_state=3)\n",
      "kfold  10  score: 87.43%\n",
      "kfold  10  score: 90.64%\n",
      "GradientBoostingClassifier(max_depth=12, min_samples_split=12, n_estimators=5,\n",
      "                           random_state=3)\n",
      "kfold  10  score: 91.92%\n",
      "kfold  10  score: 93.75%\n",
      "GradientBoostingClassifier(max_depth=12, min_samples_split=12, n_estimators=5,\n",
      "                           random_state=3)\n",
      "kfold  10  score: 89.77%\n",
      "kfold  10  score: 92.30%\n",
      "GradientBoostingClassifier(max_depth=12, min_samples_split=12, n_estimators=5,\n",
      "                           random_state=3)\n",
      "kfold  10  score: 87.79%\n",
      "kfold  10  score: 91.44%\n",
      "GradientBoostingClassifier(max_depth=12, min_samples_split=12, n_estimators=5,\n",
      "                           random_state=3)\n",
      "kfold  10  score: 85.82%\n",
      "kfold  10  score: 92.66%\n",
      "GradientBoostingClassifier(max_depth=12, min_samples_split=12, n_estimators=5,\n",
      "                           random_state=3)\n",
      "kfold  10  score: 91.56%\n",
      "kfold  10  score: 94.03%\n",
      "GradientBoostingClassifier(max_depth=12, min_samples_split=12, n_estimators=5,\n",
      "                           random_state=3)\n",
      "kfold  10  score: 78.64%\n",
      "kfold  10  score: 84.06%\n",
      "GradientBoostingClassifier(max_depth=12, min_samples_split=12, n_estimators=5,\n",
      "                           random_state=3)\n",
      "kfold  10  score: 87.07%\n",
      "kfold  10  score: 89.96%\n",
      "GradientBoostingClassifier(max_depth=12, min_samples_split=12, n_estimators=5,\n",
      "                           random_state=3)\n",
      "kfold  10  score: 89.21%\n",
      "kfold  10  score: 91.30%\n",
      "Average score over k-fold 87.54% (+/-3.57%) [Gradient Boosting]\n",
      "RandomForestClassifier(max_depth=12, n_estimators=10, random_state=3)\n",
      "kfold  10  score: 90.48%\n",
      "kfold  10  score: 96.51%\n",
      "RandomForestClassifier(max_depth=12, n_estimators=10, random_state=3)\n",
      "kfold  10  score: 93.54%\n",
      "kfold  10  score: 97.31%\n",
      "RandomForestClassifier(max_depth=12, n_estimators=10, random_state=3)\n",
      "kfold  10  score: 93.90%\n",
      "kfold  10  score: 98.00%\n",
      "RandomForestClassifier(max_depth=12, n_estimators=10, random_state=3)\n",
      "kfold  10  score: 93.18%\n",
      "kfold  10  score: 97.47%\n",
      "RandomForestClassifier(max_depth=12, n_estimators=10, random_state=3)\n",
      "kfold  10  score: 93.90%\n",
      "kfold  10  score: 97.85%\n",
      "RandomForestClassifier(max_depth=12, n_estimators=10, random_state=3)\n",
      "kfold  10  score: 90.48%\n",
      "kfold  10  score: 97.37%\n",
      "RandomForestClassifier(max_depth=12, n_estimators=10, random_state=3)\n",
      "kfold  10  score: 93.72%\n",
      "kfold  10  score: 97.05%\n",
      "RandomForestClassifier(max_depth=12, n_estimators=10, random_state=3)\n",
      "kfold  10  score: 91.02%\n",
      "kfold  10  score: 96.75%\n",
      "RandomForestClassifier(max_depth=12, n_estimators=10, random_state=3)\n",
      "kfold  10  score: 95.51%\n",
      "kfold  10  score: 97.43%\n",
      "RandomForestClassifier(max_depth=12, n_estimators=10, random_state=3)\n",
      "kfold  10  score: 94.06%\n",
      "kfold  10  score: 97.61%\n",
      "Average score over k-fold 92.98% (+/-1.63%) [Random Forest]\n",
      "[21:48:03] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:48:03] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
      "              gamma=0, gpu_id=-1, importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.300000012,\n",
      "              max_delta_step=0, max_depth=12, max_leaf_nodes=None,\n",
      "              min_child_weight=1, min_samples_split=1, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=10, n_jobs=8,\n",
      "              num_parallel_tree=1, predictor='auto', random_state=3,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)\n",
      "kfold  10  score: 92.10%\n",
      "kfold  10  score: 99.14%\n",
      "[21:48:03] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:48:03] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
      "              gamma=0, gpu_id=-1, importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.300000012,\n",
      "              max_delta_step=0, max_depth=12, max_leaf_nodes=None,\n",
      "              min_child_weight=1, min_samples_split=1, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=10, n_jobs=8,\n",
      "              num_parallel_tree=1, predictor='auto', random_state=3,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)\n",
      "kfold  10  score: 94.43%\n",
      "kfold  10  score: 98.62%\n",
      "[21:48:03] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:48:03] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
      "              gamma=0, gpu_id=-1, importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.300000012,\n",
      "              max_delta_step=0, max_depth=12, max_leaf_nodes=None,\n",
      "              min_child_weight=1, min_samples_split=1, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=10, n_jobs=8,\n",
      "              num_parallel_tree=1, predictor='auto', random_state=3,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)\n",
      "kfold  10  score: 94.79%\n",
      "kfold  10  score: 99.16%\n",
      "[21:48:03] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21:48:03] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
      "              gamma=0, gpu_id=-1, importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.300000012,\n",
      "              max_delta_step=0, max_depth=12, max_leaf_nodes=None,\n",
      "              min_child_weight=1, min_samples_split=1, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=10, n_jobs=8,\n",
      "              num_parallel_tree=1, predictor='auto', random_state=3,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)\n",
      "kfold  10  score: 94.61%\n",
      "kfold  10  score: 99.04%\n",
      "[21:48:03] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:48:03] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
      "              gamma=0, gpu_id=-1, importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.300000012,\n",
      "              max_delta_step=0, max_depth=12, max_leaf_nodes=None,\n",
      "              min_child_weight=1, min_samples_split=1, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=10, n_jobs=8,\n",
      "              num_parallel_tree=1, predictor='auto', random_state=3,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)\n",
      "kfold  10  score: 93.00%\n",
      "kfold  10  score: 98.78%\n",
      "[21:48:03] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:48:03] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
      "              gamma=0, gpu_id=-1, importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.300000012,\n",
      "              max_delta_step=0, max_depth=12, max_leaf_nodes=None,\n",
      "              min_child_weight=1, min_samples_split=1, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=10, n_jobs=8,\n",
      "              num_parallel_tree=1, predictor='auto', random_state=3,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)\n",
      "kfold  10  score: 93.18%\n",
      "kfold  10  score: 98.88%\n",
      "[21:48:03] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:48:03] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
      "              gamma=0, gpu_id=-1, importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.300000012,\n",
      "              max_delta_step=0, max_depth=12, max_leaf_nodes=None,\n",
      "              min_child_weight=1, min_samples_split=1, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=10, n_jobs=8,\n",
      "              num_parallel_tree=1, predictor='auto', random_state=3,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)\n",
      "kfold  10  score: 94.25%\n",
      "kfold  10  score: 99.14%\n",
      "[21:48:03] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:48:03] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
      "              gamma=0, gpu_id=-1, importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.300000012,\n",
      "              max_delta_step=0, max_depth=12, max_leaf_nodes=None,\n",
      "              min_child_weight=1, min_samples_split=1, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=10, n_jobs=8,\n",
      "              num_parallel_tree=1, predictor='auto', random_state=3,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)\n",
      "kfold  10  score: 94.79%\n",
      "kfold  10  score: 99.04%\n",
      "[21:48:04] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:48:04] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
      "              gamma=0, gpu_id=-1, importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.300000012,\n",
      "              max_delta_step=0, max_depth=12, max_leaf_nodes=None,\n",
      "              min_child_weight=1, min_samples_split=1, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=10, n_jobs=8,\n",
      "              num_parallel_tree=1, predictor='auto', random_state=3,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)\n",
      "kfold  10  score: 94.43%\n",
      "kfold  10  score: 98.90%\n",
      "[21:48:04] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:48:04] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
      "              gamma=0, gpu_id=-1, importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.300000012,\n",
      "              max_delta_step=0, max_depth=12, max_leaf_nodes=None,\n",
      "              min_child_weight=1, min_samples_split=1, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=10, n_jobs=8,\n",
      "              num_parallel_tree=1, predictor='auto', random_state=3,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)\n",
      "kfold  10  score: 95.50%\n",
      "kfold  10  score: 98.92%\n",
      "Average score over k-fold 94.11% (+/-0.97%) [XGBClassifier]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21:48:04] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:48:04] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "VotingClassifier(estimators=[('gb',\n",
      "                              GradientBoostingClassifier(max_depth=12,\n",
      "                                                         min_samples_split=12,\n",
      "                                                         n_estimators=5,\n",
      "                                                         random_state=3)),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(max_depth=12,\n",
      "                                                     n_estimators=10,\n",
      "                                                     random_state=3)),\n",
      "                             ('xgb',\n",
      "                              XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "                                            colsample_bylevel=1,\n",
      "                                            colsample_bynode=1,\n",
      "                                            colsample_bytree=1,\n",
      "                                            enable_categorical=False, gamma=0,\n",
      "                                            gpu...\n",
      "                                            interaction_constraints='',\n",
      "                                            learning_rate=0.300000012,\n",
      "                                            max_delta_step=0, max_depth=12,\n",
      "                                            max_leaf_nodes=None,\n",
      "                                            min_child_weight=1,\n",
      "                                            min_samples_split=1, missing=nan,\n",
      "                                            monotone_constraints='()',\n",
      "                                            n_estimators=10, n_jobs=8,\n",
      "                                            num_parallel_tree=1,\n",
      "                                            predictor='auto', random_state=3,\n",
      "                                            reg_alpha=0, reg_lambda=1,\n",
      "                                            scale_pos_weight=1, subsample=1,\n",
      "                                            tree_method='exact',\n",
      "                                            validate_parameters=1,\n",
      "                                            verbosity=None))])\n",
      "kfold  10  score: 90.84%\n",
      "kfold  10  score: 97.39%\n",
      "[21:48:05] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:48:05] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "VotingClassifier(estimators=[('gb',\n",
      "                              GradientBoostingClassifier(max_depth=12,\n",
      "                                                         min_samples_split=12,\n",
      "                                                         n_estimators=5,\n",
      "                                                         random_state=3)),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(max_depth=12,\n",
      "                                                     n_estimators=10,\n",
      "                                                     random_state=3)),\n",
      "                             ('xgb',\n",
      "                              XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "                                            colsample_bylevel=1,\n",
      "                                            colsample_bynode=1,\n",
      "                                            colsample_bytree=1,\n",
      "                                            enable_categorical=False, gamma=0,\n",
      "                                            gpu...\n",
      "                                            interaction_constraints='',\n",
      "                                            learning_rate=0.300000012,\n",
      "                                            max_delta_step=0, max_depth=12,\n",
      "                                            max_leaf_nodes=None,\n",
      "                                            min_child_weight=1,\n",
      "                                            min_samples_split=1, missing=nan,\n",
      "                                            monotone_constraints='()',\n",
      "                                            n_estimators=10, n_jobs=8,\n",
      "                                            num_parallel_tree=1,\n",
      "                                            predictor='auto', random_state=3,\n",
      "                                            reg_alpha=0, reg_lambda=1,\n",
      "                                            scale_pos_weight=1, subsample=1,\n",
      "                                            tree_method='exact',\n",
      "                                            validate_parameters=1,\n",
      "                                            verbosity=None))])\n",
      "kfold  10  score: 93.36%\n",
      "kfold  10  score: 97.43%\n",
      "[21:48:05] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:48:05] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "VotingClassifier(estimators=[('gb',\n",
      "                              GradientBoostingClassifier(max_depth=12,\n",
      "                                                         min_samples_split=12,\n",
      "                                                         n_estimators=5,\n",
      "                                                         random_state=3)),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(max_depth=12,\n",
      "                                                     n_estimators=10,\n",
      "                                                     random_state=3)),\n",
      "                             ('xgb',\n",
      "                              XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "                                            colsample_bylevel=1,\n",
      "                                            colsample_bynode=1,\n",
      "                                            colsample_bytree=1,\n",
      "                                            enable_categorical=False, gamma=0,\n",
      "                                            gpu...\n",
      "                                            interaction_constraints='',\n",
      "                                            learning_rate=0.300000012,\n",
      "                                            max_delta_step=0, max_depth=12,\n",
      "                                            max_leaf_nodes=None,\n",
      "                                            min_child_weight=1,\n",
      "                                            min_samples_split=1, missing=nan,\n",
      "                                            monotone_constraints='()',\n",
      "                                            n_estimators=10, n_jobs=8,\n",
      "                                            num_parallel_tree=1,\n",
      "                                            predictor='auto', random_state=3,\n",
      "                                            reg_alpha=0, reg_lambda=1,\n",
      "                                            scale_pos_weight=1, subsample=1,\n",
      "                                            tree_method='exact',\n",
      "                                            validate_parameters=1,\n",
      "                                            verbosity=None))])\n",
      "kfold  10  score: 94.43%\n",
      "kfold  10  score: 98.62%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21:48:06] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:48:06] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "VotingClassifier(estimators=[('gb',\n",
      "                              GradientBoostingClassifier(max_depth=12,\n",
      "                                                         min_samples_split=12,\n",
      "                                                         n_estimators=5,\n",
      "                                                         random_state=3)),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(max_depth=12,\n",
      "                                                     n_estimators=10,\n",
      "                                                     random_state=3)),\n",
      "                             ('xgb',\n",
      "                              XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "                                            colsample_bylevel=1,\n",
      "                                            colsample_bynode=1,\n",
      "                                            colsample_bytree=1,\n",
      "                                            enable_categorical=False, gamma=0,\n",
      "                                            gpu...\n",
      "                                            interaction_constraints='',\n",
      "                                            learning_rate=0.300000012,\n",
      "                                            max_delta_step=0, max_depth=12,\n",
      "                                            max_leaf_nodes=None,\n",
      "                                            min_child_weight=1,\n",
      "                                            min_samples_split=1, missing=nan,\n",
      "                                            monotone_constraints='()',\n",
      "                                            n_estimators=10, n_jobs=8,\n",
      "                                            num_parallel_tree=1,\n",
      "                                            predictor='auto', random_state=3,\n",
      "                                            reg_alpha=0, reg_lambda=1,\n",
      "                                            scale_pos_weight=1, subsample=1,\n",
      "                                            tree_method='exact',\n",
      "                                            validate_parameters=1,\n",
      "                                            verbosity=None))])\n",
      "kfold  10  score: 94.25%\n",
      "kfold  10  score: 97.89%\n",
      "[21:48:06] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:48:06] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "VotingClassifier(estimators=[('gb',\n",
      "                              GradientBoostingClassifier(max_depth=12,\n",
      "                                                         min_samples_split=12,\n",
      "                                                         n_estimators=5,\n",
      "                                                         random_state=3)),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(max_depth=12,\n",
      "                                                     n_estimators=10,\n",
      "                                                     random_state=3)),\n",
      "                             ('xgb',\n",
      "                              XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "                                            colsample_bylevel=1,\n",
      "                                            colsample_bynode=1,\n",
      "                                            colsample_bytree=1,\n",
      "                                            enable_categorical=False, gamma=0,\n",
      "                                            gpu...\n",
      "                                            interaction_constraints='',\n",
      "                                            learning_rate=0.300000012,\n",
      "                                            max_delta_step=0, max_depth=12,\n",
      "                                            max_leaf_nodes=None,\n",
      "                                            min_child_weight=1,\n",
      "                                            min_samples_split=1, missing=nan,\n",
      "                                            monotone_constraints='()',\n",
      "                                            n_estimators=10, n_jobs=8,\n",
      "                                            num_parallel_tree=1,\n",
      "                                            predictor='auto', random_state=3,\n",
      "                                            reg_alpha=0, reg_lambda=1,\n",
      "                                            scale_pos_weight=1, subsample=1,\n",
      "                                            tree_method='exact',\n",
      "                                            validate_parameters=1,\n",
      "                                            verbosity=None))])\n",
      "kfold  10  score: 92.46%\n",
      "kfold  10  score: 98.14%\n",
      "[21:48:07] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:48:07] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "VotingClassifier(estimators=[('gb',\n",
      "                              GradientBoostingClassifier(max_depth=12,\n",
      "                                                         min_samples_split=12,\n",
      "                                                         n_estimators=5,\n",
      "                                                         random_state=3)),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(max_depth=12,\n",
      "                                                     n_estimators=10,\n",
      "                                                     random_state=3)),\n",
      "                             ('xgb',\n",
      "                              XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "                                            colsample_bylevel=1,\n",
      "                                            colsample_bynode=1,\n",
      "                                            colsample_bytree=1,\n",
      "                                            enable_categorical=False, gamma=0,\n",
      "                                            gpu...\n",
      "                                            interaction_constraints='',\n",
      "                                            learning_rate=0.300000012,\n",
      "                                            max_delta_step=0, max_depth=12,\n",
      "                                            max_leaf_nodes=None,\n",
      "                                            min_child_weight=1,\n",
      "                                            min_samples_split=1, missing=nan,\n",
      "                                            monotone_constraints='()',\n",
      "                                            n_estimators=10, n_jobs=8,\n",
      "                                            num_parallel_tree=1,\n",
      "                                            predictor='auto', random_state=3,\n",
      "                                            reg_alpha=0, reg_lambda=1,\n",
      "                                            scale_pos_weight=1, subsample=1,\n",
      "                                            tree_method='exact',\n",
      "                                            validate_parameters=1,\n",
      "                                            verbosity=None))])\n",
      "kfold  10  score: 91.20%\n",
      "kfold  10  score: 97.59%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21:48:07] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:48:07] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "VotingClassifier(estimators=[('gb',\n",
      "                              GradientBoostingClassifier(max_depth=12,\n",
      "                                                         min_samples_split=12,\n",
      "                                                         n_estimators=5,\n",
      "                                                         random_state=3)),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(max_depth=12,\n",
      "                                                     n_estimators=10,\n",
      "                                                     random_state=3)),\n",
      "                             ('xgb',\n",
      "                              XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "                                            colsample_bylevel=1,\n",
      "                                            colsample_bynode=1,\n",
      "                                            colsample_bytree=1,\n",
      "                                            enable_categorical=False, gamma=0,\n",
      "                                            gpu...\n",
      "                                            interaction_constraints='',\n",
      "                                            learning_rate=0.300000012,\n",
      "                                            max_delta_step=0, max_depth=12,\n",
      "                                            max_leaf_nodes=None,\n",
      "                                            min_child_weight=1,\n",
      "                                            min_samples_split=1, missing=nan,\n",
      "                                            monotone_constraints='()',\n",
      "                                            n_estimators=10, n_jobs=8,\n",
      "                                            num_parallel_tree=1,\n",
      "                                            predictor='auto', random_state=3,\n",
      "                                            reg_alpha=0, reg_lambda=1,\n",
      "                                            scale_pos_weight=1, subsample=1,\n",
      "                                            tree_method='exact',\n",
      "                                            validate_parameters=1,\n",
      "                                            verbosity=None))])\n",
      "kfold  10  score: 94.61%\n",
      "kfold  10  score: 98.00%\n",
      "[21:48:08] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:48:08] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "VotingClassifier(estimators=[('gb',\n",
      "                              GradientBoostingClassifier(max_depth=12,\n",
      "                                                         min_samples_split=12,\n",
      "                                                         n_estimators=5,\n",
      "                                                         random_state=3)),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(max_depth=12,\n",
      "                                                     n_estimators=10,\n",
      "                                                     random_state=3)),\n",
      "                             ('xgb',\n",
      "                              XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "                                            colsample_bylevel=1,\n",
      "                                            colsample_bynode=1,\n",
      "                                            colsample_bytree=1,\n",
      "                                            enable_categorical=False, gamma=0,\n",
      "                                            gpu...\n",
      "                                            interaction_constraints='',\n",
      "                                            learning_rate=0.300000012,\n",
      "                                            max_delta_step=0, max_depth=12,\n",
      "                                            max_leaf_nodes=None,\n",
      "                                            min_child_weight=1,\n",
      "                                            min_samples_split=1, missing=nan,\n",
      "                                            monotone_constraints='()',\n",
      "                                            n_estimators=10, n_jobs=8,\n",
      "                                            num_parallel_tree=1,\n",
      "                                            predictor='auto', random_state=3,\n",
      "                                            reg_alpha=0, reg_lambda=1,\n",
      "                                            scale_pos_weight=1, subsample=1,\n",
      "                                            tree_method='exact',\n",
      "                                            validate_parameters=1,\n",
      "                                            verbosity=None))])\n",
      "kfold  10  score: 91.74%\n",
      "kfold  10  score: 97.15%\n",
      "[21:48:08] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:48:08] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "VotingClassifier(estimators=[('gb',\n",
      "                              GradientBoostingClassifier(max_depth=12,\n",
      "                                                         min_samples_split=12,\n",
      "                                                         n_estimators=5,\n",
      "                                                         random_state=3)),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(max_depth=12,\n",
      "                                                     n_estimators=10,\n",
      "                                                     random_state=3)),\n",
      "                             ('xgb',\n",
      "                              XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "                                            colsample_bylevel=1,\n",
      "                                            colsample_bynode=1,\n",
      "                                            colsample_bytree=1,\n",
      "                                            enable_categorical=False, gamma=0,\n",
      "                                            gpu...\n",
      "                                            interaction_constraints='',\n",
      "                                            learning_rate=0.300000012,\n",
      "                                            max_delta_step=0, max_depth=12,\n",
      "                                            max_leaf_nodes=None,\n",
      "                                            min_child_weight=1,\n",
      "                                            min_samples_split=1, missing=nan,\n",
      "                                            monotone_constraints='()',\n",
      "                                            n_estimators=10, n_jobs=8,\n",
      "                                            num_parallel_tree=1,\n",
      "                                            predictor='auto', random_state=3,\n",
      "                                            reg_alpha=0, reg_lambda=1,\n",
      "                                            scale_pos_weight=1, subsample=1,\n",
      "                                            tree_method='exact',\n",
      "                                            validate_parameters=1,\n",
      "                                            verbosity=None))])\n",
      "kfold  10  score: 95.33%\n",
      "kfold  10  score: 97.77%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21:48:09] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:48:09] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "VotingClassifier(estimators=[('gb',\n",
      "                              GradientBoostingClassifier(max_depth=12,\n",
      "                                                         min_samples_split=12,\n",
      "                                                         n_estimators=5,\n",
      "                                                         random_state=3)),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(max_depth=12,\n",
      "                                                     n_estimators=10,\n",
      "                                                     random_state=3)),\n",
      "                             ('xgb',\n",
      "                              XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "                                            colsample_bylevel=1,\n",
      "                                            colsample_bynode=1,\n",
      "                                            colsample_bytree=1,\n",
      "                                            enable_categorical=False, gamma=0,\n",
      "                                            gpu...\n",
      "                                            interaction_constraints='',\n",
      "                                            learning_rate=0.300000012,\n",
      "                                            max_delta_step=0, max_depth=12,\n",
      "                                            max_leaf_nodes=None,\n",
      "                                            min_child_weight=1,\n",
      "                                            min_samples_split=1, missing=nan,\n",
      "                                            monotone_constraints='()',\n",
      "                                            n_estimators=10, n_jobs=8,\n",
      "                                            num_parallel_tree=1,\n",
      "                                            predictor='auto', random_state=3,\n",
      "                                            reg_alpha=0, reg_lambda=1,\n",
      "                                            scale_pos_weight=1, subsample=1,\n",
      "                                            tree_method='exact',\n",
      "                                            validate_parameters=1,\n",
      "                                            verbosity=None))])\n",
      "kfold  10  score: 93.88%\n",
      "kfold  10  score: 98.03%\n",
      "Average score over k-fold 93.21% (+/-1.48%) [Ensemble Majority]\n",
      "./Files_Majority_vote/128_features/filter11.csv\n",
      "n_estimators = 100 \n",
      "\n",
      "10-fold cross validation for file :  11\n",
      "GradientBoostingClassifier(max_depth=12, min_samples_split=12, n_estimators=5,\n",
      "                           random_state=3)\n",
      "kfold  11  score: 88.21%\n",
      "kfold  11  score: 92.40%\n",
      "GradientBoostingClassifier(max_depth=12, min_samples_split=12, n_estimators=5,\n",
      "                           random_state=3)\n",
      "kfold  11  score: 87.43%\n",
      "kfold  11  score: 91.68%\n",
      "GradientBoostingClassifier(max_depth=12, min_samples_split=12, n_estimators=5,\n",
      "                           random_state=3)\n",
      "kfold  11  score: 89.14%\n",
      "kfold  11  score: 91.75%\n",
      "GradientBoostingClassifier(max_depth=12, min_samples_split=12, n_estimators=5,\n",
      "                           random_state=3)\n",
      "kfold  11  score: 89.52%\n",
      "kfold  11  score: 92.30%\n",
      "GradientBoostingClassifier(max_depth=12, min_samples_split=12, n_estimators=5,\n",
      "                           random_state=3)\n",
      "kfold  11  score: 88.76%\n",
      "kfold  11  score: 91.56%\n",
      "GradientBoostingClassifier(max_depth=12, min_samples_split=12, n_estimators=5,\n",
      "                           random_state=3)\n",
      "kfold  11  score: 89.52%\n",
      "kfold  11  score: 93.78%\n",
      "GradientBoostingClassifier(max_depth=12, min_samples_split=12, n_estimators=5,\n",
      "                           random_state=3)\n",
      "kfold  11  score: 86.86%\n",
      "kfold  11  score: 88.76%\n",
      "GradientBoostingClassifier(max_depth=12, min_samples_split=12, n_estimators=5,\n",
      "                           random_state=3)\n",
      "kfold  11  score: 89.33%\n",
      "kfold  11  score: 91.32%\n",
      "GradientBoostingClassifier(max_depth=12, min_samples_split=12, n_estimators=5,\n",
      "                           random_state=3)\n",
      "kfold  11  score: 84.95%\n",
      "kfold  11  score: 88.30%\n",
      "GradientBoostingClassifier(max_depth=12, min_samples_split=12, n_estimators=5,\n",
      "                           random_state=3)\n",
      "kfold  11  score: 86.67%\n",
      "kfold  11  score: 92.38%\n",
      "Average score over k-fold 88.04% (+/-1.45%) [Gradient Boosting]\n",
      "RandomForestClassifier(max_depth=12, n_estimators=10, random_state=3)\n",
      "kfold  11  score: 92.02%\n",
      "kfold  11  score: 97.21%\n",
      "RandomForestClassifier(max_depth=12, n_estimators=10, random_state=3)\n",
      "kfold  11  score: 92.76%\n",
      "kfold  11  score: 97.23%\n",
      "RandomForestClassifier(max_depth=12, n_estimators=10, random_state=3)\n",
      "kfold  11  score: 92.38%\n",
      "kfold  11  score: 98.05%\n",
      "RandomForestClassifier(max_depth=12, n_estimators=10, random_state=3)\n",
      "kfold  11  score: 94.86%\n",
      "kfold  11  score: 98.46%\n",
      "RandomForestClassifier(max_depth=12, n_estimators=10, random_state=3)\n",
      "kfold  11  score: 91.81%\n",
      "kfold  11  score: 97.95%\n",
      "RandomForestClassifier(max_depth=12, n_estimators=10, random_state=3)\n",
      "kfold  11  score: 91.81%\n",
      "kfold  11  score: 97.59%\n",
      "RandomForestClassifier(max_depth=12, n_estimators=10, random_state=3)\n",
      "kfold  11  score: 94.10%\n",
      "kfold  11  score: 96.64%\n",
      "RandomForestClassifier(max_depth=12, n_estimators=10, random_state=3)\n",
      "kfold  11  score: 92.38%\n",
      "kfold  11  score: 97.23%\n",
      "RandomForestClassifier(max_depth=12, n_estimators=10, random_state=3)\n",
      "kfold  11  score: 92.76%\n",
      "kfold  11  score: 97.33%\n",
      "RandomForestClassifier(max_depth=12, n_estimators=10, random_state=3)\n",
      "kfold  11  score: 93.33%\n",
      "kfold  11  score: 97.29%\n",
      "Average score over k-fold 92.82% (+/-0.96%) [Random Forest]\n",
      "[21:48:12] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:48:12] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
      "              gamma=0, gpu_id=-1, importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.300000012,\n",
      "              max_delta_step=0, max_depth=12, max_leaf_nodes=None,\n",
      "              min_child_weight=1, min_samples_split=1, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=10, n_jobs=8,\n",
      "              num_parallel_tree=1, predictor='auto', random_state=3,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)\n",
      "kfold  11  score: 93.35%\n",
      "kfold  11  score: 99.39%\n",
      "[21:48:12] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:48:13] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
      "              gamma=0, gpu_id=-1, importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.300000012,\n",
      "              max_delta_step=0, max_depth=12, max_leaf_nodes=None,\n",
      "              min_child_weight=1, min_samples_split=1, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=10, n_jobs=8,\n",
      "              num_parallel_tree=1, predictor='auto', random_state=3,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)\n",
      "kfold  11  score: 95.05%\n",
      "kfold  11  score: 99.26%\n",
      "[21:48:13] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21:48:13] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
      "              gamma=0, gpu_id=-1, importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.300000012,\n",
      "              max_delta_step=0, max_depth=12, max_leaf_nodes=None,\n",
      "              min_child_weight=1, min_samples_split=1, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=10, n_jobs=8,\n",
      "              num_parallel_tree=1, predictor='auto', random_state=3,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)\n",
      "kfold  11  score: 93.90%\n",
      "kfold  11  score: 98.88%\n",
      "[21:48:13] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:48:13] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
      "              gamma=0, gpu_id=-1, importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.300000012,\n",
      "              max_delta_step=0, max_depth=12, max_leaf_nodes=None,\n",
      "              min_child_weight=1, min_samples_split=1, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=10, n_jobs=8,\n",
      "              num_parallel_tree=1, predictor='auto', random_state=3,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)\n",
      "kfold  11  score: 95.62%\n",
      "kfold  11  score: 99.34%\n",
      "[21:48:13] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:48:13] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
      "              gamma=0, gpu_id=-1, importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.300000012,\n",
      "              max_delta_step=0, max_depth=12, max_leaf_nodes=None,\n",
      "              min_child_weight=1, min_samples_split=1, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=10, n_jobs=8,\n",
      "              num_parallel_tree=1, predictor='auto', random_state=3,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)\n",
      "kfold  11  score: 93.71%\n",
      "kfold  11  score: 99.11%\n",
      "[21:48:13] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:48:13] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
      "              gamma=0, gpu_id=-1, importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.300000012,\n",
      "              max_delta_step=0, max_depth=12, max_leaf_nodes=None,\n",
      "              min_child_weight=1, min_samples_split=1, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=10, n_jobs=8,\n",
      "              num_parallel_tree=1, predictor='auto', random_state=3,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)\n",
      "kfold  11  score: 92.00%\n",
      "kfold  11  score: 98.98%\n",
      "[21:48:13] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:48:13] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
      "              gamma=0, gpu_id=-1, importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.300000012,\n",
      "              max_delta_step=0, max_depth=12, max_leaf_nodes=None,\n",
      "              min_child_weight=1, min_samples_split=1, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=10, n_jobs=8,\n",
      "              num_parallel_tree=1, predictor='auto', random_state=3,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)\n",
      "kfold  11  score: 94.48%\n",
      "kfold  11  score: 99.01%\n",
      "[21:48:13] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:48:13] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
      "              gamma=0, gpu_id=-1, importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.300000012,\n",
      "              max_delta_step=0, max_depth=12, max_leaf_nodes=None,\n",
      "              min_child_weight=1, min_samples_split=1, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=10, n_jobs=8,\n",
      "              num_parallel_tree=1, predictor='auto', random_state=3,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)\n",
      "kfold  11  score: 95.24%\n",
      "kfold  11  score: 99.01%\n",
      "[21:48:13] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:48:13] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
      "              gamma=0, gpu_id=-1, importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.300000012,\n",
      "              max_delta_step=0, max_depth=12, max_leaf_nodes=None,\n",
      "              min_child_weight=1, min_samples_split=1, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=10, n_jobs=8,\n",
      "              num_parallel_tree=1, predictor='auto', random_state=3,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)\n",
      "kfold  11  score: 94.10%\n",
      "kfold  11  score: 98.62%\n",
      "[21:48:13] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:48:13] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
      "              gamma=0, gpu_id=-1, importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.300000012,\n",
      "              max_delta_step=0, max_depth=12, max_leaf_nodes=None,\n",
      "              min_child_weight=1, min_samples_split=1, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=10, n_jobs=8,\n",
      "              num_parallel_tree=1, predictor='auto', random_state=3,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)\n",
      "kfold  11  score: 94.48%\n",
      "kfold  11  score: 99.17%\n",
      "Average score over k-fold 94.19% (+/-0.99%) [XGBClassifier]\n",
      "[21:48:14] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:48:14] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "VotingClassifier(estimators=[('gb',\n",
      "                              GradientBoostingClassifier(max_depth=12,\n",
      "                                                         min_samples_split=12,\n",
      "                                                         n_estimators=5,\n",
      "                                                         random_state=3)),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(max_depth=12,\n",
      "                                                     n_estimators=10,\n",
      "                                                     random_state=3)),\n",
      "                             ('xgb',\n",
      "                              XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "                                            colsample_bylevel=1,\n",
      "                                            colsample_bynode=1,\n",
      "                                            colsample_bytree=1,\n",
      "                                            enable_categorical=False, gamma=0,\n",
      "                                            gpu...\n",
      "                                            interaction_constraints='',\n",
      "                                            learning_rate=0.300000012,\n",
      "                                            max_delta_step=0, max_depth=12,\n",
      "                                            max_leaf_nodes=None,\n",
      "                                            min_child_weight=1,\n",
      "                                            min_samples_split=1, missing=nan,\n",
      "                                            monotone_constraints='()',\n",
      "                                            n_estimators=10, n_jobs=8,\n",
      "                                            num_parallel_tree=1,\n",
      "                                            predictor='auto', random_state=3,\n",
      "                                            reg_alpha=0, reg_lambda=1,\n",
      "                                            scale_pos_weight=1, subsample=1,\n",
      "                                            tree_method='exact',\n",
      "                                            validate_parameters=1,\n",
      "                                            verbosity=None))])\n",
      "kfold  11  score: 92.40%\n",
      "kfold  11  score: 97.74%\n",
      "[21:48:14] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:48:14] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "VotingClassifier(estimators=[('gb',\n",
      "                              GradientBoostingClassifier(max_depth=12,\n",
      "                                                         min_samples_split=12,\n",
      "                                                         n_estimators=5,\n",
      "                                                         random_state=3)),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(max_depth=12,\n",
      "                                                     n_estimators=10,\n",
      "                                                     random_state=3)),\n",
      "                             ('xgb',\n",
      "                              XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "                                            colsample_bylevel=1,\n",
      "                                            colsample_bynode=1,\n",
      "                                            colsample_bytree=1,\n",
      "                                            enable_categorical=False, gamma=0,\n",
      "                                            gpu...\n",
      "                                            interaction_constraints='',\n",
      "                                            learning_rate=0.300000012,\n",
      "                                            max_delta_step=0, max_depth=12,\n",
      "                                            max_leaf_nodes=None,\n",
      "                                            min_child_weight=1,\n",
      "                                            min_samples_split=1, missing=nan,\n",
      "                                            monotone_constraints='()',\n",
      "                                            n_estimators=10, n_jobs=8,\n",
      "                                            num_parallel_tree=1,\n",
      "                                            predictor='auto', random_state=3,\n",
      "                                            reg_alpha=0, reg_lambda=1,\n",
      "                                            scale_pos_weight=1, subsample=1,\n",
      "                                            tree_method='exact',\n",
      "                                            validate_parameters=1,\n",
      "                                            verbosity=None))])\n",
      "kfold  11  score: 93.52%\n",
      "kfold  11  score: 97.69%\n",
      "[21:48:15] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:48:15] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "VotingClassifier(estimators=[('gb',\n",
      "                              GradientBoostingClassifier(max_depth=12,\n",
      "                                                         min_samples_split=12,\n",
      "                                                         n_estimators=5,\n",
      "                                                         random_state=3)),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(max_depth=12,\n",
      "                                                     n_estimators=10,\n",
      "                                                     random_state=3)),\n",
      "                             ('xgb',\n",
      "                              XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "                                            colsample_bylevel=1,\n",
      "                                            colsample_bynode=1,\n",
      "                                            colsample_bytree=1,\n",
      "                                            enable_categorical=False, gamma=0,\n",
      "                                            gpu...\n",
      "                                            interaction_constraints='',\n",
      "                                            learning_rate=0.300000012,\n",
      "                                            max_delta_step=0, max_depth=12,\n",
      "                                            max_leaf_nodes=None,\n",
      "                                            min_child_weight=1,\n",
      "                                            min_samples_split=1, missing=nan,\n",
      "                                            monotone_constraints='()',\n",
      "                                            n_estimators=10, n_jobs=8,\n",
      "                                            num_parallel_tree=1,\n",
      "                                            predictor='auto', random_state=3,\n",
      "                                            reg_alpha=0, reg_lambda=1,\n",
      "                                            scale_pos_weight=1, subsample=1,\n",
      "                                            tree_method='exact',\n",
      "                                            validate_parameters=1,\n",
      "                                            verbosity=None))])\n",
      "kfold  11  score: 94.29%\n",
      "kfold  11  score: 98.12%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21:48:15] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:48:15] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "VotingClassifier(estimators=[('gb',\n",
      "                              GradientBoostingClassifier(max_depth=12,\n",
      "                                                         min_samples_split=12,\n",
      "                                                         n_estimators=5,\n",
      "                                                         random_state=3)),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(max_depth=12,\n",
      "                                                     n_estimators=10,\n",
      "                                                     random_state=3)),\n",
      "                             ('xgb',\n",
      "                              XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "                                            colsample_bylevel=1,\n",
      "                                            colsample_bynode=1,\n",
      "                                            colsample_bytree=1,\n",
      "                                            enable_categorical=False, gamma=0,\n",
      "                                            gpu...\n",
      "                                            interaction_constraints='',\n",
      "                                            learning_rate=0.300000012,\n",
      "                                            max_delta_step=0, max_depth=12,\n",
      "                                            max_leaf_nodes=None,\n",
      "                                            min_child_weight=1,\n",
      "                                            min_samples_split=1, missing=nan,\n",
      "                                            monotone_constraints='()',\n",
      "                                            n_estimators=10, n_jobs=8,\n",
      "                                            num_parallel_tree=1,\n",
      "                                            predictor='auto', random_state=3,\n",
      "                                            reg_alpha=0, reg_lambda=1,\n",
      "                                            scale_pos_weight=1, subsample=1,\n",
      "                                            tree_method='exact',\n",
      "                                            validate_parameters=1,\n",
      "                                            verbosity=None))])\n",
      "kfold  11  score: 95.81%\n",
      "kfold  11  score: 98.56%\n",
      "[21:48:16] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:48:16] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "VotingClassifier(estimators=[('gb',\n",
      "                              GradientBoostingClassifier(max_depth=12,\n",
      "                                                         min_samples_split=12,\n",
      "                                                         n_estimators=5,\n",
      "                                                         random_state=3)),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(max_depth=12,\n",
      "                                                     n_estimators=10,\n",
      "                                                     random_state=3)),\n",
      "                             ('xgb',\n",
      "                              XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "                                            colsample_bylevel=1,\n",
      "                                            colsample_bynode=1,\n",
      "                                            colsample_bytree=1,\n",
      "                                            enable_categorical=False, gamma=0,\n",
      "                                            gpu...\n",
      "                                            interaction_constraints='',\n",
      "                                            learning_rate=0.300000012,\n",
      "                                            max_delta_step=0, max_depth=12,\n",
      "                                            max_leaf_nodes=None,\n",
      "                                            min_child_weight=1,\n",
      "                                            min_samples_split=1, missing=nan,\n",
      "                                            monotone_constraints='()',\n",
      "                                            n_estimators=10, n_jobs=8,\n",
      "                                            num_parallel_tree=1,\n",
      "                                            predictor='auto', random_state=3,\n",
      "                                            reg_alpha=0, reg_lambda=1,\n",
      "                                            scale_pos_weight=1, subsample=1,\n",
      "                                            tree_method='exact',\n",
      "                                            validate_parameters=1,\n",
      "                                            verbosity=None))])\n",
      "kfold  11  score: 92.95%\n",
      "kfold  11  score: 98.24%\n",
      "[21:48:16] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:48:16] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "VotingClassifier(estimators=[('gb',\n",
      "                              GradientBoostingClassifier(max_depth=12,\n",
      "                                                         min_samples_split=12,\n",
      "                                                         n_estimators=5,\n",
      "                                                         random_state=3)),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(max_depth=12,\n",
      "                                                     n_estimators=10,\n",
      "                                                     random_state=3)),\n",
      "                             ('xgb',\n",
      "                              XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "                                            colsample_bylevel=1,\n",
      "                                            colsample_bynode=1,\n",
      "                                            colsample_bytree=1,\n",
      "                                            enable_categorical=False, gamma=0,\n",
      "                                            gpu...\n",
      "                                            interaction_constraints='',\n",
      "                                            learning_rate=0.300000012,\n",
      "                                            max_delta_step=0, max_depth=12,\n",
      "                                            max_leaf_nodes=None,\n",
      "                                            min_child_weight=1,\n",
      "                                            min_samples_split=1, missing=nan,\n",
      "                                            monotone_constraints='()',\n",
      "                                            n_estimators=10, n_jobs=8,\n",
      "                                            num_parallel_tree=1,\n",
      "                                            predictor='auto', random_state=3,\n",
      "                                            reg_alpha=0, reg_lambda=1,\n",
      "                                            scale_pos_weight=1, subsample=1,\n",
      "                                            tree_method='exact',\n",
      "                                            validate_parameters=1,\n",
      "                                            verbosity=None))])\n",
      "kfold  11  score: 92.38%\n",
      "kfold  11  score: 98.26%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21:48:17] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:48:17] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "VotingClassifier(estimators=[('gb',\n",
      "                              GradientBoostingClassifier(max_depth=12,\n",
      "                                                         min_samples_split=12,\n",
      "                                                         n_estimators=5,\n",
      "                                                         random_state=3)),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(max_depth=12,\n",
      "                                                     n_estimators=10,\n",
      "                                                     random_state=3)),\n",
      "                             ('xgb',\n",
      "                              XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "                                            colsample_bylevel=1,\n",
      "                                            colsample_bynode=1,\n",
      "                                            colsample_bytree=1,\n",
      "                                            enable_categorical=False, gamma=0,\n",
      "                                            gpu...\n",
      "                                            interaction_constraints='',\n",
      "                                            learning_rate=0.300000012,\n",
      "                                            max_delta_step=0, max_depth=12,\n",
      "                                            max_leaf_nodes=None,\n",
      "                                            min_child_weight=1,\n",
      "                                            min_samples_split=1, missing=nan,\n",
      "                                            monotone_constraints='()',\n",
      "                                            n_estimators=10, n_jobs=8,\n",
      "                                            num_parallel_tree=1,\n",
      "                                            predictor='auto', random_state=3,\n",
      "                                            reg_alpha=0, reg_lambda=1,\n",
      "                                            scale_pos_weight=1, subsample=1,\n",
      "                                            tree_method='exact',\n",
      "                                            validate_parameters=1,\n",
      "                                            verbosity=None))])\n",
      "kfold  11  score: 93.71%\n",
      "kfold  11  score: 97.29%\n",
      "[21:48:17] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:48:17] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "VotingClassifier(estimators=[('gb',\n",
      "                              GradientBoostingClassifier(max_depth=12,\n",
      "                                                         min_samples_split=12,\n",
      "                                                         n_estimators=5,\n",
      "                                                         random_state=3)),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(max_depth=12,\n",
      "                                                     n_estimators=10,\n",
      "                                                     random_state=3)),\n",
      "                             ('xgb',\n",
      "                              XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "                                            colsample_bylevel=1,\n",
      "                                            colsample_bynode=1,\n",
      "                                            colsample_bytree=1,\n",
      "                                            enable_categorical=False, gamma=0,\n",
      "                                            gpu...\n",
      "                                            interaction_constraints='',\n",
      "                                            learning_rate=0.300000012,\n",
      "                                            max_delta_step=0, max_depth=12,\n",
      "                                            max_leaf_nodes=None,\n",
      "                                            min_child_weight=1,\n",
      "                                            min_samples_split=1, missing=nan,\n",
      "                                            monotone_constraints='()',\n",
      "                                            n_estimators=10, n_jobs=8,\n",
      "                                            num_parallel_tree=1,\n",
      "                                            predictor='auto', random_state=3,\n",
      "                                            reg_alpha=0, reg_lambda=1,\n",
      "                                            scale_pos_weight=1, subsample=1,\n",
      "                                            tree_method='exact',\n",
      "                                            validate_parameters=1,\n",
      "                                            verbosity=None))])\n",
      "kfold  11  score: 93.71%\n",
      "kfold  11  score: 97.84%\n",
      "[21:48:18] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:48:18] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "VotingClassifier(estimators=[('gb',\n",
      "                              GradientBoostingClassifier(max_depth=12,\n",
      "                                                         min_samples_split=12,\n",
      "                                                         n_estimators=5,\n",
      "                                                         random_state=3)),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(max_depth=12,\n",
      "                                                     n_estimators=10,\n",
      "                                                     random_state=3)),\n",
      "                             ('xgb',\n",
      "                              XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "                                            colsample_bylevel=1,\n",
      "                                            colsample_bynode=1,\n",
      "                                            colsample_bytree=1,\n",
      "                                            enable_categorical=False, gamma=0,\n",
      "                                            gpu...\n",
      "                                            interaction_constraints='',\n",
      "                                            learning_rate=0.300000012,\n",
      "                                            max_delta_step=0, max_depth=12,\n",
      "                                            max_leaf_nodes=None,\n",
      "                                            min_child_weight=1,\n",
      "                                            min_samples_split=1, missing=nan,\n",
      "                                            monotone_constraints='()',\n",
      "                                            n_estimators=10, n_jobs=8,\n",
      "                                            num_parallel_tree=1,\n",
      "                                            predictor='auto', random_state=3,\n",
      "                                            reg_alpha=0, reg_lambda=1,\n",
      "                                            scale_pos_weight=1, subsample=1,\n",
      "                                            tree_method='exact',\n",
      "                                            validate_parameters=1,\n",
      "                                            verbosity=None))])\n",
      "kfold  11  score: 93.33%\n",
      "kfold  11  score: 97.31%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21:48:18] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:48:18] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "VotingClassifier(estimators=[('gb',\n",
      "                              GradientBoostingClassifier(max_depth=12,\n",
      "                                                         min_samples_split=12,\n",
      "                                                         n_estimators=5,\n",
      "                                                         random_state=3)),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(max_depth=12,\n",
      "                                                     n_estimators=10,\n",
      "                                                     random_state=3)),\n",
      "                             ('xgb',\n",
      "                              XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "                                            colsample_bylevel=1,\n",
      "                                            colsample_bynode=1,\n",
      "                                            colsample_bytree=1,\n",
      "                                            enable_categorical=False, gamma=0,\n",
      "                                            gpu...\n",
      "                                            interaction_constraints='',\n",
      "                                            learning_rate=0.300000012,\n",
      "                                            max_delta_step=0, max_depth=12,\n",
      "                                            max_leaf_nodes=None,\n",
      "                                            min_child_weight=1,\n",
      "                                            min_samples_split=1, missing=nan,\n",
      "                                            monotone_constraints='()',\n",
      "                                            n_estimators=10, n_jobs=8,\n",
      "                                            num_parallel_tree=1,\n",
      "                                            predictor='auto', random_state=3,\n",
      "                                            reg_alpha=0, reg_lambda=1,\n",
      "                                            scale_pos_weight=1, subsample=1,\n",
      "                                            tree_method='exact',\n",
      "                                            validate_parameters=1,\n",
      "                                            verbosity=None))])\n",
      "kfold  11  score: 93.33%\n",
      "kfold  11  score: 97.50%\n",
      "Average score over k-fold 93.54% (+/-0.94%) [Ensemble Majority]\n",
      "./Files_Majority_vote/128_features/filter12.csv\n",
      "n_estimators = 100 \n",
      "\n",
      "10-fold cross validation for file :  12\n",
      "GradientBoostingClassifier(max_depth=12, min_samples_split=12, n_estimators=5,\n",
      "                           random_state=3)\n",
      "kfold  12  score: 90.25%\n",
      "kfold  12  score: 93.28%\n",
      "GradientBoostingClassifier(max_depth=12, min_samples_split=12, n_estimators=5,\n",
      "                           random_state=3)\n",
      "kfold  12  score: 88.15%\n",
      "kfold  12  score: 93.13%\n",
      "GradientBoostingClassifier(max_depth=12, min_samples_split=12, n_estimators=5,\n",
      "                           random_state=3)\n",
      "kfold  12  score: 89.29%\n",
      "kfold  12  score: 92.47%\n",
      "GradientBoostingClassifier(max_depth=12, min_samples_split=12, n_estimators=5,\n",
      "                           random_state=3)\n",
      "kfold  12  score: 90.06%\n",
      "kfold  12  score: 94.13%\n",
      "GradientBoostingClassifier(max_depth=12, min_samples_split=12, n_estimators=5,\n",
      "                           random_state=3)\n",
      "kfold  12  score: 88.89%\n",
      "kfold  12  score: 94.07%\n",
      "GradientBoostingClassifier(max_depth=12, min_samples_split=12, n_estimators=5,\n",
      "                           random_state=3)\n",
      "kfold  12  score: 89.85%\n",
      "kfold  12  score: 92.00%\n",
      "GradientBoostingClassifier(max_depth=12, min_samples_split=12, n_estimators=5,\n",
      "                           random_state=3)\n",
      "kfold  12  score: 89.46%\n",
      "kfold  12  score: 93.96%\n",
      "GradientBoostingClassifier(max_depth=12, min_samples_split=12, n_estimators=5,\n",
      "                           random_state=3)\n",
      "kfold  12  score: 92.91%\n",
      "kfold  12  score: 95.02%\n",
      "GradientBoostingClassifier(max_depth=12, min_samples_split=12, n_estimators=5,\n",
      "                           random_state=3)\n",
      "kfold  12  score: 90.80%\n",
      "kfold  12  score: 93.26%\n",
      "GradientBoostingClassifier(max_depth=12, min_samples_split=12, n_estimators=5,\n",
      "                           random_state=3)\n",
      "kfold  12  score: 91.00%\n",
      "kfold  12  score: 93.62%\n",
      "Average score over k-fold 90.07% (+/-1.25%) [Gradient Boosting]\n",
      "RandomForestClassifier(max_depth=12, n_estimators=10, random_state=3)\n",
      "kfold  12  score: 92.73%\n",
      "kfold  12  score: 96.72%\n",
      "RandomForestClassifier(max_depth=12, n_estimators=10, random_state=3)\n",
      "kfold  12  score: 92.35%\n",
      "kfold  12  score: 96.51%\n",
      "RandomForestClassifier(max_depth=12, n_estimators=10, random_state=3)\n",
      "kfold  12  score: 92.35%\n",
      "kfold  12  score: 96.68%\n",
      "RandomForestClassifier(max_depth=12, n_estimators=10, random_state=3)\n",
      "kfold  12  score: 91.59%\n",
      "kfold  12  score: 96.75%\n",
      "RandomForestClassifier(max_depth=12, n_estimators=10, random_state=3)\n",
      "kfold  12  score: 92.53%\n",
      "kfold  12  score: 97.81%\n",
      "RandomForestClassifier(max_depth=12, n_estimators=10, random_state=3)\n",
      "kfold  12  score: 93.10%\n",
      "kfold  12  score: 96.77%\n",
      "RandomForestClassifier(max_depth=12, n_estimators=10, random_state=3)\n",
      "kfold  12  score: 94.06%\n",
      "kfold  12  score: 97.60%\n",
      "RandomForestClassifier(max_depth=12, n_estimators=10, random_state=3)\n",
      "kfold  12  score: 92.91%\n",
      "kfold  12  score: 96.21%\n",
      "RandomForestClassifier(max_depth=12, n_estimators=10, random_state=3)\n",
      "kfold  12  score: 93.30%\n",
      "kfold  12  score: 97.00%\n",
      "RandomForestClassifier(max_depth=12, n_estimators=10, random_state=3)\n",
      "kfold  12  score: 93.49%\n",
      "kfold  12  score: 96.51%\n",
      "Average score over k-fold 92.84% (+/-0.66%) [Random Forest]\n",
      "[21:48:22] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:48:22] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
      "              gamma=0, gpu_id=-1, importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.300000012,\n",
      "              max_delta_step=0, max_depth=12, max_leaf_nodes=None,\n",
      "              min_child_weight=1, min_samples_split=1, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=10, n_jobs=8,\n",
      "              num_parallel_tree=1, predictor='auto', random_state=3,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)\n",
      "kfold  12  score: 95.03%\n",
      "kfold  12  score: 99.32%\n",
      "[21:48:22] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:48:22] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
      "              gamma=0, gpu_id=-1, importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.300000012,\n",
      "              max_delta_step=0, max_depth=12, max_leaf_nodes=None,\n",
      "              min_child_weight=1, min_samples_split=1, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=10, n_jobs=8,\n",
      "              num_parallel_tree=1, predictor='auto', random_state=3,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)\n",
      "kfold  12  score: 94.65%\n",
      "kfold  12  score: 99.04%\n",
      "[21:48:22] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:48:22] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
      "              gamma=0, gpu_id=-1, importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.300000012,\n",
      "              max_delta_step=0, max_depth=12, max_leaf_nodes=None,\n",
      "              min_child_weight=1, min_samples_split=1, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=10, n_jobs=8,\n",
      "              num_parallel_tree=1, predictor='auto', random_state=3,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)\n",
      "kfold  12  score: 95.98%\n",
      "kfold  12  score: 99.64%\n",
      "[21:48:22] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:48:22] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
      "              gamma=0, gpu_id=-1, importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.300000012,\n",
      "              max_delta_step=0, max_depth=12, max_leaf_nodes=None,\n",
      "              min_child_weight=1, min_samples_split=1, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=10, n_jobs=8,\n",
      "              num_parallel_tree=1, predictor='auto', random_state=3,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)\n",
      "kfold  12  score: 95.22%\n",
      "kfold  12  score: 99.49%\n",
      "[21:48:23] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:48:23] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
      "              gamma=0, gpu_id=-1, importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.300000012,\n",
      "              max_delta_step=0, max_depth=12, max_leaf_nodes=None,\n",
      "              min_child_weight=1, min_samples_split=1, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=10, n_jobs=8,\n",
      "              num_parallel_tree=1, predictor='auto', random_state=3,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)\n",
      "kfold  12  score: 92.91%\n",
      "kfold  12  score: 99.51%\n",
      "[21:48:23] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:48:23] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
      "              gamma=0, gpu_id=-1, importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.300000012,\n",
      "              max_delta_step=0, max_depth=12, max_leaf_nodes=None,\n",
      "              min_child_weight=1, min_samples_split=1, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=10, n_jobs=8,\n",
      "              num_parallel_tree=1, predictor='auto', random_state=3,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)\n",
      "kfold  12  score: 95.79%\n",
      "kfold  12  score: 99.40%\n",
      "[21:48:23] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:48:23] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
      "              gamma=0, gpu_id=-1, importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.300000012,\n",
      "              max_delta_step=0, max_depth=12, max_leaf_nodes=None,\n",
      "              min_child_weight=1, min_samples_split=1, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=10, n_jobs=8,\n",
      "              num_parallel_tree=1, predictor='auto', random_state=3,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)\n",
      "kfold  12  score: 94.06%\n",
      "kfold  12  score: 99.51%\n",
      "[21:48:23] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:48:23] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
      "              gamma=0, gpu_id=-1, importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.300000012,\n",
      "              max_delta_step=0, max_depth=12, max_leaf_nodes=None,\n",
      "              min_child_weight=1, min_samples_split=1, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=10, n_jobs=8,\n",
      "              num_parallel_tree=1, predictor='auto', random_state=3,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)\n",
      "kfold  12  score: 94.64%\n",
      "kfold  12  score: 99.32%\n",
      "[21:48:23] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:48:23] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
      "              gamma=0, gpu_id=-1, importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.300000012,\n",
      "              max_delta_step=0, max_depth=12, max_leaf_nodes=None,\n",
      "              min_child_weight=1, min_samples_split=1, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=10, n_jobs=8,\n",
      "              num_parallel_tree=1, predictor='auto', random_state=3,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)\n",
      "kfold  12  score: 94.83%\n",
      "kfold  12  score: 99.45%\n",
      "[21:48:23] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:48:23] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
      "              gamma=0, gpu_id=-1, importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.300000012,\n",
      "              max_delta_step=0, max_depth=12, max_leaf_nodes=None,\n",
      "              min_child_weight=1, min_samples_split=1, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=10, n_jobs=8,\n",
      "              num_parallel_tree=1, predictor='auto', random_state=3,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)\n",
      "kfold  12  score: 95.21%\n",
      "kfold  12  score: 99.34%\n",
      "Average score over k-fold 94.83% (+/-0.83%) [XGBClassifier]\n",
      "[21:48:24] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:48:24] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "VotingClassifier(estimators=[('gb',\n",
      "                              GradientBoostingClassifier(max_depth=12,\n",
      "                                                         min_samples_split=12,\n",
      "                                                         n_estimators=5,\n",
      "                                                         random_state=3)),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(max_depth=12,\n",
      "                                                     n_estimators=10,\n",
      "                                                     random_state=3)),\n",
      "                             ('xgb',\n",
      "                              XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "                                            colsample_bylevel=1,\n",
      "                                            colsample_bynode=1,\n",
      "                                            colsample_bytree=1,\n",
      "                                            enable_categorical=False, gamma=0,\n",
      "                                            gpu...\n",
      "                                            interaction_constraints='',\n",
      "                                            learning_rate=0.300000012,\n",
      "                                            max_delta_step=0, max_depth=12,\n",
      "                                            max_leaf_nodes=None,\n",
      "                                            min_child_weight=1,\n",
      "                                            min_samples_split=1, missing=nan,\n",
      "                                            monotone_constraints='()',\n",
      "                                            n_estimators=10, n_jobs=8,\n",
      "                                            num_parallel_tree=1,\n",
      "                                            predictor='auto', random_state=3,\n",
      "                                            reg_alpha=0, reg_lambda=1,\n",
      "                                            scale_pos_weight=1, subsample=1,\n",
      "                                            tree_method='exact',\n",
      "                                            validate_parameters=1,\n",
      "                                            verbosity=None))])\n",
      "kfold  12  score: 93.31%\n",
      "kfold  12  score: 97.00%\n",
      "[21:48:25] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:48:25] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "VotingClassifier(estimators=[('gb',\n",
      "                              GradientBoostingClassifier(max_depth=12,\n",
      "                                                         min_samples_split=12,\n",
      "                                                         n_estimators=5,\n",
      "                                                         random_state=3)),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(max_depth=12,\n",
      "                                                     n_estimators=10,\n",
      "                                                     random_state=3)),\n",
      "                             ('xgb',\n",
      "                              XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "                                            colsample_bylevel=1,\n",
      "                                            colsample_bynode=1,\n",
      "                                            colsample_bytree=1,\n",
      "                                            enable_categorical=False, gamma=0,\n",
      "                                            gpu...\n",
      "                                            interaction_constraints='',\n",
      "                                            learning_rate=0.300000012,\n",
      "                                            max_delta_step=0, max_depth=12,\n",
      "                                            max_leaf_nodes=None,\n",
      "                                            min_child_weight=1,\n",
      "                                            min_samples_split=1, missing=nan,\n",
      "                                            monotone_constraints='()',\n",
      "                                            n_estimators=10, n_jobs=8,\n",
      "                                            num_parallel_tree=1,\n",
      "                                            predictor='auto', random_state=3,\n",
      "                                            reg_alpha=0, reg_lambda=1,\n",
      "                                            scale_pos_weight=1, subsample=1,\n",
      "                                            tree_method='exact',\n",
      "                                            validate_parameters=1,\n",
      "                                            verbosity=None))])\n",
      "kfold  12  score: 92.73%\n",
      "kfold  12  score: 97.00%\n",
      "[21:48:25] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:48:25] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "VotingClassifier(estimators=[('gb',\n",
      "                              GradientBoostingClassifier(max_depth=12,\n",
      "                                                         min_samples_split=12,\n",
      "                                                         n_estimators=5,\n",
      "                                                         random_state=3)),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(max_depth=12,\n",
      "                                                     n_estimators=10,\n",
      "                                                     random_state=3)),\n",
      "                             ('xgb',\n",
      "                              XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "                                            colsample_bylevel=1,\n",
      "                                            colsample_bynode=1,\n",
      "                                            colsample_bytree=1,\n",
      "                                            enable_categorical=False, gamma=0,\n",
      "                                            gpu...\n",
      "                                            interaction_constraints='',\n",
      "                                            learning_rate=0.300000012,\n",
      "                                            max_delta_step=0, max_depth=12,\n",
      "                                            max_leaf_nodes=None,\n",
      "                                            min_child_weight=1,\n",
      "                                            min_samples_split=1, missing=nan,\n",
      "                                            monotone_constraints='()',\n",
      "                                            n_estimators=10, n_jobs=8,\n",
      "                                            num_parallel_tree=1,\n",
      "                                            predictor='auto', random_state=3,\n",
      "                                            reg_alpha=0, reg_lambda=1,\n",
      "                                            scale_pos_weight=1, subsample=1,\n",
      "                                            tree_method='exact',\n",
      "                                            validate_parameters=1,\n",
      "                                            verbosity=None))])\n",
      "kfold  12  score: 93.88%\n",
      "kfold  12  score: 97.57%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21:48:26] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:48:26] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "VotingClassifier(estimators=[('gb',\n",
      "                              GradientBoostingClassifier(max_depth=12,\n",
      "                                                         min_samples_split=12,\n",
      "                                                         n_estimators=5,\n",
      "                                                         random_state=3)),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(max_depth=12,\n",
      "                                                     n_estimators=10,\n",
      "                                                     random_state=3)),\n",
      "                             ('xgb',\n",
      "                              XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "                                            colsample_bylevel=1,\n",
      "                                            colsample_bynode=1,\n",
      "                                            colsample_bytree=1,\n",
      "                                            enable_categorical=False, gamma=0,\n",
      "                                            gpu...\n",
      "                                            interaction_constraints='',\n",
      "                                            learning_rate=0.300000012,\n",
      "                                            max_delta_step=0, max_depth=12,\n",
      "                                            max_leaf_nodes=None,\n",
      "                                            min_child_weight=1,\n",
      "                                            min_samples_split=1, missing=nan,\n",
      "                                            monotone_constraints='()',\n",
      "                                            n_estimators=10, n_jobs=8,\n",
      "                                            num_parallel_tree=1,\n",
      "                                            predictor='auto', random_state=3,\n",
      "                                            reg_alpha=0, reg_lambda=1,\n",
      "                                            scale_pos_weight=1, subsample=1,\n",
      "                                            tree_method='exact',\n",
      "                                            validate_parameters=1,\n",
      "                                            verbosity=None))])\n",
      "kfold  12  score: 92.73%\n",
      "kfold  12  score: 97.17%\n",
      "[21:48:26] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:48:26] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "VotingClassifier(estimators=[('gb',\n",
      "                              GradientBoostingClassifier(max_depth=12,\n",
      "                                                         min_samples_split=12,\n",
      "                                                         n_estimators=5,\n",
      "                                                         random_state=3)),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(max_depth=12,\n",
      "                                                     n_estimators=10,\n",
      "                                                     random_state=3)),\n",
      "                             ('xgb',\n",
      "                              XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "                                            colsample_bylevel=1,\n",
      "                                            colsample_bynode=1,\n",
      "                                            colsample_bytree=1,\n",
      "                                            enable_categorical=False, gamma=0,\n",
      "                                            gpu...\n",
      "                                            interaction_constraints='',\n",
      "                                            learning_rate=0.300000012,\n",
      "                                            max_delta_step=0, max_depth=12,\n",
      "                                            max_leaf_nodes=None,\n",
      "                                            min_child_weight=1,\n",
      "                                            min_samples_split=1, missing=nan,\n",
      "                                            monotone_constraints='()',\n",
      "                                            n_estimators=10, n_jobs=8,\n",
      "                                            num_parallel_tree=1,\n",
      "                                            predictor='auto', random_state=3,\n",
      "                                            reg_alpha=0, reg_lambda=1,\n",
      "                                            scale_pos_weight=1, subsample=1,\n",
      "                                            tree_method='exact',\n",
      "                                            validate_parameters=1,\n",
      "                                            verbosity=None))])\n",
      "kfold  12  score: 91.76%\n",
      "kfold  12  score: 98.19%\n",
      "[21:48:27] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:48:27] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "VotingClassifier(estimators=[('gb',\n",
      "                              GradientBoostingClassifier(max_depth=12,\n",
      "                                                         min_samples_split=12,\n",
      "                                                         n_estimators=5,\n",
      "                                                         random_state=3)),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(max_depth=12,\n",
      "                                                     n_estimators=10,\n",
      "                                                     random_state=3)),\n",
      "                             ('xgb',\n",
      "                              XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "                                            colsample_bylevel=1,\n",
      "                                            colsample_bynode=1,\n",
      "                                            colsample_bytree=1,\n",
      "                                            enable_categorical=False, gamma=0,\n",
      "                                            gpu...\n",
      "                                            interaction_constraints='',\n",
      "                                            learning_rate=0.300000012,\n",
      "                                            max_delta_step=0, max_depth=12,\n",
      "                                            max_leaf_nodes=None,\n",
      "                                            min_child_weight=1,\n",
      "                                            min_samples_split=1, missing=nan,\n",
      "                                            monotone_constraints='()',\n",
      "                                            n_estimators=10, n_jobs=8,\n",
      "                                            num_parallel_tree=1,\n",
      "                                            predictor='auto', random_state=3,\n",
      "                                            reg_alpha=0, reg_lambda=1,\n",
      "                                            scale_pos_weight=1, subsample=1,\n",
      "                                            tree_method='exact',\n",
      "                                            validate_parameters=1,\n",
      "                                            verbosity=None))])\n",
      "kfold  12  score: 93.68%\n",
      "kfold  12  score: 97.09%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21:48:27] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:48:27] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "VotingClassifier(estimators=[('gb',\n",
      "                              GradientBoostingClassifier(max_depth=12,\n",
      "                                                         min_samples_split=12,\n",
      "                                                         n_estimators=5,\n",
      "                                                         random_state=3)),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(max_depth=12,\n",
      "                                                     n_estimators=10,\n",
      "                                                     random_state=3)),\n",
      "                             ('xgb',\n",
      "                              XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "                                            colsample_bylevel=1,\n",
      "                                            colsample_bynode=1,\n",
      "                                            colsample_bytree=1,\n",
      "                                            enable_categorical=False, gamma=0,\n",
      "                                            gpu...\n",
      "                                            interaction_constraints='',\n",
      "                                            learning_rate=0.300000012,\n",
      "                                            max_delta_step=0, max_depth=12,\n",
      "                                            max_leaf_nodes=None,\n",
      "                                            min_child_weight=1,\n",
      "                                            min_samples_split=1, missing=nan,\n",
      "                                            monotone_constraints='()',\n",
      "                                            n_estimators=10, n_jobs=8,\n",
      "                                            num_parallel_tree=1,\n",
      "                                            predictor='auto', random_state=3,\n",
      "                                            reg_alpha=0, reg_lambda=1,\n",
      "                                            scale_pos_weight=1, subsample=1,\n",
      "                                            tree_method='exact',\n",
      "                                            validate_parameters=1,\n",
      "                                            verbosity=None))])\n",
      "kfold  12  score: 94.25%\n",
      "kfold  12  score: 98.13%\n",
      "[21:48:28] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:48:28] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "VotingClassifier(estimators=[('gb',\n",
      "                              GradientBoostingClassifier(max_depth=12,\n",
      "                                                         min_samples_split=12,\n",
      "                                                         n_estimators=5,\n",
      "                                                         random_state=3)),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(max_depth=12,\n",
      "                                                     n_estimators=10,\n",
      "                                                     random_state=3)),\n",
      "                             ('xgb',\n",
      "                              XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "                                            colsample_bylevel=1,\n",
      "                                            colsample_bynode=1,\n",
      "                                            colsample_bytree=1,\n",
      "                                            enable_categorical=False, gamma=0,\n",
      "                                            gpu...\n",
      "                                            interaction_constraints='',\n",
      "                                            learning_rate=0.300000012,\n",
      "                                            max_delta_step=0, max_depth=12,\n",
      "                                            max_leaf_nodes=None,\n",
      "                                            min_child_weight=1,\n",
      "                                            min_samples_split=1, missing=nan,\n",
      "                                            monotone_constraints='()',\n",
      "                                            n_estimators=10, n_jobs=8,\n",
      "                                            num_parallel_tree=1,\n",
      "                                            predictor='auto', random_state=3,\n",
      "                                            reg_alpha=0, reg_lambda=1,\n",
      "                                            scale_pos_weight=1, subsample=1,\n",
      "                                            tree_method='exact',\n",
      "                                            validate_parameters=1,\n",
      "                                            verbosity=None))])\n",
      "kfold  12  score: 93.30%\n",
      "kfold  12  score: 97.09%\n",
      "[21:48:28] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:48:28] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "VotingClassifier(estimators=[('gb',\n",
      "                              GradientBoostingClassifier(max_depth=12,\n",
      "                                                         min_samples_split=12,\n",
      "                                                         n_estimators=5,\n",
      "                                                         random_state=3)),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(max_depth=12,\n",
      "                                                     n_estimators=10,\n",
      "                                                     random_state=3)),\n",
      "                             ('xgb',\n",
      "                              XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "                                            colsample_bylevel=1,\n",
      "                                            colsample_bynode=1,\n",
      "                                            colsample_bytree=1,\n",
      "                                            enable_categorical=False, gamma=0,\n",
      "                                            gpu...\n",
      "                                            interaction_constraints='',\n",
      "                                            learning_rate=0.300000012,\n",
      "                                            max_delta_step=0, max_depth=12,\n",
      "                                            max_leaf_nodes=None,\n",
      "                                            min_child_weight=1,\n",
      "                                            min_samples_split=1, missing=nan,\n",
      "                                            monotone_constraints='()',\n",
      "                                            n_estimators=10, n_jobs=8,\n",
      "                                            num_parallel_tree=1,\n",
      "                                            predictor='auto', random_state=3,\n",
      "                                            reg_alpha=0, reg_lambda=1,\n",
      "                                            scale_pos_weight=1, subsample=1,\n",
      "                                            tree_method='exact',\n",
      "                                            validate_parameters=1,\n",
      "                                            verbosity=None))])\n",
      "kfold  12  score: 94.06%\n",
      "kfold  12  score: 97.83%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21:48:29] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:48:29] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "VotingClassifier(estimators=[('gb',\n",
      "                              GradientBoostingClassifier(max_depth=12,\n",
      "                                                         min_samples_split=12,\n",
      "                                                         n_estimators=5,\n",
      "                                                         random_state=3)),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(max_depth=12,\n",
      "                                                     n_estimators=10,\n",
      "                                                     random_state=3)),\n",
      "                             ('xgb',\n",
      "                              XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "                                            colsample_bylevel=1,\n",
      "                                            colsample_bynode=1,\n",
      "                                            colsample_bytree=1,\n",
      "                                            enable_categorical=False, gamma=0,\n",
      "                                            gpu...\n",
      "                                            interaction_constraints='',\n",
      "                                            learning_rate=0.300000012,\n",
      "                                            max_delta_step=0, max_depth=12,\n",
      "                                            max_leaf_nodes=None,\n",
      "                                            min_child_weight=1,\n",
      "                                            min_samples_split=1, missing=nan,\n",
      "                                            monotone_constraints='()',\n",
      "                                            n_estimators=10, n_jobs=8,\n",
      "                                            num_parallel_tree=1,\n",
      "                                            predictor='auto', random_state=3,\n",
      "                                            reg_alpha=0, reg_lambda=1,\n",
      "                                            scale_pos_weight=1, subsample=1,\n",
      "                                            tree_method='exact',\n",
      "                                            validate_parameters=1,\n",
      "                                            verbosity=None))])\n",
      "kfold  12  score: 93.87%\n",
      "kfold  12  score: 97.28%\n",
      "Average score over k-fold 93.36% (+/-0.73%) [Ensemble Majority]\n",
      "./Files_Majority_vote/128_features/filter13.csv\n",
      "n_estimators = 100 \n",
      "\n",
      "10-fold cross validation for file :  13\n",
      "GradientBoostingClassifier(max_depth=12, min_samples_split=12, n_estimators=5,\n",
      "                           random_state=3)\n",
      "kfold  13  score: 89.39%\n",
      "kfold  13  score: 90.11%\n",
      "GradientBoostingClassifier(max_depth=12, min_samples_split=12, n_estimators=5,\n",
      "                           random_state=3)\n",
      "kfold  13  score: 87.10%\n",
      "kfold  13  score: 89.00%\n",
      "GradientBoostingClassifier(max_depth=12, min_samples_split=12, n_estimators=5,\n",
      "                           random_state=3)\n",
      "kfold  13  score: 91.46%\n",
      "kfold  13  score: 93.87%\n",
      "GradientBoostingClassifier(max_depth=12, min_samples_split=12, n_estimators=5,\n",
      "                           random_state=3)\n",
      "kfold  13  score: 88.99%\n",
      "kfold  13  score: 91.23%\n",
      "GradientBoostingClassifier(max_depth=12, min_samples_split=12, n_estimators=5,\n",
      "                           random_state=3)\n",
      "kfold  13  score: 84.44%\n",
      "kfold  13  score: 88.58%\n",
      "GradientBoostingClassifier(max_depth=12, min_samples_split=12, n_estimators=5,\n",
      "                           random_state=3)\n",
      "kfold  13  score: 89.75%\n",
      "kfold  13  score: 92.05%\n",
      "GradientBoostingClassifier(max_depth=12, min_samples_split=12, n_estimators=5,\n",
      "                           random_state=3)\n",
      "kfold  13  score: 86.72%\n",
      "kfold  13  score: 89.44%\n",
      "GradientBoostingClassifier(max_depth=12, min_samples_split=12, n_estimators=5,\n",
      "                           random_state=3)\n",
      "kfold  13  score: 86.53%\n",
      "kfold  13  score: 90.43%\n",
      "GradientBoostingClassifier(max_depth=12, min_samples_split=12, n_estimators=5,\n",
      "                           random_state=3)\n",
      "kfold  13  score: 88.43%\n",
      "kfold  13  score: 91.06%\n",
      "GradientBoostingClassifier(max_depth=12, min_samples_split=12, n_estimators=5,\n",
      "                           random_state=3)\n",
      "kfold  13  score: 88.43%\n",
      "kfold  13  score: 89.40%\n",
      "Average score over k-fold 88.12% (+/-1.88%) [Gradient Boosting]\n",
      "RandomForestClassifier(max_depth=12, n_estimators=10, random_state=3)\n",
      "kfold  13  score: 92.80%\n",
      "kfold  13  score: 96.25%\n",
      "RandomForestClassifier(max_depth=12, n_estimators=10, random_state=3)\n",
      "kfold  13  score: 89.94%\n",
      "kfold  13  score: 96.80%\n",
      "RandomForestClassifier(max_depth=12, n_estimators=10, random_state=3)\n",
      "kfold  13  score: 93.17%\n",
      "kfold  13  score: 97.68%\n",
      "RandomForestClassifier(max_depth=12, n_estimators=10, random_state=3)\n",
      "kfold  13  score: 91.46%\n",
      "kfold  13  score: 96.84%\n",
      "RandomForestClassifier(max_depth=12, n_estimators=10, random_state=3)\n",
      "kfold  13  score: 90.13%\n",
      "kfold  13  score: 95.95%\n",
      "RandomForestClassifier(max_depth=12, n_estimators=10, random_state=3)\n",
      "kfold  13  score: 93.36%\n",
      "kfold  13  score: 96.96%\n",
      "RandomForestClassifier(max_depth=12, n_estimators=10, random_state=3)\n",
      "kfold  13  score: 91.27%\n",
      "kfold  13  score: 95.05%\n",
      "RandomForestClassifier(max_depth=12, n_estimators=10, random_state=3)\n",
      "kfold  13  score: 92.41%\n",
      "kfold  13  score: 96.46%\n",
      "RandomForestClassifier(max_depth=12, n_estimators=10, random_state=3)\n",
      "kfold  13  score: 93.36%\n",
      "kfold  13  score: 97.70%\n",
      "RandomForestClassifier(max_depth=12, n_estimators=10, random_state=3)\n",
      "kfold  13  score: 91.46%\n",
      "kfold  13  score: 96.02%\n",
      "Average score over k-fold 91.94% (+/-1.21%) [Random Forest]\n",
      "[21:48:33] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:48:33] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
      "              gamma=0, gpu_id=-1, importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.300000012,\n",
      "              max_delta_step=0, max_depth=12, max_leaf_nodes=None,\n",
      "              min_child_weight=1, min_samples_split=1, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=10, n_jobs=8,\n",
      "              num_parallel_tree=1, predictor='auto', random_state=3,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)\n",
      "kfold  13  score: 94.70%\n",
      "kfold  13  score: 99.07%\n",
      "[21:48:33] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:48:33] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
      "              gamma=0, gpu_id=-1, importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.300000012,\n",
      "              max_delta_step=0, max_depth=12, max_leaf_nodes=None,\n",
      "              min_child_weight=1, min_samples_split=1, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=10, n_jobs=8,\n",
      "              num_parallel_tree=1, predictor='auto', random_state=3,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)\n",
      "kfold  13  score: 93.74%\n",
      "kfold  13  score: 99.01%\n",
      "[21:48:33] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:48:33] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
      "              gamma=0, gpu_id=-1, importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.300000012,\n",
      "              max_delta_step=0, max_depth=12, max_leaf_nodes=None,\n",
      "              min_child_weight=1, min_samples_split=1, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=10, n_jobs=8,\n",
      "              num_parallel_tree=1, predictor='auto', random_state=3,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)\n",
      "kfold  13  score: 94.69%\n",
      "kfold  13  score: 99.22%\n",
      "[21:48:33] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:48:33] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
      "              gamma=0, gpu_id=-1, importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.300000012,\n",
      "              max_delta_step=0, max_depth=12, max_leaf_nodes=None,\n",
      "              min_child_weight=1, min_samples_split=1, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=10, n_jobs=8,\n",
      "              num_parallel_tree=1, predictor='auto', random_state=3,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)\n",
      "kfold  13  score: 95.45%\n",
      "kfold  13  score: 99.24%\n",
      "[21:48:33] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:48:33] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
      "              gamma=0, gpu_id=-1, importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.300000012,\n",
      "              max_delta_step=0, max_depth=12, max_leaf_nodes=None,\n",
      "              min_child_weight=1, min_samples_split=1, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=10, n_jobs=8,\n",
      "              num_parallel_tree=1, predictor='auto', random_state=3,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)\n",
      "kfold  13  score: 93.17%\n",
      "kfold  13  score: 98.50%\n",
      "[21:48:33] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:48:33] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
      "              gamma=0, gpu_id=-1, importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.300000012,\n",
      "              max_delta_step=0, max_depth=12, max_leaf_nodes=None,\n",
      "              min_child_weight=1, min_samples_split=1, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=10, n_jobs=8,\n",
      "              num_parallel_tree=1, predictor='auto', random_state=3,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)\n",
      "kfold  13  score: 93.74%\n",
      "kfold  13  score: 98.80%\n",
      "[21:48:33] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:48:33] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
      "              gamma=0, gpu_id=-1, importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.300000012,\n",
      "              max_delta_step=0, max_depth=12, max_leaf_nodes=None,\n",
      "              min_child_weight=1, min_samples_split=1, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=10, n_jobs=8,\n",
      "              num_parallel_tree=1, predictor='auto', random_state=3,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)\n",
      "kfold  13  score: 94.88%\n",
      "kfold  13  score: 99.05%\n",
      "[21:48:34] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:48:34] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
      "              gamma=0, gpu_id=-1, importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.300000012,\n",
      "              max_delta_step=0, max_depth=12, max_leaf_nodes=None,\n",
      "              min_child_weight=1, min_samples_split=1, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=10, n_jobs=8,\n",
      "              num_parallel_tree=1, predictor='auto', random_state=3,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)\n",
      "kfold  13  score: 95.64%\n",
      "kfold  13  score: 99.03%\n",
      "[21:48:34] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:48:34] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
      "              gamma=0, gpu_id=-1, importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.300000012,\n",
      "              max_delta_step=0, max_depth=12, max_leaf_nodes=None,\n",
      "              min_child_weight=1, min_samples_split=1, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=10, n_jobs=8,\n",
      "              num_parallel_tree=1, predictor='auto', random_state=3,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)\n",
      "kfold  13  score: 95.45%\n",
      "kfold  13  score: 98.69%\n",
      "[21:48:34] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:48:34] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
      "              gamma=0, gpu_id=-1, importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.300000012,\n",
      "              max_delta_step=0, max_depth=12, max_leaf_nodes=None,\n",
      "              min_child_weight=1, min_samples_split=1, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=10, n_jobs=8,\n",
      "              num_parallel_tree=1, predictor='auto', random_state=3,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)\n",
      "kfold  13  score: 94.88%\n",
      "kfold  13  score: 98.67%\n",
      "Average score over k-fold 94.63% (+/-0.79%) [XGBClassifier]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21:48:34] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:48:34] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "VotingClassifier(estimators=[('gb',\n",
      "                              GradientBoostingClassifier(max_depth=12,\n",
      "                                                         min_samples_split=12,\n",
      "                                                         n_estimators=5,\n",
      "                                                         random_state=3)),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(max_depth=12,\n",
      "                                                     n_estimators=10,\n",
      "                                                     random_state=3)),\n",
      "                             ('xgb',\n",
      "                              XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "                                            colsample_bylevel=1,\n",
      "                                            colsample_bynode=1,\n",
      "                                            colsample_bytree=1,\n",
      "                                            enable_categorical=False, gamma=0,\n",
      "                                            gpu...\n",
      "                                            interaction_constraints='',\n",
      "                                            learning_rate=0.300000012,\n",
      "                                            max_delta_step=0, max_depth=12,\n",
      "                                            max_leaf_nodes=None,\n",
      "                                            min_child_weight=1,\n",
      "                                            min_samples_split=1, missing=nan,\n",
      "                                            monotone_constraints='()',\n",
      "                                            n_estimators=10, n_jobs=8,\n",
      "                                            num_parallel_tree=1,\n",
      "                                            predictor='auto', random_state=3,\n",
      "                                            reg_alpha=0, reg_lambda=1,\n",
      "                                            scale_pos_weight=1, subsample=1,\n",
      "                                            tree_method='exact',\n",
      "                                            validate_parameters=1,\n",
      "                                            verbosity=None))])\n",
      "kfold  13  score: 93.94%\n",
      "kfold  13  score: 96.37%\n",
      "[21:48:35] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:48:35] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "VotingClassifier(estimators=[('gb',\n",
      "                              GradientBoostingClassifier(max_depth=12,\n",
      "                                                         min_samples_split=12,\n",
      "                                                         n_estimators=5,\n",
      "                                                         random_state=3)),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(max_depth=12,\n",
      "                                                     n_estimators=10,\n",
      "                                                     random_state=3)),\n",
      "                             ('xgb',\n",
      "                              XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "                                            colsample_bylevel=1,\n",
      "                                            colsample_bynode=1,\n",
      "                                            colsample_bytree=1,\n",
      "                                            enable_categorical=False, gamma=0,\n",
      "                                            gpu...\n",
      "                                            interaction_constraints='',\n",
      "                                            learning_rate=0.300000012,\n",
      "                                            max_delta_step=0, max_depth=12,\n",
      "                                            max_leaf_nodes=None,\n",
      "                                            min_child_weight=1,\n",
      "                                            min_samples_split=1, missing=nan,\n",
      "                                            monotone_constraints='()',\n",
      "                                            n_estimators=10, n_jobs=8,\n",
      "                                            num_parallel_tree=1,\n",
      "                                            predictor='auto', random_state=3,\n",
      "                                            reg_alpha=0, reg_lambda=1,\n",
      "                                            scale_pos_weight=1, subsample=1,\n",
      "                                            tree_method='exact',\n",
      "                                            validate_parameters=1,\n",
      "                                            verbosity=None))])\n",
      "kfold  13  score: 90.89%\n",
      "kfold  13  score: 96.56%\n",
      "[21:48:36] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:48:36] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "VotingClassifier(estimators=[('gb',\n",
      "                              GradientBoostingClassifier(max_depth=12,\n",
      "                                                         min_samples_split=12,\n",
      "                                                         n_estimators=5,\n",
      "                                                         random_state=3)),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(max_depth=12,\n",
      "                                                     n_estimators=10,\n",
      "                                                     random_state=3)),\n",
      "                             ('xgb',\n",
      "                              XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "                                            colsample_bylevel=1,\n",
      "                                            colsample_bynode=1,\n",
      "                                            colsample_bytree=1,\n",
      "                                            enable_categorical=False, gamma=0,\n",
      "                                            gpu...\n",
      "                                            interaction_constraints='',\n",
      "                                            learning_rate=0.300000012,\n",
      "                                            max_delta_step=0, max_depth=12,\n",
      "                                            max_leaf_nodes=None,\n",
      "                                            min_child_weight=1,\n",
      "                                            min_samples_split=1, missing=nan,\n",
      "                                            monotone_constraints='()',\n",
      "                                            n_estimators=10, n_jobs=8,\n",
      "                                            num_parallel_tree=1,\n",
      "                                            predictor='auto', random_state=3,\n",
      "                                            reg_alpha=0, reg_lambda=1,\n",
      "                                            scale_pos_weight=1, subsample=1,\n",
      "                                            tree_method='exact',\n",
      "                                            validate_parameters=1,\n",
      "                                            verbosity=None))])\n",
      "kfold  13  score: 93.74%\n",
      "kfold  13  score: 98.23%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21:48:36] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:48:36] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "VotingClassifier(estimators=[('gb',\n",
      "                              GradientBoostingClassifier(max_depth=12,\n",
      "                                                         min_samples_split=12,\n",
      "                                                         n_estimators=5,\n",
      "                                                         random_state=3)),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(max_depth=12,\n",
      "                                                     n_estimators=10,\n",
      "                                                     random_state=3)),\n",
      "                             ('xgb',\n",
      "                              XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "                                            colsample_bylevel=1,\n",
      "                                            colsample_bynode=1,\n",
      "                                            colsample_bytree=1,\n",
      "                                            enable_categorical=False, gamma=0,\n",
      "                                            gpu...\n",
      "                                            interaction_constraints='',\n",
      "                                            learning_rate=0.300000012,\n",
      "                                            max_delta_step=0, max_depth=12,\n",
      "                                            max_leaf_nodes=None,\n",
      "                                            min_child_weight=1,\n",
      "                                            min_samples_split=1, missing=nan,\n",
      "                                            monotone_constraints='()',\n",
      "                                            n_estimators=10, n_jobs=8,\n",
      "                                            num_parallel_tree=1,\n",
      "                                            predictor='auto', random_state=3,\n",
      "                                            reg_alpha=0, reg_lambda=1,\n",
      "                                            scale_pos_weight=1, subsample=1,\n",
      "                                            tree_method='exact',\n",
      "                                            validate_parameters=1,\n",
      "                                            verbosity=None))])\n",
      "kfold  13  score: 92.22%\n",
      "kfold  13  score: 97.07%\n",
      "[21:48:37] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:48:37] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "VotingClassifier(estimators=[('gb',\n",
      "                              GradientBoostingClassifier(max_depth=12,\n",
      "                                                         min_samples_split=12,\n",
      "                                                         n_estimators=5,\n",
      "                                                         random_state=3)),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(max_depth=12,\n",
      "                                                     n_estimators=10,\n",
      "                                                     random_state=3)),\n",
      "                             ('xgb',\n",
      "                              XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "                                            colsample_bylevel=1,\n",
      "                                            colsample_bynode=1,\n",
      "                                            colsample_bytree=1,\n",
      "                                            enable_categorical=False, gamma=0,\n",
      "                                            gpu...\n",
      "                                            interaction_constraints='',\n",
      "                                            learning_rate=0.300000012,\n",
      "                                            max_delta_step=0, max_depth=12,\n",
      "                                            max_leaf_nodes=None,\n",
      "                                            min_child_weight=1,\n",
      "                                            min_samples_split=1, missing=nan,\n",
      "                                            monotone_constraints='()',\n",
      "                                            n_estimators=10, n_jobs=8,\n",
      "                                            num_parallel_tree=1,\n",
      "                                            predictor='auto', random_state=3,\n",
      "                                            reg_alpha=0, reg_lambda=1,\n",
      "                                            scale_pos_weight=1, subsample=1,\n",
      "                                            tree_method='exact',\n",
      "                                            validate_parameters=1,\n",
      "                                            verbosity=None))])\n",
      "kfold  13  score: 89.37%\n",
      "kfold  13  score: 95.66%\n",
      "[21:48:37] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:48:37] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "VotingClassifier(estimators=[('gb',\n",
      "                              GradientBoostingClassifier(max_depth=12,\n",
      "                                                         min_samples_split=12,\n",
      "                                                         n_estimators=5,\n",
      "                                                         random_state=3)),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(max_depth=12,\n",
      "                                                     n_estimators=10,\n",
      "                                                     random_state=3)),\n",
      "                             ('xgb',\n",
      "                              XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "                                            colsample_bylevel=1,\n",
      "                                            colsample_bynode=1,\n",
      "                                            colsample_bytree=1,\n",
      "                                            enable_categorical=False, gamma=0,\n",
      "                                            gpu...\n",
      "                                            interaction_constraints='',\n",
      "                                            learning_rate=0.300000012,\n",
      "                                            max_delta_step=0, max_depth=12,\n",
      "                                            max_leaf_nodes=None,\n",
      "                                            min_child_weight=1,\n",
      "                                            min_samples_split=1, missing=nan,\n",
      "                                            monotone_constraints='()',\n",
      "                                            n_estimators=10, n_jobs=8,\n",
      "                                            num_parallel_tree=1,\n",
      "                                            predictor='auto', random_state=3,\n",
      "                                            reg_alpha=0, reg_lambda=1,\n",
      "                                            scale_pos_weight=1, subsample=1,\n",
      "                                            tree_method='exact',\n",
      "                                            validate_parameters=1,\n",
      "                                            verbosity=None))])\n",
      "kfold  13  score: 93.17%\n",
      "kfold  13  score: 97.22%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21:48:38] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:48:38] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "VotingClassifier(estimators=[('gb',\n",
      "                              GradientBoostingClassifier(max_depth=12,\n",
      "                                                         min_samples_split=12,\n",
      "                                                         n_estimators=5,\n",
      "                                                         random_state=3)),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(max_depth=12,\n",
      "                                                     n_estimators=10,\n",
      "                                                     random_state=3)),\n",
      "                             ('xgb',\n",
      "                              XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "                                            colsample_bylevel=1,\n",
      "                                            colsample_bynode=1,\n",
      "                                            colsample_bytree=1,\n",
      "                                            enable_categorical=False, gamma=0,\n",
      "                                            gpu...\n",
      "                                            interaction_constraints='',\n",
      "                                            learning_rate=0.300000012,\n",
      "                                            max_delta_step=0, max_depth=12,\n",
      "                                            max_leaf_nodes=None,\n",
      "                                            min_child_weight=1,\n",
      "                                            min_samples_split=1, missing=nan,\n",
      "                                            monotone_constraints='()',\n",
      "                                            n_estimators=10, n_jobs=8,\n",
      "                                            num_parallel_tree=1,\n",
      "                                            predictor='auto', random_state=3,\n",
      "                                            reg_alpha=0, reg_lambda=1,\n",
      "                                            scale_pos_weight=1, subsample=1,\n",
      "                                            tree_method='exact',\n",
      "                                            validate_parameters=1,\n",
      "                                            verbosity=None))])\n",
      "kfold  13  score: 91.65%\n",
      "kfold  13  score: 95.24%\n",
      "[21:48:38] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:48:38] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "VotingClassifier(estimators=[('gb',\n",
      "                              GradientBoostingClassifier(max_depth=12,\n",
      "                                                         min_samples_split=12,\n",
      "                                                         n_estimators=5,\n",
      "                                                         random_state=3)),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(max_depth=12,\n",
      "                                                     n_estimators=10,\n",
      "                                                     random_state=3)),\n",
      "                             ('xgb',\n",
      "                              XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "                                            colsample_bylevel=1,\n",
      "                                            colsample_bynode=1,\n",
      "                                            colsample_bytree=1,\n",
      "                                            enable_categorical=False, gamma=0,\n",
      "                                            gpu...\n",
      "                                            interaction_constraints='',\n",
      "                                            learning_rate=0.300000012,\n",
      "                                            max_delta_step=0, max_depth=12,\n",
      "                                            max_leaf_nodes=None,\n",
      "                                            min_child_weight=1,\n",
      "                                            min_samples_split=1, missing=nan,\n",
      "                                            monotone_constraints='()',\n",
      "                                            n_estimators=10, n_jobs=8,\n",
      "                                            num_parallel_tree=1,\n",
      "                                            predictor='auto', random_state=3,\n",
      "                                            reg_alpha=0, reg_lambda=1,\n",
      "                                            scale_pos_weight=1, subsample=1,\n",
      "                                            tree_method='exact',\n",
      "                                            validate_parameters=1,\n",
      "                                            verbosity=None))])\n",
      "kfold  13  score: 92.79%\n",
      "kfold  13  score: 96.65%\n",
      "[21:48:39] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:48:39] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "VotingClassifier(estimators=[('gb',\n",
      "                              GradientBoostingClassifier(max_depth=12,\n",
      "                                                         min_samples_split=12,\n",
      "                                                         n_estimators=5,\n",
      "                                                         random_state=3)),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(max_depth=12,\n",
      "                                                     n_estimators=10,\n",
      "                                                     random_state=3)),\n",
      "                             ('xgb',\n",
      "                              XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "                                            colsample_bylevel=1,\n",
      "                                            colsample_bynode=1,\n",
      "                                            colsample_bytree=1,\n",
      "                                            enable_categorical=False, gamma=0,\n",
      "                                            gpu...\n",
      "                                            interaction_constraints='',\n",
      "                                            learning_rate=0.300000012,\n",
      "                                            max_delta_step=0, max_depth=12,\n",
      "                                            max_leaf_nodes=None,\n",
      "                                            min_child_weight=1,\n",
      "                                            min_samples_split=1, missing=nan,\n",
      "                                            monotone_constraints='()',\n",
      "                                            n_estimators=10, n_jobs=8,\n",
      "                                            num_parallel_tree=1,\n",
      "                                            predictor='auto', random_state=3,\n",
      "                                            reg_alpha=0, reg_lambda=1,\n",
      "                                            scale_pos_weight=1, subsample=1,\n",
      "                                            tree_method='exact',\n",
      "                                            validate_parameters=1,\n",
      "                                            verbosity=None))])\n",
      "kfold  13  score: 93.36%\n",
      "kfold  13  score: 97.30%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21:48:39] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:48:39] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "VotingClassifier(estimators=[('gb',\n",
      "                              GradientBoostingClassifier(max_depth=12,\n",
      "                                                         min_samples_split=12,\n",
      "                                                         n_estimators=5,\n",
      "                                                         random_state=3)),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(max_depth=12,\n",
      "                                                     n_estimators=10,\n",
      "                                                     random_state=3)),\n",
      "                             ('xgb',\n",
      "                              XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "                                            colsample_bylevel=1,\n",
      "                                            colsample_bynode=1,\n",
      "                                            colsample_bytree=1,\n",
      "                                            enable_categorical=False, gamma=0,\n",
      "                                            gpu...\n",
      "                                            interaction_constraints='',\n",
      "                                            learning_rate=0.300000012,\n",
      "                                            max_delta_step=0, max_depth=12,\n",
      "                                            max_leaf_nodes=None,\n",
      "                                            min_child_weight=1,\n",
      "                                            min_samples_split=1, missing=nan,\n",
      "                                            monotone_constraints='()',\n",
      "                                            n_estimators=10, n_jobs=8,\n",
      "                                            num_parallel_tree=1,\n",
      "                                            predictor='auto', random_state=3,\n",
      "                                            reg_alpha=0, reg_lambda=1,\n",
      "                                            scale_pos_weight=1, subsample=1,\n",
      "                                            tree_method='exact',\n",
      "                                            validate_parameters=1,\n",
      "                                            verbosity=None))])\n",
      "kfold  13  score: 91.84%\n",
      "kfold  13  score: 95.87%\n",
      "Average score over k-fold 92.30% (+/-1.34%) [Ensemble Majority]\n",
      "./Files_Majority_vote/128_features/filter14.csv\n",
      "n_estimators = 100 \n",
      "\n",
      "10-fold cross validation for file :  14\n",
      "GradientBoostingClassifier(max_depth=12, min_samples_split=12, n_estimators=5,\n",
      "                           random_state=3)\n",
      "kfold  14  score: 87.11%\n",
      "kfold  14  score: 91.74%\n",
      "GradientBoostingClassifier(max_depth=12, min_samples_split=12, n_estimators=5,\n",
      "                           random_state=3)\n",
      "kfold  14  score: 87.11%\n",
      "kfold  14  score: 90.68%\n",
      "GradientBoostingClassifier(max_depth=12, min_samples_split=12, n_estimators=5,\n",
      "                           random_state=3)\n",
      "kfold  14  score: 86.72%\n",
      "kfold  14  score: 90.75%\n",
      "GradientBoostingClassifier(max_depth=12, min_samples_split=12, n_estimators=5,\n",
      "                           random_state=3)\n",
      "kfold  14  score: 84.77%\n",
      "kfold  14  score: 89.92%\n",
      "GradientBoostingClassifier(max_depth=12, min_samples_split=12, n_estimators=5,\n",
      "                           random_state=3)\n",
      "kfold  14  score: 86.72%\n",
      "kfold  14  score: 89.98%\n",
      "GradientBoostingClassifier(max_depth=12, min_samples_split=12, n_estimators=5,\n",
      "                           random_state=3)\n",
      "kfold  14  score: 86.50%\n",
      "kfold  14  score: 90.62%\n",
      "GradientBoostingClassifier(max_depth=12, min_samples_split=12, n_estimators=5,\n",
      "                           random_state=3)\n",
      "kfold  14  score: 86.89%\n",
      "kfold  14  score: 89.73%\n",
      "GradientBoostingClassifier(max_depth=12, min_samples_split=12, n_estimators=5,\n",
      "                           random_state=3)\n",
      "kfold  14  score: 87.67%\n",
      "kfold  14  score: 90.31%\n",
      "GradientBoostingClassifier(max_depth=12, min_samples_split=12, n_estimators=5,\n",
      "                           random_state=3)\n",
      "kfold  14  score: 88.65%\n",
      "kfold  14  score: 92.14%\n",
      "GradientBoostingClassifier(max_depth=12, min_samples_split=12, n_estimators=5,\n",
      "                           random_state=3)\n",
      "kfold  14  score: 88.45%\n",
      "kfold  14  score: 90.53%\n",
      "Average score over k-fold 87.06% (+/-1.03%) [Gradient Boosting]\n",
      "RandomForestClassifier(max_depth=12, n_estimators=10, random_state=3)\n",
      "kfold  14  score: 90.62%\n",
      "kfold  14  score: 95.85%\n",
      "RandomForestClassifier(max_depth=12, n_estimators=10, random_state=3)\n",
      "kfold  14  score: 91.99%\n",
      "kfold  14  score: 96.65%\n",
      "RandomForestClassifier(max_depth=12, n_estimators=10, random_state=3)\n",
      "kfold  14  score: 91.41%\n",
      "kfold  14  score: 95.61%\n",
      "RandomForestClassifier(max_depth=12, n_estimators=10, random_state=3)\n",
      "kfold  14  score: 91.80%\n",
      "kfold  14  score: 95.18%\n",
      "RandomForestClassifier(max_depth=12, n_estimators=10, random_state=3)\n",
      "kfold  14  score: 89.65%\n",
      "kfold  14  score: 94.46%\n",
      "RandomForestClassifier(max_depth=12, n_estimators=10, random_state=3)\n",
      "kfold  14  score: 89.24%\n",
      "kfold  14  score: 94.40%\n",
      "RandomForestClassifier(max_depth=12, n_estimators=10, random_state=3)\n",
      "kfold  14  score: 91.98%\n",
      "kfold  14  score: 95.09%\n",
      "RandomForestClassifier(max_depth=12, n_estimators=10, random_state=3)\n",
      "kfold  14  score: 92.37%\n",
      "kfold  14  score: 95.37%\n",
      "RandomForestClassifier(max_depth=12, n_estimators=10, random_state=3)\n",
      "kfold  14  score: 91.98%\n",
      "kfold  14  score: 95.79%\n",
      "RandomForestClassifier(max_depth=12, n_estimators=10, random_state=3)\n",
      "kfold  14  score: 89.82%\n",
      "kfold  14  score: 94.40%\n",
      "Average score over k-fold 91.09% (+/-1.09%) [Random Forest]\n",
      "[21:48:43] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:48:43] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
      "              gamma=0, gpu_id=-1, importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.300000012,\n",
      "              max_delta_step=0, max_depth=12, max_leaf_nodes=None,\n",
      "              min_child_weight=1, min_samples_split=1, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=10, n_jobs=8,\n",
      "              num_parallel_tree=1, predictor='auto', random_state=3,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)\n",
      "kfold  14  score: 93.36%\n",
      "kfold  14  score: 98.94%\n",
      "[21:48:43] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:48:43] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
      "              gamma=0, gpu_id=-1, importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.300000012,\n",
      "              max_delta_step=0, max_depth=12, max_leaf_nodes=None,\n",
      "              min_child_weight=1, min_samples_split=1, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=10, n_jobs=8,\n",
      "              num_parallel_tree=1, predictor='auto', random_state=3,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)\n",
      "kfold  14  score: 93.55%\n",
      "kfold  14  score: 99.13%\n",
      "[21:48:43] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:48:43] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
      "              gamma=0, gpu_id=-1, importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.300000012,\n",
      "              max_delta_step=0, max_depth=12, max_leaf_nodes=None,\n",
      "              min_child_weight=1, min_samples_split=1, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=10, n_jobs=8,\n",
      "              num_parallel_tree=1, predictor='auto', random_state=3,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)\n",
      "kfold  14  score: 94.14%\n",
      "kfold  14  score: 99.26%\n",
      "[21:48:44] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:48:44] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
      "              gamma=0, gpu_id=-1, importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.300000012,\n",
      "              max_delta_step=0, max_depth=12, max_leaf_nodes=None,\n",
      "              min_child_weight=1, min_samples_split=1, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=10, n_jobs=8,\n",
      "              num_parallel_tree=1, predictor='auto', random_state=3,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              tree_method='exact', validate_parameters=1, verbosity=None)\n",
      "kfold  14  score: 93.55%\n",
      "kfold  14  score: 98.59%\n",
      "[21:48:44] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:48:44] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
      "              gamma=0, gpu_id=-1, importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.300000012,\n",
      "              max_delta_step=0, max_depth=12, max_leaf_nodes=None,\n",
      "              min_child_weight=1, min_samples_split=1, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=10, n_jobs=8,\n",
      "              num_parallel_tree=1, predictor='auto', random_state=3,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)\n",
      "kfold  14  score: 92.77%\n",
      "kfold  14  score: 98.46%\n",
      "[21:48:44] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:48:44] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
      "              gamma=0, gpu_id=-1, importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.300000012,\n",
      "              max_delta_step=0, max_depth=12, max_leaf_nodes=None,\n",
      "              min_child_weight=1, min_samples_split=1, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=10, n_jobs=8,\n",
      "              num_parallel_tree=1, predictor='auto', random_state=3,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)\n",
      "kfold  14  score: 93.15%\n",
      "kfold  14  score: 98.94%\n",
      "[21:48:44] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:48:44] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
      "              gamma=0, gpu_id=-1, importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.300000012,\n",
      "              max_delta_step=0, max_depth=12, max_leaf_nodes=None,\n",
      "              min_child_weight=1, min_samples_split=1, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=10, n_jobs=8,\n",
      "              num_parallel_tree=1, predictor='auto', random_state=3,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)\n",
      "kfold  14  score: 93.54%\n",
      "kfold  14  score: 98.52%\n",
      "[21:48:44] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:48:44] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
      "              gamma=0, gpu_id=-1, importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.300000012,\n",
      "              max_delta_step=0, max_depth=12, max_leaf_nodes=None,\n",
      "              min_child_weight=1, min_samples_split=1, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=10, n_jobs=8,\n",
      "              num_parallel_tree=1, predictor='auto', random_state=3,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)\n",
      "kfold  14  score: 94.32%\n",
      "kfold  14  score: 98.94%\n",
      "[21:48:44] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:48:44] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
      "              gamma=0, gpu_id=-1, importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.300000012,\n",
      "              max_delta_step=0, max_depth=12, max_leaf_nodes=None,\n",
      "              min_child_weight=1, min_samples_split=1, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=10, n_jobs=8,\n",
      "              num_parallel_tree=1, predictor='auto', random_state=3,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)\n",
      "kfold  14  score: 93.54%\n",
      "kfold  14  score: 99.20%\n",
      "[21:48:44] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:48:44] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
      "              gamma=0, gpu_id=-1, importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.300000012,\n",
      "              max_delta_step=0, max_depth=12, max_leaf_nodes=None,\n",
      "              min_child_weight=1, min_samples_split=1, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=10, n_jobs=8,\n",
      "              num_parallel_tree=1, predictor='auto', random_state=3,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)\n",
      "kfold  14  score: 93.15%\n",
      "kfold  14  score: 98.76%\n",
      "Average score over k-fold 93.51% (+/-0.43%) [XGBClassifier]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21:48:45] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:48:45] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "VotingClassifier(estimators=[('gb',\n",
      "                              GradientBoostingClassifier(max_depth=12,\n",
      "                                                         min_samples_split=12,\n",
      "                                                         n_estimators=5,\n",
      "                                                         random_state=3)),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(max_depth=12,\n",
      "                                                     n_estimators=10,\n",
      "                                                     random_state=3)),\n",
      "                             ('xgb',\n",
      "                              XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "                                            colsample_bylevel=1,\n",
      "                                            colsample_bynode=1,\n",
      "                                            colsample_bytree=1,\n",
      "                                            enable_categorical=False, gamma=0,\n",
      "                                            gpu...\n",
      "                                            interaction_constraints='',\n",
      "                                            learning_rate=0.300000012,\n",
      "                                            max_delta_step=0, max_depth=12,\n",
      "                                            max_leaf_nodes=None,\n",
      "                                            min_child_weight=1,\n",
      "                                            min_samples_split=1, missing=nan,\n",
      "                                            monotone_constraints='()',\n",
      "                                            n_estimators=10, n_jobs=8,\n",
      "                                            num_parallel_tree=1,\n",
      "                                            predictor='auto', random_state=3,\n",
      "                                            reg_alpha=0, reg_lambda=1,\n",
      "                                            scale_pos_weight=1, subsample=1,\n",
      "                                            tree_method='exact',\n",
      "                                            validate_parameters=1,\n",
      "                                            verbosity=None))])\n",
      "kfold  14  score: 91.02%\n",
      "kfold  14  score: 96.13%\n",
      "[21:48:45] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:48:45] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "VotingClassifier(estimators=[('gb',\n",
      "                              GradientBoostingClassifier(max_depth=12,\n",
      "                                                         min_samples_split=12,\n",
      "                                                         n_estimators=5,\n",
      "                                                         random_state=3)),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(max_depth=12,\n",
      "                                                     n_estimators=10,\n",
      "                                                     random_state=3)),\n",
      "                             ('xgb',\n",
      "                              XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "                                            colsample_bylevel=1,\n",
      "                                            colsample_bynode=1,\n",
      "                                            colsample_bytree=1,\n",
      "                                            enable_categorical=False, gamma=0,\n",
      "                                            gpu...\n",
      "                                            interaction_constraints='',\n",
      "                                            learning_rate=0.300000012,\n",
      "                                            max_delta_step=0, max_depth=12,\n",
      "                                            max_leaf_nodes=None,\n",
      "                                            min_child_weight=1,\n",
      "                                            min_samples_split=1, missing=nan,\n",
      "                                            monotone_constraints='()',\n",
      "                                            n_estimators=10, n_jobs=8,\n",
      "                                            num_parallel_tree=1,\n",
      "                                            predictor='auto', random_state=3,\n",
      "                                            reg_alpha=0, reg_lambda=1,\n",
      "                                            scale_pos_weight=1, subsample=1,\n",
      "                                            tree_method='exact',\n",
      "                                            validate_parameters=1,\n",
      "                                            verbosity=None))])\n",
      "kfold  14  score: 91.80%\n",
      "kfold  14  score: 96.76%\n",
      "[21:48:46] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:48:46] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "VotingClassifier(estimators=[('gb',\n",
      "                              GradientBoostingClassifier(max_depth=12,\n",
      "                                                         min_samples_split=12,\n",
      "                                                         n_estimators=5,\n",
      "                                                         random_state=3)),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(max_depth=12,\n",
      "                                                     n_estimators=10,\n",
      "                                                     random_state=3)),\n",
      "                             ('xgb',\n",
      "                              XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "                                            colsample_bylevel=1,\n",
      "                                            colsample_bynode=1,\n",
      "                                            colsample_bytree=1,\n",
      "                                            enable_categorical=False, gamma=0,\n",
      "                                            gpu...\n",
      "                                            interaction_constraints='',\n",
      "                                            learning_rate=0.300000012,\n",
      "                                            max_delta_step=0, max_depth=12,\n",
      "                                            max_leaf_nodes=None,\n",
      "                                            min_child_weight=1,\n",
      "                                            min_samples_split=1, missing=nan,\n",
      "                                            monotone_constraints='()',\n",
      "                                            n_estimators=10, n_jobs=8,\n",
      "                                            num_parallel_tree=1,\n",
      "                                            predictor='auto', random_state=3,\n",
      "                                            reg_alpha=0, reg_lambda=1,\n",
      "                                            scale_pos_weight=1, subsample=1,\n",
      "                                            tree_method='exact',\n",
      "                                            validate_parameters=1,\n",
      "                                            verbosity=None))])\n",
      "kfold  14  score: 91.02%\n",
      "kfold  14  score: 95.96%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21:48:46] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:48:46] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "VotingClassifier(estimators=[('gb',\n",
      "                              GradientBoostingClassifier(max_depth=12,\n",
      "                                                         min_samples_split=12,\n",
      "                                                         n_estimators=5,\n",
      "                                                         random_state=3)),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(max_depth=12,\n",
      "                                                     n_estimators=10,\n",
      "                                                     random_state=3)),\n",
      "                             ('xgb',\n",
      "                              XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "                                            colsample_bylevel=1,\n",
      "                                            colsample_bynode=1,\n",
      "                                            colsample_bytree=1,\n",
      "                                            enable_categorical=False, gamma=0,\n",
      "                                            gpu...\n",
      "                                            interaction_constraints='',\n",
      "                                            learning_rate=0.300000012,\n",
      "                                            max_delta_step=0, max_depth=12,\n",
      "                                            max_leaf_nodes=None,\n",
      "                                            min_child_weight=1,\n",
      "                                            min_samples_split=1, missing=nan,\n",
      "                                            monotone_constraints='()',\n",
      "                                            n_estimators=10, n_jobs=8,\n",
      "                                            num_parallel_tree=1,\n",
      "                                            predictor='auto', random_state=3,\n",
      "                                            reg_alpha=0, reg_lambda=1,\n",
      "                                            scale_pos_weight=1, subsample=1,\n",
      "                                            tree_method='exact',\n",
      "                                            validate_parameters=1,\n",
      "                                            verbosity=None))])\n",
      "kfold  14  score: 91.80%\n",
      "kfold  14  score: 96.11%\n",
      "[21:48:47] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:48:47] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "VotingClassifier(estimators=[('gb',\n",
      "                              GradientBoostingClassifier(max_depth=12,\n",
      "                                                         min_samples_split=12,\n",
      "                                                         n_estimators=5,\n",
      "                                                         random_state=3)),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(max_depth=12,\n",
      "                                                     n_estimators=10,\n",
      "                                                     random_state=3)),\n",
      "                             ('xgb',\n",
      "                              XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "                                            colsample_bylevel=1,\n",
      "                                            colsample_bynode=1,\n",
      "                                            colsample_bytree=1,\n",
      "                                            enable_categorical=False, gamma=0,\n",
      "                                            gpu...\n",
      "                                            interaction_constraints='',\n",
      "                                            learning_rate=0.300000012,\n",
      "                                            max_delta_step=0, max_depth=12,\n",
      "                                            max_leaf_nodes=None,\n",
      "                                            min_child_weight=1,\n",
      "                                            min_samples_split=1, missing=nan,\n",
      "                                            monotone_constraints='()',\n",
      "                                            n_estimators=10, n_jobs=8,\n",
      "                                            num_parallel_tree=1,\n",
      "                                            predictor='auto', random_state=3,\n",
      "                                            reg_alpha=0, reg_lambda=1,\n",
      "                                            scale_pos_weight=1, subsample=1,\n",
      "                                            tree_method='exact',\n",
      "                                            validate_parameters=1,\n",
      "                                            verbosity=None))])\n",
      "kfold  14  score: 90.62%\n",
      "kfold  14  score: 95.50%\n",
      "[21:48:47] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:48:47] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "VotingClassifier(estimators=[('gb',\n",
      "                              GradientBoostingClassifier(max_depth=12,\n",
      "                                                         min_samples_split=12,\n",
      "                                                         n_estimators=5,\n",
      "                                                         random_state=3)),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(max_depth=12,\n",
      "                                                     n_estimators=10,\n",
      "                                                     random_state=3)),\n",
      "                             ('xgb',\n",
      "                              XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "                                            colsample_bylevel=1,\n",
      "                                            colsample_bynode=1,\n",
      "                                            colsample_bytree=1,\n",
      "                                            enable_categorical=False, gamma=0,\n",
      "                                            gpu...\n",
      "                                            interaction_constraints='',\n",
      "                                            learning_rate=0.300000012,\n",
      "                                            max_delta_step=0, max_depth=12,\n",
      "                                            max_leaf_nodes=None,\n",
      "                                            min_child_weight=1,\n",
      "                                            min_samples_split=1, missing=nan,\n",
      "                                            monotone_constraints='()',\n",
      "                                            n_estimators=10, n_jobs=8,\n",
      "                                            num_parallel_tree=1,\n",
      "                                            predictor='auto', random_state=3,\n",
      "                                            reg_alpha=0, reg_lambda=1,\n",
      "                                            scale_pos_weight=1, subsample=1,\n",
      "                                            tree_method='exact',\n",
      "                                            validate_parameters=1,\n",
      "                                            verbosity=None))])\n",
      "kfold  14  score: 90.22%\n",
      "kfold  14  score: 95.22%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21:48:48] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:48:48] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "VotingClassifier(estimators=[('gb',\n",
      "                              GradientBoostingClassifier(max_depth=12,\n",
      "                                                         min_samples_split=12,\n",
      "                                                         n_estimators=5,\n",
      "                                                         random_state=3)),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(max_depth=12,\n",
      "                                                     n_estimators=10,\n",
      "                                                     random_state=3)),\n",
      "                             ('xgb',\n",
      "                              XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "                                            colsample_bylevel=1,\n",
      "                                            colsample_bynode=1,\n",
      "                                            colsample_bytree=1,\n",
      "                                            enable_categorical=False, gamma=0,\n",
      "                                            gpu...\n",
      "                                            interaction_constraints='',\n",
      "                                            learning_rate=0.300000012,\n",
      "                                            max_delta_step=0, max_depth=12,\n",
      "                                            max_leaf_nodes=None,\n",
      "                                            min_child_weight=1,\n",
      "                                            min_samples_split=1, missing=nan,\n",
      "                                            monotone_constraints='()',\n",
      "                                            n_estimators=10, n_jobs=8,\n",
      "                                            num_parallel_tree=1,\n",
      "                                            predictor='auto', random_state=3,\n",
      "                                            reg_alpha=0, reg_lambda=1,\n",
      "                                            scale_pos_weight=1, subsample=1,\n",
      "                                            tree_method='exact',\n",
      "                                            validate_parameters=1,\n",
      "                                            verbosity=None))])\n",
      "kfold  14  score: 91.98%\n",
      "kfold  14  score: 95.68%\n",
      "[21:48:48] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:48:48] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "VotingClassifier(estimators=[('gb',\n",
      "                              GradientBoostingClassifier(max_depth=12,\n",
      "                                                         min_samples_split=12,\n",
      "                                                         n_estimators=5,\n",
      "                                                         random_state=3)),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(max_depth=12,\n",
      "                                                     n_estimators=10,\n",
      "                                                     random_state=3)),\n",
      "                             ('xgb',\n",
      "                              XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "                                            colsample_bylevel=1,\n",
      "                                            colsample_bynode=1,\n",
      "                                            colsample_bytree=1,\n",
      "                                            enable_categorical=False, gamma=0,\n",
      "                                            gpu...\n",
      "                                            interaction_constraints='',\n",
      "                                            learning_rate=0.300000012,\n",
      "                                            max_delta_step=0, max_depth=12,\n",
      "                                            max_leaf_nodes=None,\n",
      "                                            min_child_weight=1,\n",
      "                                            min_samples_split=1, missing=nan,\n",
      "                                            monotone_constraints='()',\n",
      "                                            n_estimators=10, n_jobs=8,\n",
      "                                            num_parallel_tree=1,\n",
      "                                            predictor='auto', random_state=3,\n",
      "                                            reg_alpha=0, reg_lambda=1,\n",
      "                                            scale_pos_weight=1, subsample=1,\n",
      "                                            tree_method='exact',\n",
      "                                            validate_parameters=1,\n",
      "                                            verbosity=None))])\n",
      "kfold  14  score: 92.17%\n",
      "kfold  14  score: 95.89%\n",
      "[21:48:49] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:48:49] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "VotingClassifier(estimators=[('gb',\n",
      "                              GradientBoostingClassifier(max_depth=12,\n",
      "                                                         min_samples_split=12,\n",
      "                                                         n_estimators=5,\n",
      "                                                         random_state=3)),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(max_depth=12,\n",
      "                                                     n_estimators=10,\n",
      "                                                     random_state=3)),\n",
      "                             ('xgb',\n",
      "                              XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "                                            colsample_bylevel=1,\n",
      "                                            colsample_bynode=1,\n",
      "                                            colsample_bytree=1,\n",
      "                                            enable_categorical=False, gamma=0,\n",
      "                                            gpu...\n",
      "                                            interaction_constraints='',\n",
      "                                            learning_rate=0.300000012,\n",
      "                                            max_delta_step=0, max_depth=12,\n",
      "                                            max_leaf_nodes=None,\n",
      "                                            min_child_weight=1,\n",
      "                                            min_samples_split=1, missing=nan,\n",
      "                                            monotone_constraints='()',\n",
      "                                            n_estimators=10, n_jobs=8,\n",
      "                                            num_parallel_tree=1,\n",
      "                                            predictor='auto', random_state=3,\n",
      "                                            reg_alpha=0, reg_lambda=1,\n",
      "                                            scale_pos_weight=1, subsample=1,\n",
      "                                            tree_method='exact',\n",
      "                                            validate_parameters=1,\n",
      "                                            verbosity=None))])\n",
      "kfold  14  score: 91.59%\n",
      "kfold  14  score: 96.33%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21:48:49] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:48:49] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "VotingClassifier(estimators=[('gb',\n",
      "                              GradientBoostingClassifier(max_depth=12,\n",
      "                                                         min_samples_split=12,\n",
      "                                                         n_estimators=5,\n",
      "                                                         random_state=3)),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(max_depth=12,\n",
      "                                                     n_estimators=10,\n",
      "                                                     random_state=3)),\n",
      "                             ('xgb',\n",
      "                              XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "                                            colsample_bylevel=1,\n",
      "                                            colsample_bynode=1,\n",
      "                                            colsample_bytree=1,\n",
      "                                            enable_categorical=False, gamma=0,\n",
      "                                            gpu...\n",
      "                                            interaction_constraints='',\n",
      "                                            learning_rate=0.300000012,\n",
      "                                            max_delta_step=0, max_depth=12,\n",
      "                                            max_leaf_nodes=None,\n",
      "                                            min_child_weight=1,\n",
      "                                            min_samples_split=1, missing=nan,\n",
      "                                            monotone_constraints='()',\n",
      "                                            n_estimators=10, n_jobs=8,\n",
      "                                            num_parallel_tree=1,\n",
      "                                            predictor='auto', random_state=3,\n",
      "                                            reg_alpha=0, reg_lambda=1,\n",
      "                                            scale_pos_weight=1, subsample=1,\n",
      "                                            tree_method='exact',\n",
      "                                            validate_parameters=1,\n",
      "                                            verbosity=None))])\n",
      "kfold  14  score: 90.41%\n",
      "kfold  14  score: 94.81%\n",
      "Average score over k-fold 91.26% (+/-0.66%) [Ensemble Majority]\n",
      "./Files_Majority_vote/128_features/filter15.csv\n",
      "n_estimators = 100 \n",
      "\n",
      "10-fold cross validation for file :  15\n",
      "GradientBoostingClassifier(max_depth=12, min_samples_split=12, n_estimators=5,\n",
      "                           random_state=3)\n",
      "kfold  15  score: 89.02%\n",
      "kfold  15  score: 93.60%\n",
      "GradientBoostingClassifier(max_depth=12, min_samples_split=12, n_estimators=5,\n",
      "                           random_state=3)\n",
      "kfold  15  score: 90.15%\n",
      "kfold  15  score: 92.69%\n",
      "GradientBoostingClassifier(max_depth=12, min_samples_split=12, n_estimators=5,\n",
      "                           random_state=3)\n",
      "kfold  15  score: 86.93%\n",
      "kfold  15  score: 93.60%\n",
      "GradientBoostingClassifier(max_depth=12, min_samples_split=12, n_estimators=5,\n",
      "                           random_state=3)\n",
      "kfold  15  score: 92.99%\n",
      "kfold  15  score: 96.38%\n",
      "GradientBoostingClassifier(max_depth=12, min_samples_split=12, n_estimators=5,\n",
      "                           random_state=3)\n",
      "kfold  15  score: 91.10%\n",
      "kfold  15  score: 95.37%\n",
      "GradientBoostingClassifier(max_depth=12, min_samples_split=12, n_estimators=5,\n",
      "                           random_state=3)\n",
      "kfold  15  score: 87.88%\n",
      "kfold  15  score: 92.94%\n",
      "GradientBoostingClassifier(max_depth=12, min_samples_split=12, n_estimators=5,\n",
      "                           random_state=3)\n",
      "kfold  15  score: 91.46%\n",
      "kfold  15  score: 95.75%\n",
      "GradientBoostingClassifier(max_depth=12, min_samples_split=12, n_estimators=5,\n",
      "                           random_state=3)\n",
      "kfold  15  score: 89.56%\n",
      "kfold  15  score: 95.11%\n",
      "GradientBoostingClassifier(max_depth=12, min_samples_split=12, n_estimators=5,\n",
      "                           random_state=3)\n",
      "kfold  15  score: 86.34%\n",
      "kfold  15  score: 92.52%\n",
      "GradientBoostingClassifier(max_depth=12, min_samples_split=12, n_estimators=5,\n",
      "                           random_state=3)\n",
      "kfold  15  score: 91.27%\n",
      "kfold  15  score: 95.94%\n",
      "Average score over k-fold 89.67% (+/-2.03%) [Gradient Boosting]\n",
      "RandomForestClassifier(max_depth=12, n_estimators=10, random_state=3)\n",
      "kfold  15  score: 95.27%\n",
      "kfold  15  score: 99.07%\n",
      "RandomForestClassifier(max_depth=12, n_estimators=10, random_state=3)\n",
      "kfold  15  score: 95.83%\n",
      "kfold  15  score: 99.18%\n",
      "RandomForestClassifier(max_depth=12, n_estimators=10, random_state=3)\n",
      "kfold  15  score: 92.42%\n",
      "kfold  15  score: 98.78%\n",
      "RandomForestClassifier(max_depth=12, n_estimators=10, random_state=3)\n",
      "kfold  15  score: 94.51%\n",
      "kfold  15  score: 98.65%\n",
      "RandomForestClassifier(max_depth=12, n_estimators=10, random_state=3)\n",
      "kfold  15  score: 93.56%\n",
      "kfold  15  score: 98.29%\n",
      "RandomForestClassifier(max_depth=12, n_estimators=10, random_state=3)\n",
      "kfold  15  score: 91.86%\n",
      "kfold  15  score: 98.72%\n",
      "RandomForestClassifier(max_depth=12, n_estimators=10, random_state=3)\n",
      "kfold  15  score: 92.79%\n",
      "kfold  15  score: 98.34%\n",
      "RandomForestClassifier(max_depth=12, n_estimators=10, random_state=3)\n",
      "kfold  15  score: 92.79%\n",
      "kfold  15  score: 98.32%\n",
      "RandomForestClassifier(max_depth=12, n_estimators=10, random_state=3)\n",
      "kfold  15  score: 92.41%\n",
      "kfold  15  score: 98.82%\n",
      "RandomForestClassifier(max_depth=12, n_estimators=10, random_state=3)\n",
      "kfold  15  score: 92.60%\n",
      "kfold  15  score: 98.78%\n",
      "Average score over k-fold 93.40% (+/-1.28%) [Random Forest]\n",
      "[21:48:52] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:48:52] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
      "              gamma=0, gpu_id=-1, importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.300000012,\n",
      "              max_delta_step=0, max_depth=12, max_leaf_nodes=None,\n",
      "              min_child_weight=1, min_samples_split=1, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=10, n_jobs=8,\n",
      "              num_parallel_tree=1, predictor='auto', random_state=3,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)\n",
      "kfold  15  score: 94.70%\n",
      "kfold  15  score: 99.47%\n",
      "[21:48:53] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:48:53] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
      "              gamma=0, gpu_id=-1, importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.300000012,\n",
      "              max_delta_step=0, max_depth=12, max_leaf_nodes=None,\n",
      "              min_child_weight=1, min_samples_split=1, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=10, n_jobs=8,\n",
      "              num_parallel_tree=1, predictor='auto', random_state=3,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)\n",
      "kfold  15  score: 95.27%\n",
      "kfold  15  score: 99.43%\n",
      "[21:48:53] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:48:53] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
      "              gamma=0, gpu_id=-1, importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.300000012,\n",
      "              max_delta_step=0, max_depth=12, max_leaf_nodes=None,\n",
      "              min_child_weight=1, min_samples_split=1, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=10, n_jobs=8,\n",
      "              num_parallel_tree=1, predictor='auto', random_state=3,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)\n",
      "kfold  15  score: 92.42%\n",
      "kfold  15  score: 99.49%\n",
      "[21:48:53] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:48:53] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
      "              gamma=0, gpu_id=-1, importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.300000012,\n",
      "              max_delta_step=0, max_depth=12, max_leaf_nodes=None,\n",
      "              min_child_weight=1, min_samples_split=1, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=10, n_jobs=8,\n",
      "              num_parallel_tree=1, predictor='auto', random_state=3,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)\n",
      "kfold  15  score: 94.89%\n",
      "kfold  15  score: 99.54%\n",
      "[21:48:53] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:48:53] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
      "              gamma=0, gpu_id=-1, importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.300000012,\n",
      "              max_delta_step=0, max_depth=12, max_leaf_nodes=None,\n",
      "              min_child_weight=1, min_samples_split=1, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=10, n_jobs=8,\n",
      "              num_parallel_tree=1, predictor='auto', random_state=3,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)\n",
      "kfold  15  score: 93.94%\n",
      "kfold  15  score: 99.37%\n",
      "[21:48:53] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:48:53] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
      "              gamma=0, gpu_id=-1, importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.300000012,\n",
      "              max_delta_step=0, max_depth=12, max_leaf_nodes=None,\n",
      "              min_child_weight=1, min_samples_split=1, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=10, n_jobs=8,\n",
      "              num_parallel_tree=1, predictor='auto', random_state=3,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)\n",
      "kfold  15  score: 92.99%\n",
      "kfold  15  score: 99.47%\n",
      "[21:48:53] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:48:53] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
      "              gamma=0, gpu_id=-1, importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.300000012,\n",
      "              max_delta_step=0, max_depth=12, max_leaf_nodes=None,\n",
      "              min_child_weight=1, min_samples_split=1, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=10, n_jobs=8,\n",
      "              num_parallel_tree=1, predictor='auto', random_state=3,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)\n",
      "kfold  15  score: 94.88%\n",
      "kfold  15  score: 99.58%\n",
      "[21:48:53] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:48:53] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
      "              gamma=0, gpu_id=-1, importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.300000012,\n",
      "              max_delta_step=0, max_depth=12, max_leaf_nodes=None,\n",
      "              min_child_weight=1, min_samples_split=1, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=10, n_jobs=8,\n",
      "              num_parallel_tree=1, predictor='auto', random_state=3,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)\n",
      "kfold  15  score: 93.36%\n",
      "kfold  15  score: 99.39%\n",
      "[21:48:53] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:48:53] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
      "              gamma=0, gpu_id=-1, importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.300000012,\n",
      "              max_delta_step=0, max_depth=12, max_leaf_nodes=None,\n",
      "              min_child_weight=1, min_samples_split=1, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=10, n_jobs=8,\n",
      "              num_parallel_tree=1, predictor='auto', random_state=3,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)\n",
      "kfold  15  score: 92.22%\n",
      "kfold  15  score: 99.37%\n",
      "[21:48:53] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:48:53] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
      "              gamma=0, gpu_id=-1, importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.300000012,\n",
      "              max_delta_step=0, max_depth=12, max_leaf_nodes=None,\n",
      "              min_child_weight=1, min_samples_split=1, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=10, n_jobs=8,\n",
      "              num_parallel_tree=1, predictor='auto', random_state=3,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)\n",
      "kfold  15  score: 93.17%\n",
      "kfold  15  score: 99.41%\n",
      "Average score over k-fold 93.78% (+/-1.05%) [XGBClassifier]\n",
      "[21:48:54] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:48:54] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "VotingClassifier(estimators=[('gb',\n",
      "                              GradientBoostingClassifier(max_depth=12,\n",
      "                                                         min_samples_split=12,\n",
      "                                                         n_estimators=5,\n",
      "                                                         random_state=3)),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(max_depth=12,\n",
      "                                                     n_estimators=10,\n",
      "                                                     random_state=3)),\n",
      "                             ('xgb',\n",
      "                              XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "                                            colsample_bylevel=1,\n",
      "                                            colsample_bynode=1,\n",
      "                                            colsample_bytree=1,\n",
      "                                            enable_categorical=False, gamma=0,\n",
      "                                            gpu...\n",
      "                                            interaction_constraints='',\n",
      "                                            learning_rate=0.300000012,\n",
      "                                            max_delta_step=0, max_depth=12,\n",
      "                                            max_leaf_nodes=None,\n",
      "                                            min_child_weight=1,\n",
      "                                            min_samples_split=1, missing=nan,\n",
      "                                            monotone_constraints='()',\n",
      "                                            n_estimators=10, n_jobs=8,\n",
      "                                            num_parallel_tree=1,\n",
      "                                            predictor='auto', random_state=3,\n",
      "                                            reg_alpha=0, reg_lambda=1,\n",
      "                                            scale_pos_weight=1, subsample=1,\n",
      "                                            tree_method='exact',\n",
      "                                            validate_parameters=1,\n",
      "                                            verbosity=None))])\n",
      "kfold  15  score: 94.70%\n",
      "kfold  15  score: 99.49%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21:48:54] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:48:54] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "VotingClassifier(estimators=[('gb',\n",
      "                              GradientBoostingClassifier(max_depth=12,\n",
      "                                                         min_samples_split=12,\n",
      "                                                         n_estimators=5,\n",
      "                                                         random_state=3)),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(max_depth=12,\n",
      "                                                     n_estimators=10,\n",
      "                                                     random_state=3)),\n",
      "                             ('xgb',\n",
      "                              XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "                                            colsample_bylevel=1,\n",
      "                                            colsample_bynode=1,\n",
      "                                            colsample_bytree=1,\n",
      "                                            enable_categorical=False, gamma=0,\n",
      "                                            gpu...\n",
      "                                            interaction_constraints='',\n",
      "                                            learning_rate=0.300000012,\n",
      "                                            max_delta_step=0, max_depth=12,\n",
      "                                            max_leaf_nodes=None,\n",
      "                                            min_child_weight=1,\n",
      "                                            min_samples_split=1, missing=nan,\n",
      "                                            monotone_constraints='()',\n",
      "                                            n_estimators=10, n_jobs=8,\n",
      "                                            num_parallel_tree=1,\n",
      "                                            predictor='auto', random_state=3,\n",
      "                                            reg_alpha=0, reg_lambda=1,\n",
      "                                            scale_pos_weight=1, subsample=1,\n",
      "                                            tree_method='exact',\n",
      "                                            validate_parameters=1,\n",
      "                                            verbosity=None))])\n",
      "kfold  15  score: 96.02%\n",
      "kfold  15  score: 99.30%\n",
      "[21:48:55] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:48:55] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "VotingClassifier(estimators=[('gb',\n",
      "                              GradientBoostingClassifier(max_depth=12,\n",
      "                                                         min_samples_split=12,\n",
      "                                                         n_estimators=5,\n",
      "                                                         random_state=3)),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(max_depth=12,\n",
      "                                                     n_estimators=10,\n",
      "                                                     random_state=3)),\n",
      "                             ('xgb',\n",
      "                              XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "                                            colsample_bylevel=1,\n",
      "                                            colsample_bynode=1,\n",
      "                                            colsample_bytree=1,\n",
      "                                            enable_categorical=False, gamma=0,\n",
      "                                            gpu...\n",
      "                                            interaction_constraints='',\n",
      "                                            learning_rate=0.300000012,\n",
      "                                            max_delta_step=0, max_depth=12,\n",
      "                                            max_leaf_nodes=None,\n",
      "                                            min_child_weight=1,\n",
      "                                            min_samples_split=1, missing=nan,\n",
      "                                            monotone_constraints='()',\n",
      "                                            n_estimators=10, n_jobs=8,\n",
      "                                            num_parallel_tree=1,\n",
      "                                            predictor='auto', random_state=3,\n",
      "                                            reg_alpha=0, reg_lambda=1,\n",
      "                                            scale_pos_weight=1, subsample=1,\n",
      "                                            tree_method='exact',\n",
      "                                            validate_parameters=1,\n",
      "                                            verbosity=None))])\n",
      "kfold  15  score: 91.86%\n",
      "kfold  15  score: 99.03%\n",
      "[21:48:55] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:48:55] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "VotingClassifier(estimators=[('gb',\n",
      "                              GradientBoostingClassifier(max_depth=12,\n",
      "                                                         min_samples_split=12,\n",
      "                                                         n_estimators=5,\n",
      "                                                         random_state=3)),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(max_depth=12,\n",
      "                                                     n_estimators=10,\n",
      "                                                     random_state=3)),\n",
      "                             ('xgb',\n",
      "                              XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "                                            colsample_bylevel=1,\n",
      "                                            colsample_bynode=1,\n",
      "                                            colsample_bytree=1,\n",
      "                                            enable_categorical=False, gamma=0,\n",
      "                                            gpu...\n",
      "                                            interaction_constraints='',\n",
      "                                            learning_rate=0.300000012,\n",
      "                                            max_delta_step=0, max_depth=12,\n",
      "                                            max_leaf_nodes=None,\n",
      "                                            min_child_weight=1,\n",
      "                                            min_samples_split=1, missing=nan,\n",
      "                                            monotone_constraints='()',\n",
      "                                            n_estimators=10, n_jobs=8,\n",
      "                                            num_parallel_tree=1,\n",
      "                                            predictor='auto', random_state=3,\n",
      "                                            reg_alpha=0, reg_lambda=1,\n",
      "                                            scale_pos_weight=1, subsample=1,\n",
      "                                            tree_method='exact',\n",
      "                                            validate_parameters=1,\n",
      "                                            verbosity=None))])\n",
      "kfold  15  score: 96.21%\n",
      "kfold  15  score: 99.26%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21:48:56] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:48:56] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "VotingClassifier(estimators=[('gb',\n",
      "                              GradientBoostingClassifier(max_depth=12,\n",
      "                                                         min_samples_split=12,\n",
      "                                                         n_estimators=5,\n",
      "                                                         random_state=3)),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(max_depth=12,\n",
      "                                                     n_estimators=10,\n",
      "                                                     random_state=3)),\n",
      "                             ('xgb',\n",
      "                              XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "                                            colsample_bylevel=1,\n",
      "                                            colsample_bynode=1,\n",
      "                                            colsample_bytree=1,\n",
      "                                            enable_categorical=False, gamma=0,\n",
      "                                            gpu...\n",
      "                                            interaction_constraints='',\n",
      "                                            learning_rate=0.300000012,\n",
      "                                            max_delta_step=0, max_depth=12,\n",
      "                                            max_leaf_nodes=None,\n",
      "                                            min_child_weight=1,\n",
      "                                            min_samples_split=1, missing=nan,\n",
      "                                            monotone_constraints='()',\n",
      "                                            n_estimators=10, n_jobs=8,\n",
      "                                            num_parallel_tree=1,\n",
      "                                            predictor='auto', random_state=3,\n",
      "                                            reg_alpha=0, reg_lambda=1,\n",
      "                                            scale_pos_weight=1, subsample=1,\n",
      "                                            tree_method='exact',\n",
      "                                            validate_parameters=1,\n",
      "                                            verbosity=None))])\n",
      "kfold  15  score: 94.70%\n",
      "kfold  15  score: 98.97%\n",
      "[21:48:56] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:48:56] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "VotingClassifier(estimators=[('gb',\n",
      "                              GradientBoostingClassifier(max_depth=12,\n",
      "                                                         min_samples_split=12,\n",
      "                                                         n_estimators=5,\n",
      "                                                         random_state=3)),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(max_depth=12,\n",
      "                                                     n_estimators=10,\n",
      "                                                     random_state=3)),\n",
      "                             ('xgb',\n",
      "                              XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "                                            colsample_bylevel=1,\n",
      "                                            colsample_bynode=1,\n",
      "                                            colsample_bytree=1,\n",
      "                                            enable_categorical=False, gamma=0,\n",
      "                                            gpu...\n",
      "                                            interaction_constraints='',\n",
      "                                            learning_rate=0.300000012,\n",
      "                                            max_delta_step=0, max_depth=12,\n",
      "                                            max_leaf_nodes=None,\n",
      "                                            min_child_weight=1,\n",
      "                                            min_samples_split=1, missing=nan,\n",
      "                                            monotone_constraints='()',\n",
      "                                            n_estimators=10, n_jobs=8,\n",
      "                                            num_parallel_tree=1,\n",
      "                                            predictor='auto', random_state=3,\n",
      "                                            reg_alpha=0, reg_lambda=1,\n",
      "                                            scale_pos_weight=1, subsample=1,\n",
      "                                            tree_method='exact',\n",
      "                                            validate_parameters=1,\n",
      "                                            verbosity=None))])\n",
      "kfold  15  score: 93.18%\n",
      "kfold  15  score: 99.12%\n",
      "[21:48:57] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:48:57] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "VotingClassifier(estimators=[('gb',\n",
      "                              GradientBoostingClassifier(max_depth=12,\n",
      "                                                         min_samples_split=12,\n",
      "                                                         n_estimators=5,\n",
      "                                                         random_state=3)),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(max_depth=12,\n",
      "                                                     n_estimators=10,\n",
      "                                                     random_state=3)),\n",
      "                             ('xgb',\n",
      "                              XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "                                            colsample_bylevel=1,\n",
      "                                            colsample_bynode=1,\n",
      "                                            colsample_bytree=1,\n",
      "                                            enable_categorical=False, gamma=0,\n",
      "                                            gpu...\n",
      "                                            interaction_constraints='',\n",
      "                                            learning_rate=0.300000012,\n",
      "                                            max_delta_step=0, max_depth=12,\n",
      "                                            max_leaf_nodes=None,\n",
      "                                            min_child_weight=1,\n",
      "                                            min_samples_split=1, missing=nan,\n",
      "                                            monotone_constraints='()',\n",
      "                                            n_estimators=10, n_jobs=8,\n",
      "                                            num_parallel_tree=1,\n",
      "                                            predictor='auto', random_state=3,\n",
      "                                            reg_alpha=0, reg_lambda=1,\n",
      "                                            scale_pos_weight=1, subsample=1,\n",
      "                                            tree_method='exact',\n",
      "                                            validate_parameters=1,\n",
      "                                            verbosity=None))])\n",
      "kfold  15  score: 94.50%\n",
      "kfold  15  score: 99.22%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21:48:57] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:48:57] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "VotingClassifier(estimators=[('gb',\n",
      "                              GradientBoostingClassifier(max_depth=12,\n",
      "                                                         min_samples_split=12,\n",
      "                                                         n_estimators=5,\n",
      "                                                         random_state=3)),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(max_depth=12,\n",
      "                                                     n_estimators=10,\n",
      "                                                     random_state=3)),\n",
      "                             ('xgb',\n",
      "                              XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "                                            colsample_bylevel=1,\n",
      "                                            colsample_bynode=1,\n",
      "                                            colsample_bytree=1,\n",
      "                                            enable_categorical=False, gamma=0,\n",
      "                                            gpu...\n",
      "                                            interaction_constraints='',\n",
      "                                            learning_rate=0.300000012,\n",
      "                                            max_delta_step=0, max_depth=12,\n",
      "                                            max_leaf_nodes=None,\n",
      "                                            min_child_weight=1,\n",
      "                                            min_samples_split=1, missing=nan,\n",
      "                                            monotone_constraints='()',\n",
      "                                            n_estimators=10, n_jobs=8,\n",
      "                                            num_parallel_tree=1,\n",
      "                                            predictor='auto', random_state=3,\n",
      "                                            reg_alpha=0, reg_lambda=1,\n",
      "                                            scale_pos_weight=1, subsample=1,\n",
      "                                            tree_method='exact',\n",
      "                                            validate_parameters=1,\n",
      "                                            verbosity=None))])\n",
      "kfold  15  score: 93.36%\n",
      "kfold  15  score: 99.26%\n",
      "[21:48:58] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:48:58] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "VotingClassifier(estimators=[('gb',\n",
      "                              GradientBoostingClassifier(max_depth=12,\n",
      "                                                         min_samples_split=12,\n",
      "                                                         n_estimators=5,\n",
      "                                                         random_state=3)),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(max_depth=12,\n",
      "                                                     n_estimators=10,\n",
      "                                                     random_state=3)),\n",
      "                             ('xgb',\n",
      "                              XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "                                            colsample_bylevel=1,\n",
      "                                            colsample_bynode=1,\n",
      "                                            colsample_bytree=1,\n",
      "                                            enable_categorical=False, gamma=0,\n",
      "                                            gpu...\n",
      "                                            interaction_constraints='',\n",
      "                                            learning_rate=0.300000012,\n",
      "                                            max_delta_step=0, max_depth=12,\n",
      "                                            max_leaf_nodes=None,\n",
      "                                            min_child_weight=1,\n",
      "                                            min_samples_split=1, missing=nan,\n",
      "                                            monotone_constraints='()',\n",
      "                                            n_estimators=10, n_jobs=8,\n",
      "                                            num_parallel_tree=1,\n",
      "                                            predictor='auto', random_state=3,\n",
      "                                            reg_alpha=0, reg_lambda=1,\n",
      "                                            scale_pos_weight=1, subsample=1,\n",
      "                                            tree_method='exact',\n",
      "                                            validate_parameters=1,\n",
      "                                            verbosity=None))])\n",
      "kfold  15  score: 92.22%\n",
      "kfold  15  score: 99.12%\n",
      "[21:48:58] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:48:58] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "VotingClassifier(estimators=[('gb',\n",
      "                              GradientBoostingClassifier(max_depth=12,\n",
      "                                                         min_samples_split=12,\n",
      "                                                         n_estimators=5,\n",
      "                                                         random_state=3)),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(max_depth=12,\n",
      "                                                     n_estimators=10,\n",
      "                                                     random_state=3)),\n",
      "                             ('xgb',\n",
      "                              XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "                                            colsample_bylevel=1,\n",
      "                                            colsample_bynode=1,\n",
      "                                            colsample_bytree=1,\n",
      "                                            enable_categorical=False, gamma=0,\n",
      "                                            gpu...\n",
      "                                            interaction_constraints='',\n",
      "                                            learning_rate=0.300000012,\n",
      "                                            max_delta_step=0, max_depth=12,\n",
      "                                            max_leaf_nodes=None,\n",
      "                                            min_child_weight=1,\n",
      "                                            min_samples_split=1, missing=nan,\n",
      "                                            monotone_constraints='()',\n",
      "                                            n_estimators=10, n_jobs=8,\n",
      "                                            num_parallel_tree=1,\n",
      "                                            predictor='auto', random_state=3,\n",
      "                                            reg_alpha=0, reg_lambda=1,\n",
      "                                            scale_pos_weight=1, subsample=1,\n",
      "                                            tree_method='exact',\n",
      "                                            validate_parameters=1,\n",
      "                                            verbosity=None))])\n",
      "kfold  15  score: 93.93%\n",
      "kfold  15  score: 99.22%\n",
      "Average score over k-fold 94.07% (+/-1.38%) [Ensemble Majority]\n",
      "==end==\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAD8CAYAAAB0IB+mAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8/fFQqAAAACXBIWXMAAAsTAAALEwEAmpwYAAANT0lEQVR4nO3cYYjkd33H8ffHO1NpjKb0VpC706T00njYQtIlTRFqirZc8uDugUXuIFgleGAbKVWEFEuU+MiGWhCu1ZOKVdAYfSALntwDjQTEC7chNXgXItvTeheFrDHNk6Ax7bcPZtKdrneZf3Zndy/7fb/gYP7/+e3Mlx97752d2ZlUFZKk7e8VWz2AJGlzGHxJasLgS1ITBl+SmjD4ktSEwZekJqYGP8lnkzyZ5PuXuD5JPplkKcmjSW6c/ZiSpPUa8gj/c8CBF7n+VmDf+N9R4F/WP5YkadamBr+qHgR+/iJLDgGfr5FTwNVJXj+rASVJs7FzBrexGzg/cXxhfO6nqxcmOcrotwCuvPLKP7z++utncPeS1MfDDz/8s6qaW8vXziL4g1XVceA4wPz8fC0uLm7m3UvSy16S/1zr187ir3SeAPZOHO8Zn5MkXUZmEfwF4F3jv9a5GXimqn7t6RxJ0taa+pROki8BtwC7klwAPgK8EqCqPgWcAG4DloBngfds1LCSpLWbGvyqOjLl+gL+emYTSZI2hO+0laQmDL4kNWHwJakJgy9JTRh8SWrC4EtSEwZfkpow+JLUhMGXpCYMviQ1YfAlqQmDL0lNGHxJasLgS1ITBl+SmjD4ktSEwZekJgy+JDVh8CWpCYMvSU0YfElqwuBLUhMGX5KaMPiS1ITBl6QmDL4kNWHwJakJgy9JTRh8SWrC4EtSEwZfkpow+JLUhMGXpCYMviQ1YfAlqYlBwU9yIMnjSZaS3HWR69+Q5IEkjyR5NMltsx9VkrQeU4OfZAdwDLgV2A8cSbJ/1bK/B+6vqhuAw8A/z3pQSdL6DHmEfxOwVFXnquo54D7g0Ko1BbxmfPm1wE9mN6IkaRaGBH83cH7i+ML43KSPArcnuQCcAN5/sRtKcjTJYpLF5eXlNYwrSVqrWb1oewT4XFXtAW4DvpDk1267qo5X1XxVzc/Nzc3oriVJQwwJ/hPA3onjPeNzk+4A7geoqu8CrwJ2zWJASdJsDAn+aWBfkmuTXMHoRdmFVWt+DLwNIMmbGAXf52wk6TIyNfhV9TxwJ3ASeIzRX+OcSXJPkoPjZR8E3pvke8CXgHdXVW3U0JKkl27nkEVVdYLRi7GT5+6euHwWeMtsR5MkzZLvtJWkJgy+JDVh8CWpCYMvSU0YfElqwuBLUhMGX5KaMPiS1ITBl6QmDL4kNWHwJakJgy9JTRh8SWrC4EtSEwZfkpow+JLUhMGXpCYMviQ1YfAlqQmDL0lNGHxJasLgS1ITBl+SmjD4ktSEwZekJgy+JDVh8CWpCYMvSU0YfElqwuBLUhMGX5KaMPiS1ITBl6QmDL4kNTEo+EkOJHk8yVKSuy6x5p1JziY5k+SLsx1TkrReO6ctSLIDOAb8GXABOJ1koarOTqzZB/wd8JaqejrJ6zZqYEnS2gx5hH8TsFRV56rqOeA+4NCqNe8FjlXV0wBV9eRsx5QkrdeQ4O8Gzk8cXxifm3QdcF2S7yQ5leTAxW4oydEki0kWl5eX1zaxJGlNZvWi7U5gH3ALcAT4TJKrVy+qquNVNV9V83NzczO6a0nSEEOC/wSwd+J4z/jcpAvAQlX9qqp+CPyA0Q8ASdJlYkjwTwP7klyb5ArgMLCwas3XGD26J8kuRk/xnJvdmJKk9Zoa/Kp6HrgTOAk8BtxfVWeS3JPk4HjZSeCpJGeBB4APVdVTGzW0JOmlS1VtyR3Pz8/X4uLilty3JL1cJXm4qubX8rW+01aSmjD4ktSEwZekJgy+JDVh8CWpCYMvSU0YfElqwuBLUhMGX5KaMPiS1ITBl6QmDL4kNWHwJakJgy9JTRh8SWrC4EtSEwZfkpow+JLUhMGXpCYMviQ1YfAlqQmDL0lNGHxJasLgS1ITBl+SmjD4ktSEwZekJgy+JDVh8CWpCYMvSU0YfElqwuBLUhMGX5KaMPiS1ITBl6QmBgU/yYEkjydZSnLXi6x7R5JKMj+7ESVJszA1+El2AMeAW4H9wJEk+y+y7irgb4CHZj2kJGn9hjzCvwlYqqpzVfUccB9w6CLrPgZ8HPjFDOeTJM3IkODvBs5PHF8Yn/s/SW4E9lbV11/shpIcTbKYZHF5efklDytJWrt1v2ib5BXAJ4APTltbVcerar6q5ufm5tZ715Kkl2BI8J8A9k4c7xmfe8FVwJuBbyf5EXAzsOALt5J0eRkS/NPAviTXJrkCOAwsvHBlVT1TVbuq6pqqugY4BRysqsUNmViStCZTg19VzwN3AieBx4D7q+pMknuSHNzoASVJs7FzyKKqOgGcWHXu7kusvWX9Y0mSZs132kpSEwZfkpow+JLUhMGXpCYMviQ1YfAlqQmDL0lNGHxJasLgS1ITBl+SmjD4ktSEwZekJgy+JDVh8CWpCYMvSU0YfElqwuBLUhMGX5KaMPiS1ITBl6QmDL4kNWHwJakJgy9JTRh8SWrC4EtSEwZfkpow+JLUhMGXpCYMviQ1YfAlqQmDL0lNGHxJasLgS1ITBl+SmhgU/CQHkjyeZCnJXRe5/gNJziZ5NMk3k7xx9qNKktZjavCT7ACOAbcC+4EjSfavWvYIMF9VfwB8FfiHWQ8qSVqfIY/wbwKWqupcVT0H3AccmlxQVQ9U1bPjw1PAntmOKUlaryHB3w2cnzi+MD53KXcA37jYFUmOJllMsri8vDx8SknSus30RdsktwPzwL0Xu76qjlfVfFXNz83NzfKuJUlT7Byw5glg78TxnvG5/yfJ24EPA2+tql/OZjxJ0qwMeYR/GtiX5NokVwCHgYXJBUluAD4NHKyqJ2c/piRpvaYGv6qeB+4ETgKPAfdX1Zkk9yQ5OF52L/Bq4CtJ/j3JwiVuTpK0RYY8pUNVnQBOrDp398Tlt894LknSjPlOW0lqwuBLUhMGX5KaMPiS1ITBl6QmDL4kNWHwJakJgy9JTRh8SWrC4EtSEwZfkpow+JLUhMGXpCYMviQ1YfAlqQmDL0lNGHxJasLgS1ITBl+SmjD4ktSEwZekJgy+JDVh8CWpCYMvSU0YfElqwuBLUhMGX5KaMPiS1ITBl6QmDL4kNWHwJakJgy9JTRh8SWrC4EtSEwZfkpoYFPwkB5I8nmQpyV0Xuf43knx5fP1DSa6Z+aSSpHWZGvwkO4BjwK3AfuBIkv2rlt0BPF1Vvwv8E/DxWQ8qSVqfIY/wbwKWqupcVT0H3AccWrXmEPBv48tfBd6WJLMbU5K0XjsHrNkNnJ84vgD80aXWVNXzSZ4Bfhv42eSiJEeBo+PDXyb5/lqG3oZ2sWqvGnMvVrgXK9yLFb+31i8cEvyZqarjwHGAJItVNb+Z93+5ci9WuBcr3IsV7sWKJItr/dohT+k8AeydON4zPnfRNUl2Aq8FnlrrUJKk2RsS/NPAviTXJrkCOAwsrFqzAPzl+PJfAN+qqprdmJKk9Zr6lM74Ofk7gZPADuCzVXUmyT3AYlUtAP8KfCHJEvBzRj8Upjm+jrm3G/dihXuxwr1Y4V6sWPNexAfiktSD77SVpCYMviQ1seHB92MZVgzYiw8kOZvk0STfTPLGrZhzM0zbi4l170hSSbbtn+QN2Ysk7xx/b5xJ8sXNnnGzDPg/8oYkDyR5ZPz/5LatmHOjJflskicv9V6ljHxyvE+PJrlx0A1X1Yb9Y/Qi738AvwNcAXwP2L9qzV8BnxpfPgx8eSNn2qp/A/fiT4HfHF9+X+e9GK+7CngQOAXMb/XcW/h9sQ94BPit8fHrtnruLdyL48D7xpf3Az/a6rk3aC/+BLgR+P4lrr8N+AYQ4GbgoSG3u9GP8P1YhhVT96KqHqiqZ8eHpxi952E7GvJ9AfAxRp/L9IvNHG6TDdmL9wLHquppgKp6cpNn3CxD9qKA14wvvxb4ySbOt2mq6kFGf/F4KYeAz9fIKeDqJK+fdrsbHfyLfSzD7kutqarngRc+lmG7GbIXk+5g9BN8O5q6F+NfUfdW1dc3c7AtMOT74jrguiTfSXIqyYFNm25zDdmLjwK3J7kAnADevzmjXXZeak+ATf5oBQ2T5HZgHnjrVs+yFZK8AvgE8O4tHuVysZPR0zq3MPqt78Ekv19V/7WVQ22RI8Dnquofk/wxo/f/vLmq/merB3s52OhH+H4sw4ohe0GStwMfBg5W1S83abbNNm0vrgLeDHw7yY8YPUe5sE1fuB3yfXEBWKiqX1XVD4EfMPoBsN0M2Ys7gPsBquq7wKsYfbBaN4N6stpGB9+PZVgxdS+S3AB8mlHst+vztDBlL6rqmaraVVXXVNU1jF7POFhVa/7QqMvYkP8jX2P06J4kuxg9xXNuE2fcLEP24sfA2wCSvIlR8Jc3dcrLwwLwrvFf69wMPFNVP532RRv6lE5t3McyvOwM3It7gVcDXxm/bv3jqjq4ZUNvkIF70cLAvTgJ/HmSs8B/Ax+qqm33W/DAvfgg8Jkkf8voBdx3b8cHiEm+xOiH/K7x6xUfAV4JUFWfYvT6xW3AEvAs8J5Bt7sN90qSdBG+01aSmjD4ktSEwZekJgy+JDVh8CWpCYMvSU0YfElq4n8BzPZculjwdYoAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAD8CAYAAAB0IB+mAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8/fFQqAAAACXBIWXMAAAsTAAALEwEAmpwYAAANT0lEQVR4nO3cYYjkd33H8ffHO1NpjKb0VpC706T00njYQtIlTRFqirZc8uDugUXuIFgleGAbKVWEFEuU+MiGWhCu1ZOKVdAYfSALntwDjQTEC7chNXgXItvTeheFrDHNk6Ax7bcPZtKdrneZf3Zndy/7fb/gYP7/+e3Mlx97752d2ZlUFZKk7e8VWz2AJGlzGHxJasLgS1ITBl+SmjD4ktSEwZekJqYGP8lnkzyZ5PuXuD5JPplkKcmjSW6c/ZiSpPUa8gj/c8CBF7n+VmDf+N9R4F/WP5YkadamBr+qHgR+/iJLDgGfr5FTwNVJXj+rASVJs7FzBrexGzg/cXxhfO6nqxcmOcrotwCuvPLKP7z++utncPeS1MfDDz/8s6qaW8vXziL4g1XVceA4wPz8fC0uLm7m3UvSy16S/1zr187ir3SeAPZOHO8Zn5MkXUZmEfwF4F3jv9a5GXimqn7t6RxJ0taa+pROki8BtwC7klwAPgK8EqCqPgWcAG4DloBngfds1LCSpLWbGvyqOjLl+gL+emYTSZI2hO+0laQmDL4kNWHwJakJgy9JTRh8SWrC4EtSEwZfkpow+JLUhMGXpCYMviQ1YfAlqQmDL0lNGHxJasLgS1ITBl+SmjD4ktSEwZekJgy+JDVh8CWpCYMvSU0YfElqwuBLUhMGX5KaMPiS1ITBl6QmDL4kNWHwJakJgy9JTRh8SWrC4EtSEwZfkpow+JLUhMGXpCYMviQ1YfAlqYlBwU9yIMnjSZaS3HWR69+Q5IEkjyR5NMltsx9VkrQeU4OfZAdwDLgV2A8cSbJ/1bK/B+6vqhuAw8A/z3pQSdL6DHmEfxOwVFXnquo54D7g0Ko1BbxmfPm1wE9mN6IkaRaGBH83cH7i+ML43KSPArcnuQCcAN5/sRtKcjTJYpLF5eXlNYwrSVqrWb1oewT4XFXtAW4DvpDk1267qo5X1XxVzc/Nzc3oriVJQwwJ/hPA3onjPeNzk+4A7geoqu8CrwJ2zWJASdJsDAn+aWBfkmuTXMHoRdmFVWt+DLwNIMmbGAXf52wk6TIyNfhV9TxwJ3ASeIzRX+OcSXJPkoPjZR8E3pvke8CXgHdXVW3U0JKkl27nkEVVdYLRi7GT5+6euHwWeMtsR5MkzZLvtJWkJgy+JDVh8CWpCYMvSU0YfElqwuBLUhMGX5KaMPiS1ITBl6QmDL4kNWHwJakJgy9JTRh8SWrC4EtSEwZfkpow+JLUhMGXpCYMviQ1YfAlqQmDL0lNGHxJasLgS1ITBl+SmjD4ktSEwZekJgy+JDVh8CWpCYMvSU0YfElqwuBLUhMGX5KaMPiS1ITBl6QmDL4kNTEo+EkOJHk8yVKSuy6x5p1JziY5k+SLsx1TkrReO6ctSLIDOAb8GXABOJ1koarOTqzZB/wd8JaqejrJ6zZqYEnS2gx5hH8TsFRV56rqOeA+4NCqNe8FjlXV0wBV9eRsx5QkrdeQ4O8Gzk8cXxifm3QdcF2S7yQ5leTAxW4oydEki0kWl5eX1zaxJGlNZvWi7U5gH3ALcAT4TJKrVy+qquNVNV9V83NzczO6a0nSEEOC/wSwd+J4z/jcpAvAQlX9qqp+CPyA0Q8ASdJlYkjwTwP7klyb5ArgMLCwas3XGD26J8kuRk/xnJvdmJKk9Zoa/Kp6HrgTOAk8BtxfVWeS3JPk4HjZSeCpJGeBB4APVdVTGzW0JOmlS1VtyR3Pz8/X4uLilty3JL1cJXm4qubX8rW+01aSmjD4ktSEwZekJgy+JDVh8CWpCYMvSU0YfElqwuBLUhMGX5KaMPiS1ITBl6QmDL4kNWHwJakJgy9JTRh8SWrC4EtSEwZfkpow+JLUhMGXpCYMviQ1YfAlqQmDL0lNGHxJasLgS1ITBl+SmjD4ktSEwZekJgy+JDVh8CWpCYMvSU0YfElqwuBLUhMGX5KaMPiS1ITBl6QmBgU/yYEkjydZSnLXi6x7R5JKMj+7ESVJszA1+El2AMeAW4H9wJEk+y+y7irgb4CHZj2kJGn9hjzCvwlYqqpzVfUccB9w6CLrPgZ8HPjFDOeTJM3IkODvBs5PHF8Yn/s/SW4E9lbV11/shpIcTbKYZHF5efklDytJWrt1v2ib5BXAJ4APTltbVcerar6q5ufm5tZ715Kkl2BI8J8A9k4c7xmfe8FVwJuBbyf5EXAzsOALt5J0eRkS/NPAviTXJrkCOAwsvHBlVT1TVbuq6pqqugY4BRysqsUNmViStCZTg19VzwN3AieBx4D7q+pMknuSHNzoASVJs7FzyKKqOgGcWHXu7kusvWX9Y0mSZs132kpSEwZfkpow+JLUhMGXpCYMviQ1YfAlqQmDL0lNGHxJasLgS1ITBl+SmjD4ktSEwZekJgy+JDVh8CWpCYMvSU0YfElqwuBLUhMGX5KaMPiS1ITBl6QmDL4kNWHwJakJgy9JTRh8SWrC4EtSEwZfkpow+JLUhMGXpCYMviQ1YfAlqQmDL0lNGHxJasLgS1ITBl+SmhgU/CQHkjyeZCnJXRe5/gNJziZ5NMk3k7xx9qNKktZjavCT7ACOAbcC+4EjSfavWvYIMF9VfwB8FfiHWQ8qSVqfIY/wbwKWqupcVT0H3AccmlxQVQ9U1bPjw1PAntmOKUlaryHB3w2cnzi+MD53KXcA37jYFUmOJllMsri8vDx8SknSus30RdsktwPzwL0Xu76qjlfVfFXNz83NzfKuJUlT7Byw5glg78TxnvG5/yfJ24EPA2+tql/OZjxJ0qwMeYR/GtiX5NokVwCHgYXJBUluAD4NHKyqJ2c/piRpvaYGv6qeB+4ETgKPAfdX1Zkk9yQ5OF52L/Bq4CtJ/j3JwiVuTpK0RYY8pUNVnQBOrDp398Tlt894LknSjPlOW0lqwuBLUhMGX5KaMPiS1ITBl6QmDL4kNWHwJakJgy9JTRh8SWrC4EtSEwZfkpow+JLUhMGXpCYMviQ1YfAlqQmDL0lNGHxJasLgS1ITBl+SmjD4ktSEwZekJgy+JDVh8CWpCYMvSU0YfElqwuBLUhMGX5KaMPiS1ITBl6QmDL4kNWHwJakJgy9JTRh8SWrC4EtSEwZfkpoYFPwkB5I8nmQpyV0Xuf43knx5fP1DSa6Z+aSSpHWZGvwkO4BjwK3AfuBIkv2rlt0BPF1Vvwv8E/DxWQ8qSVqfIY/wbwKWqupcVT0H3AccWrXmEPBv48tfBd6WJLMbU5K0XjsHrNkNnJ84vgD80aXWVNXzSZ4Bfhv42eSiJEeBo+PDXyb5/lqG3oZ2sWqvGnMvVrgXK9yLFb+31i8cEvyZqarjwHGAJItVNb+Z93+5ci9WuBcr3IsV7sWKJItr/dohT+k8AeydON4zPnfRNUl2Aq8FnlrrUJKk2RsS/NPAviTXJrkCOAwsrFqzAPzl+PJfAN+qqprdmJKk9Zr6lM74Ofk7gZPADuCzVXUmyT3AYlUtAP8KfCHJEvBzRj8Upjm+jrm3G/dihXuxwr1Y4V6sWPNexAfiktSD77SVpCYMviQ1seHB92MZVgzYiw8kOZvk0STfTPLGrZhzM0zbi4l170hSSbbtn+QN2Ysk7xx/b5xJ8sXNnnGzDPg/8oYkDyR5ZPz/5LatmHOjJflskicv9V6ljHxyvE+PJrlx0A1X1Yb9Y/Qi738AvwNcAXwP2L9qzV8BnxpfPgx8eSNn2qp/A/fiT4HfHF9+X+e9GK+7CngQOAXMb/XcW/h9sQ94BPit8fHrtnruLdyL48D7xpf3Az/a6rk3aC/+BLgR+P4lrr8N+AYQ4GbgoSG3u9GP8P1YhhVT96KqHqiqZ8eHpxi952E7GvJ9AfAxRp/L9IvNHG6TDdmL9wLHquppgKp6cpNn3CxD9qKA14wvvxb4ySbOt2mq6kFGf/F4KYeAz9fIKeDqJK+fdrsbHfyLfSzD7kutqarngRc+lmG7GbIXk+5g9BN8O5q6F+NfUfdW1dc3c7AtMOT74jrguiTfSXIqyYFNm25zDdmLjwK3J7kAnADevzmjXXZeak+ATf5oBQ2T5HZgHnjrVs+yFZK8AvgE8O4tHuVysZPR0zq3MPqt78Ekv19V/7WVQ22RI8Dnquofk/wxo/f/vLmq/merB3s52OhH+H4sw4ohe0GStwMfBg5W1S83abbNNm0vrgLeDHw7yY8YPUe5sE1fuB3yfXEBWKiqX1XVD4EfMPoBsN0M2Ys7gPsBquq7wKsYfbBaN4N6stpGB9+PZVgxdS+S3AB8mlHst+vztDBlL6rqmaraVVXXVNU1jF7POFhVa/7QqMvYkP8jX2P06J4kuxg9xXNuE2fcLEP24sfA2wCSvIlR8Jc3dcrLwwLwrvFf69wMPFNVP532RRv6lE5t3McyvOwM3It7gVcDXxm/bv3jqjq4ZUNvkIF70cLAvTgJ/HmSs8B/Ax+qqm33W/DAvfgg8Jkkf8voBdx3b8cHiEm+xOiH/K7x6xUfAV4JUFWfYvT6xW3AEvAs8J5Bt7sN90qSdBG+01aSmjD4ktSEwZekJgy+JDVh8CWpCYMvSU0YfElq4n8BzPZculjwdYoAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAD8CAYAAAB0IB+mAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8/fFQqAAAACXBIWXMAAAsTAAALEwEAmpwYAAANT0lEQVR4nO3cYYjkd33H8ffHO1NpjKb0VpC706T00njYQtIlTRFqirZc8uDugUXuIFgleGAbKVWEFEuU+MiGWhCu1ZOKVdAYfSALntwDjQTEC7chNXgXItvTeheFrDHNk6Ax7bcPZtKdrneZf3Zndy/7fb/gYP7/+e3Mlx97752d2ZlUFZKk7e8VWz2AJGlzGHxJasLgS1ITBl+SmjD4ktSEwZekJqYGP8lnkzyZ5PuXuD5JPplkKcmjSW6c/ZiSpPUa8gj/c8CBF7n+VmDf+N9R4F/WP5YkadamBr+qHgR+/iJLDgGfr5FTwNVJXj+rASVJs7FzBrexGzg/cXxhfO6nqxcmOcrotwCuvPLKP7z++utncPeS1MfDDz/8s6qaW8vXziL4g1XVceA4wPz8fC0uLm7m3UvSy16S/1zr187ir3SeAPZOHO8Zn5MkXUZmEfwF4F3jv9a5GXimqn7t6RxJ0taa+pROki8BtwC7klwAPgK8EqCqPgWcAG4DloBngfds1LCSpLWbGvyqOjLl+gL+emYTSZI2hO+0laQmDL4kNWHwJakJgy9JTRh8SWrC4EtSEwZfkpow+JLUhMGXpCYMviQ1YfAlqQmDL0lNGHxJasLgS1ITBl+SmjD4ktSEwZekJgy+JDVh8CWpCYMvSU0YfElqwuBLUhMGX5KaMPiS1ITBl6QmDL4kNWHwJakJgy9JTRh8SWrC4EtSEwZfkpow+JLUhMGXpCYMviQ1YfAlqYlBwU9yIMnjSZaS3HWR69+Q5IEkjyR5NMltsx9VkrQeU4OfZAdwDLgV2A8cSbJ/1bK/B+6vqhuAw8A/z3pQSdL6DHmEfxOwVFXnquo54D7g0Ko1BbxmfPm1wE9mN6IkaRaGBH83cH7i+ML43KSPArcnuQCcAN5/sRtKcjTJYpLF5eXlNYwrSVqrWb1oewT4XFXtAW4DvpDk1267qo5X1XxVzc/Nzc3oriVJQwwJ/hPA3onjPeNzk+4A7geoqu8CrwJ2zWJASdJsDAn+aWBfkmuTXMHoRdmFVWt+DLwNIMmbGAXf52wk6TIyNfhV9TxwJ3ASeIzRX+OcSXJPkoPjZR8E3pvke8CXgHdXVW3U0JKkl27nkEVVdYLRi7GT5+6euHwWeMtsR5MkzZLvtJWkJgy+JDVh8CWpCYMvSU0YfElqwuBLUhMGX5KaMPiS1ITBl6QmDL4kNWHwJakJgy9JTRh8SWrC4EtSEwZfkpow+JLUhMGXpCYMviQ1YfAlqQmDL0lNGHxJasLgS1ITBl+SmjD4ktSEwZekJgy+JDVh8CWpCYMvSU0YfElqwuBLUhMGX5KaMPiS1ITBl6QmDL4kNTEo+EkOJHk8yVKSuy6x5p1JziY5k+SLsx1TkrReO6ctSLIDOAb8GXABOJ1koarOTqzZB/wd8JaqejrJ6zZqYEnS2gx5hH8TsFRV56rqOeA+4NCqNe8FjlXV0wBV9eRsx5QkrdeQ4O8Gzk8cXxifm3QdcF2S7yQ5leTAxW4oydEki0kWl5eX1zaxJGlNZvWi7U5gH3ALcAT4TJKrVy+qquNVNV9V83NzczO6a0nSEEOC/wSwd+J4z/jcpAvAQlX9qqp+CPyA0Q8ASdJlYkjwTwP7klyb5ArgMLCwas3XGD26J8kuRk/xnJvdmJKk9Zoa/Kp6HrgTOAk8BtxfVWeS3JPk4HjZSeCpJGeBB4APVdVTGzW0JOmlS1VtyR3Pz8/X4uLilty3JL1cJXm4qubX8rW+01aSmjD4ktSEwZekJgy+JDVh8CWpCYMvSU0YfElqwuBLUhMGX5KaMPiS1ITBl6QmDL4kNWHwJakJgy9JTRh8SWrC4EtSEwZfkpow+JLUhMGXpCYMviQ1YfAlqQmDL0lNGHxJasLgS1ITBl+SmjD4ktSEwZekJgy+JDVh8CWpCYMvSU0YfElqwuBLUhMGX5KaMPiS1ITBl6QmBgU/yYEkjydZSnLXi6x7R5JKMj+7ESVJszA1+El2AMeAW4H9wJEk+y+y7irgb4CHZj2kJGn9hjzCvwlYqqpzVfUccB9w6CLrPgZ8HPjFDOeTJM3IkODvBs5PHF8Yn/s/SW4E9lbV11/shpIcTbKYZHF5efklDytJWrt1v2ib5BXAJ4APTltbVcerar6q5ufm5tZ715Kkl2BI8J8A9k4c7xmfe8FVwJuBbyf5EXAzsOALt5J0eRkS/NPAviTXJrkCOAwsvHBlVT1TVbuq6pqqugY4BRysqsUNmViStCZTg19VzwN3AieBx4D7q+pMknuSHNzoASVJs7FzyKKqOgGcWHXu7kusvWX9Y0mSZs132kpSEwZfkpow+JLUhMGXpCYMviQ1YfAlqQmDL0lNGHxJasLgS1ITBl+SmjD4ktSEwZekJgy+JDVh8CWpCYMvSU0YfElqwuBLUhMGX5KaMPiS1ITBl6QmDL4kNWHwJakJgy9JTRh8SWrC4EtSEwZfkpow+JLUhMGXpCYMviQ1YfAlqQmDL0lNGHxJasLgS1ITBl+SmhgU/CQHkjyeZCnJXRe5/gNJziZ5NMk3k7xx9qNKktZjavCT7ACOAbcC+4EjSfavWvYIMF9VfwB8FfiHWQ8qSVqfIY/wbwKWqupcVT0H3AccmlxQVQ9U1bPjw1PAntmOKUlaryHB3w2cnzi+MD53KXcA37jYFUmOJllMsri8vDx8SknSus30RdsktwPzwL0Xu76qjlfVfFXNz83NzfKuJUlT7Byw5glg78TxnvG5/yfJ24EPA2+tql/OZjxJ0qwMeYR/GtiX5NokVwCHgYXJBUluAD4NHKyqJ2c/piRpvaYGv6qeB+4ETgKPAfdX1Zkk9yQ5OF52L/Bq4CtJ/j3JwiVuTpK0RYY8pUNVnQBOrDp398Tlt894LknSjPlOW0lqwuBLUhMGX5KaMPiS1ITBl6QmDL4kNWHwJakJgy9JTRh8SWrC4EtSEwZfkpow+JLUhMGXpCYMviQ1YfAlqQmDL0lNGHxJasLgS1ITBl+SmjD4ktSEwZekJgy+JDVh8CWpCYMvSU0YfElqwuBLUhMGX5KaMPiS1ITBl6QmDL4kNWHwJakJgy9JTRh8SWrC4EtSEwZfkpoYFPwkB5I8nmQpyV0Xuf43knx5fP1DSa6Z+aSSpHWZGvwkO4BjwK3AfuBIkv2rlt0BPF1Vvwv8E/DxWQ8qSVqfIY/wbwKWqupcVT0H3AccWrXmEPBv48tfBd6WJLMbU5K0XjsHrNkNnJ84vgD80aXWVNXzSZ4Bfhv42eSiJEeBo+PDXyb5/lqG3oZ2sWqvGnMvVrgXK9yLFb+31i8cEvyZqarjwHGAJItVNb+Z93+5ci9WuBcr3IsV7sWKJItr/dohT+k8AeydON4zPnfRNUl2Aq8FnlrrUJKk2RsS/NPAviTXJrkCOAwsrFqzAPzl+PJfAN+qqprdmJKk9Zr6lM74Ofk7gZPADuCzVXUmyT3AYlUtAP8KfCHJEvBzRj8Upjm+jrm3G/dihXuxwr1Y4V6sWPNexAfiktSD77SVpCYMviQ1seHB92MZVgzYiw8kOZvk0STfTPLGrZhzM0zbi4l170hSSbbtn+QN2Ysk7xx/b5xJ8sXNnnGzDPg/8oYkDyR5ZPz/5LatmHOjJflskicv9V6ljHxyvE+PJrlx0A1X1Yb9Y/Qi738AvwNcAXwP2L9qzV8BnxpfPgx8eSNn2qp/A/fiT4HfHF9+X+e9GK+7CngQOAXMb/XcW/h9sQ94BPit8fHrtnruLdyL48D7xpf3Az/a6rk3aC/+BLgR+P4lrr8N+AYQ4GbgoSG3u9GP8P1YhhVT96KqHqiqZ8eHpxi952E7GvJ9AfAxRp/L9IvNHG6TDdmL9wLHquppgKp6cpNn3CxD9qKA14wvvxb4ySbOt2mq6kFGf/F4KYeAz9fIKeDqJK+fdrsbHfyLfSzD7kutqarngRc+lmG7GbIXk+5g9BN8O5q6F+NfUfdW1dc3c7AtMOT74jrguiTfSXIqyYFNm25zDdmLjwK3J7kAnADevzmjXXZeak+ATf5oBQ2T5HZgHnjrVs+yFZK8AvgE8O4tHuVysZPR0zq3MPqt78Ekv19V/7WVQ22RI8Dnquofk/wxo/f/vLmq/merB3s52OhH+H4sw4ohe0GStwMfBg5W1S83abbNNm0vrgLeDHw7yY8YPUe5sE1fuB3yfXEBWKiqX1XVD4EfMPoBsN0M2Ys7gPsBquq7wKsYfbBaN4N6stpGB9+PZVgxdS+S3AB8mlHst+vztDBlL6rqmaraVVXXVNU1jF7POFhVa/7QqMvYkP8jX2P06J4kuxg9xXNuE2fcLEP24sfA2wCSvIlR8Jc3dcrLwwLwrvFf69wMPFNVP532RRv6lE5t3McyvOwM3It7gVcDXxm/bv3jqjq4ZUNvkIF70cLAvTgJ/HmSs8B/Ax+qqm33W/DAvfgg8Jkkf8voBdx3b8cHiEm+xOiH/K7x6xUfAV4JUFWfYvT6xW3AEvAs8J5Bt7sN90qSdBG+01aSmjD4ktSEwZekJgy+JDVh8CWpCYMvSU0YfElq4n8BzPZculjwdYoAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "import warnings\n",
    "warnings.warn = warn\n",
    "\n",
    "name = \"./Files_Majority_vote/128_features/filter\"\n",
    "#name = \"e:/$Notebooks/Original_files_128_features/filter\"\n",
    "#name = \"G:/Darshana/Scada/Cistel/Marzia/Practical/Feature Selection/Multi Files/output files/Multifilter\"\n",
    "i = 0\n",
    "j = 1\n",
    "\n",
    "for i in range (1,16):\n",
    "    file_name = name+str(i)+\".csv\"\n",
    "    print(file_name)\n",
    "    df_file = pd.read_csv(file_name)\n",
    "    enc = LabelEncoder()\n",
    "    df_file[\"marker\"] = enc.fit_transform(df_file[\"marker\"])\n",
    "    df1_file = df_file.replace([np.inf, -np.inf], np.nan)\n",
    "    m1 = df1_file.dropna()\n",
    "    X= m1.iloc[:,0:14]\n",
    "    y1= m1.iloc[:,15]\n",
    "    label_encoder = LabelEncoder()\n",
    "    y = label_encoder.fit_transform(y1)\n",
    "    #y1 = to_categorical(y)\n",
    "    sc = StandardScaler()\n",
    "    X = sc.fit_transform(X)\n",
    "\n",
    "    clf1 = GradientBoostingClassifier(n_estimators= 5, max_leaf_nodes= None, max_depth=12, \n",
    "                                  random_state= 3, min_samples_split= 12, learning_rate=0.1)\n",
    "    clf2 = RandomForestClassifier(n_estimators=10,bootstrap=True, class_weight=None, \n",
    "                                  criterion='gini', max_depth=12, max_features='auto', max_leaf_nodes=None, \n",
    "                                  min_impurity_decrease=0.0, min_samples_leaf=1, \n",
    "                                  min_samples_split=2, min_weight_fraction_leaf=0.0, n_jobs=None, \n",
    "                                  oob_score=False, random_state=3, verbose=0, warm_start=False) \n",
    "    clf3 = XGBClassifier(n_estimators=10, max_leaf_nodes= None, max_depth=12, random_state= 3, \n",
    "                                  min_samples_split= 1)\n",
    "    eclf = VotingClassifier(estimators=[('gb', clf1), ('rf', clf2), ('xgb', clf3)],voting='hard')\n",
    "    \n",
    "    mean_fpr = np.linspace(0, 1, 100)\n",
    "    tprs = []\n",
    "    aucs = []\n",
    "    fig, ax = plt.subplots()\n",
    "    \n",
    "    print ('n_estimators = 100 \\n')\n",
    "    print('10-fold cross validation for file : ', i)\n",
    "    df_results = pd.DataFrame() \n",
    "    for clf, label in zip([clf1, clf2, clf3, eclf], ['Gradient Boosting', 'Random Forest', 'XGBClassifier', 'Ensemble Majority']):\n",
    "        seed = 123\n",
    "        kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=seed)\n",
    "        cvscores = []\n",
    "        for train, test in kfold.split(X,y):\n",
    "            history = clf.fit(X[train],y1[train])\n",
    "            scoresTrain = clf.score(X[train], y1[train]) \n",
    "            scores = clf.score(X[test], y1[test])\n",
    "            print(clf)\n",
    "#            visual_roc = skplt.metrics.plot_roc(label, X[test], y1[test])\n",
    "#\n",
    "#            interp_tpr = interp(mean_fpr, visual_roc.fpr, visual_roc.tpr)\n",
    "#            interp_tpr[0] = 0.0\n",
    "#            tprs.append(interp_tpr)\n",
    "#            aucs.append(viz.roc_auc)\n",
    "            \n",
    "            \n",
    "            print(\"kfold \",i,\" score: %.2f%%\" % ( scores*100))\n",
    "            print(\"kfold \",i,\" score: %.2f%%\" % (scoresTrain*100)) \n",
    "            cvscores.append(scores * 100)\n",
    "        df_results[label] = cvscores\n",
    "        print(\"Average score over k-fold %.2f%% (+/-%.2f%%) [%s]\" % (np.mean(cvscores), np.std(cvscores), label))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import auc\n",
    "\n",
    "ax.plot([0, 1], [0, 1], linestyle='--', lw=2, color='r',\n",
    "        label='Chance', alpha=.8)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Singleton array nan cannot be considered a valid collection.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_29829/2225150182.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mmean_tpr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtprs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m#mean_tpr[-1] = 1.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mmean_auc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mauc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmean_fpr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmean_tpr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mstd_auc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maucs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m ax.plot(mean_fpr, mean_tpr, color='b',\n",
      "\u001b[0;32m~/DL/lib/python3.8/site-packages/sklearn/metrics/_ranking.py\u001b[0m in \u001b[0;36mauc\u001b[0;34m(x, y)\u001b[0m\n\u001b[1;32m     82\u001b[0m     \u001b[0;36m0.75\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m     \"\"\"\n\u001b[0;32m---> 84\u001b[0;31m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcolumn_or_1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcolumn_or_1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/DL/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    326\u001b[0m     \"\"\"\n\u001b[1;32m    327\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m     \u001b[0mlengths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0m_num_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mX\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mX\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    329\u001b[0m     \u001b[0muniques\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/DL/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    326\u001b[0m     \"\"\"\n\u001b[1;32m    327\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m     \u001b[0mlengths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0m_num_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mX\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mX\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    329\u001b[0m     \u001b[0muniques\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/DL/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m_num_samples\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    266\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"shape\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 268\u001b[0;31m             raise TypeError(\n\u001b[0m\u001b[1;32m    269\u001b[0m                 \u001b[0;34m\"Singleton array %r cannot be considered a valid collection.\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m             )\n",
      "\u001b[0;31mTypeError\u001b[0m: Singleton array nan cannot be considered a valid collection."
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import auc\n",
    "\n",
    "ax.plot([0, 1], [0, 1], linestyle='--', lw=2, color='r',\n",
    "        label='Chance', alpha=.8)\n",
    "\n",
    "mean_tpr = np.mean(tprs, axis=0)\n",
    "#mean_tpr[-1] = 1.0\n",
    "mean_auc = auc(mean_fpr, mean_tpr)\n",
    "std_auc = np.std(aucs)\n",
    "ax.plot(mean_fpr, mean_tpr, color='b',\n",
    "        label=r'Mean ROC (AUC = %0.2f $\\pm$ %0.2f)' % (mean_auc, std_auc),\n",
    "        lw=2, alpha=.8)\n",
    "\n",
    "std_tpr = np.std(tprs, axis=0)\n",
    "tprs_upper = np.minimum(mean_tpr + std_tpr, 1)\n",
    "tprs_lower = np.maximum(mean_tpr - std_tpr, 0)\n",
    "ax.fill_between(mean_fpr, tprs_lower, tprs_upper, color='grey', alpha=.2,\n",
    "                label=r'$\\pm$ 1 std. dev.')\n",
    "\n",
    "ax.set(xlim=[-0.05, 1.05], ylim=[-0.05, 1.05],\n",
    "       title=\"Receiver operating characteristic example\")\n",
    "ax.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "est = [('gb', clf1), ('rf', clf2), ('xgb', clf3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21:54:37] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:54:37] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[0.85587751 0.86358731 0.15017779 ... 0.08969123 0.83536863 0.17202078]\n",
      "(1319, 2) (1319,)\n",
      "Hard Voting Score  0.9302501895375285\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state= 0)\n",
    "\n",
    "vot_hard = VotingClassifier(estimators = est, voting ='soft') \n",
    "vot_hard.fit(X_train, y_train) \n",
    "y_pred = vot_hard.predict(X_test) \n",
    "y_proba = vot_hard.predict_proba(X_test)\n",
    "print(y_proba[:,1])\n",
    "print(y_proba.shape, y_pred.shape)\n",
    "score = accuracy_score(y_test, y_pred) \n",
    "print(\"Hard Voting Score \", score) \n",
    "\n",
    "ns_fpr, ns_tpr, _ = roc_curve(y_test, y_proba[:,1])\n",
    "plt.plot(ns_fpr, ns_tpr, linestyle='--', label='Soft Vote')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Gradient Boosting</th>\n",
       "      <th>Random Forest</th>\n",
       "      <th>XGBClassifier</th>\n",
       "      <th>Ensemble Majority</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>89.015152</td>\n",
       "      <td>95.265152</td>\n",
       "      <td>94.696970</td>\n",
       "      <td>94.696970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>90.151515</td>\n",
       "      <td>95.833333</td>\n",
       "      <td>95.265152</td>\n",
       "      <td>96.022727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>86.931818</td>\n",
       "      <td>92.424242</td>\n",
       "      <td>92.424242</td>\n",
       "      <td>91.856061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>92.992424</td>\n",
       "      <td>94.507576</td>\n",
       "      <td>94.886364</td>\n",
       "      <td>96.212121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>91.098485</td>\n",
       "      <td>93.560606</td>\n",
       "      <td>93.939394</td>\n",
       "      <td>94.696970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>87.878788</td>\n",
       "      <td>91.856061</td>\n",
       "      <td>92.992424</td>\n",
       "      <td>93.181818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>91.461101</td>\n",
       "      <td>92.789374</td>\n",
       "      <td>94.876660</td>\n",
       "      <td>94.497154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>89.563567</td>\n",
       "      <td>92.789374</td>\n",
       "      <td>93.358634</td>\n",
       "      <td>93.358634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>86.337761</td>\n",
       "      <td>92.409867</td>\n",
       "      <td>92.220114</td>\n",
       "      <td>92.220114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>91.271347</td>\n",
       "      <td>92.599620</td>\n",
       "      <td>93.168880</td>\n",
       "      <td>93.927894</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Gradient Boosting  Random Forest  XGBClassifier  Ensemble Majority\n",
       "0          89.015152      95.265152      94.696970          94.696970\n",
       "1          90.151515      95.833333      95.265152          96.022727\n",
       "2          86.931818      92.424242      92.424242          91.856061\n",
       "3          92.992424      94.507576      94.886364          96.212121\n",
       "4          91.098485      93.560606      93.939394          94.696970\n",
       "5          87.878788      91.856061      92.992424          93.181818\n",
       "6          91.461101      92.789374      94.876660          94.497154\n",
       "7          89.563567      92.789374      93.358634          93.358634\n",
       "8          86.337761      92.409867      92.220114          92.220114\n",
       "9          91.271347      92.599620      93.168880          93.927894"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Gradient Boosting    89.670196\n",
       "Random Forest        93.403520\n",
       "XGBClassifier        93.782883\n",
       "Ensemble Majority    94.067046\n",
       "dtype: float64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_results.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#EOF"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL",
   "language": "python",
   "name": "dl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
