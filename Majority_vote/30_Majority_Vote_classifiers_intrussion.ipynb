{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Majority vote with 3 classifiers\n",
    "# Paper based on this approach\n",
    "#  doi=10.1109/TNSE.2021.3099371"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/manero/DL/lib/python3.8/site-packages/xgboost/compat.py:36: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import MultiIndex, Int64Index\n",
      "2023-02-17 21:42:22.095123: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-02-17 21:42:22.095145: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "import scikitplot as skplt\n",
    "from xgboost import XGBClassifier\n",
    "from xgboost import plot_importance\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "%matplotlib inline \n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.utils.multiclass import unique_labels\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils import to_categorical\n",
    "from keras import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "import pandas as pd\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./Files_Majority_vote/128_features/filter1.csv\n",
      "n_estimators = 100 \n",
      "\n",
      "10-fold cross validation for file :  1\n",
      "GradientBoostingClassifier(max_depth=12, min_samples_split=12, n_estimators=5,\n",
      "                           random_state=3)\n",
      "kfold  1  score: 83.10%\n",
      "kfold  1  score: 85.59%\n",
      "GradientBoostingClassifier(max_depth=12, min_samples_split=12, n_estimators=5,\n",
      "                           random_state=3)\n",
      "kfold  1  score: 86.32%\n",
      "kfold  1  score: 88.48%\n",
      "GradientBoostingClassifier(max_depth=12, min_samples_split=12, n_estimators=5,\n",
      "                           random_state=3)\n",
      "kfold  1  score: 83.10%\n",
      "kfold  1  score: 85.68%\n",
      "GradientBoostingClassifier(max_depth=12, min_samples_split=12, n_estimators=5,\n",
      "                           random_state=3)\n",
      "kfold  1  score: 88.93%\n",
      "kfold  1  score: 88.39%\n",
      "GradientBoostingClassifier(max_depth=12, min_samples_split=12, n_estimators=5,\n",
      "                           random_state=3)\n",
      "kfold  1  score: 88.73%\n",
      "kfold  1  score: 91.54%\n",
      "GradientBoostingClassifier(max_depth=12, min_samples_split=12, n_estimators=5,\n",
      "                           random_state=3)\n",
      "kfold  1  score: 90.34%\n",
      "kfold  1  score: 94.07%\n",
      "GradientBoostingClassifier(max_depth=12, min_samples_split=12, n_estimators=5,\n",
      "                           random_state=3)\n",
      "kfold  1  score: 86.29%\n",
      "kfold  1  score: 87.32%\n",
      "GradientBoostingClassifier(max_depth=12, min_samples_split=12, n_estimators=5,\n",
      "                           random_state=3)\n",
      "kfold  1  score: 84.88%\n",
      "kfold  1  score: 88.52%\n",
      "GradientBoostingClassifier(max_depth=12, min_samples_split=12, n_estimators=5,\n",
      "                           random_state=3)\n",
      "kfold  1  score: 85.48%\n",
      "kfold  1  score: 88.43%\n",
      "GradientBoostingClassifier(max_depth=12, min_samples_split=12, n_estimators=5,\n",
      "                           random_state=3)\n",
      "kfold  1  score: 84.27%\n",
      "kfold  1  score: 86.13%\n",
      "Average score over k-fold 86.15% (+/-2.37%) [Gradient Boosting]\n",
      "RandomForestClassifier(max_depth=12, n_estimators=10, random_state=3)\n",
      "kfold  1  score: 96.58%\n",
      "kfold  1  score: 99.60%\n",
      "RandomForestClassifier(max_depth=12, n_estimators=10, random_state=3)\n",
      "kfold  1  score: 95.77%\n",
      "kfold  1  score: 99.53%\n",
      "RandomForestClassifier(max_depth=12, n_estimators=10, random_state=3)\n",
      "kfold  1  score: 96.58%\n",
      "kfold  1  score: 99.49%\n",
      "RandomForestClassifier(max_depth=12, n_estimators=10, random_state=3)\n",
      "kfold  1  score: 96.18%\n",
      "kfold  1  score: 99.46%\n",
      "RandomForestClassifier(max_depth=12, n_estimators=10, random_state=3)\n",
      "kfold  1  score: 95.17%\n",
      "kfold  1  score: 99.44%\n",
      "RandomForestClassifier(max_depth=12, n_estimators=10, random_state=3)\n",
      "kfold  1  score: 93.56%\n",
      "kfold  1  score: 99.37%\n",
      "RandomForestClassifier(max_depth=12, n_estimators=10, random_state=3)\n",
      "kfold  1  score: 96.17%\n",
      "kfold  1  score: 99.57%\n",
      "RandomForestClassifier(max_depth=12, n_estimators=10, random_state=3)\n",
      "kfold  1  score: 96.37%\n",
      "kfold  1  score: 99.55%\n",
      "RandomForestClassifier(max_depth=12, n_estimators=10, random_state=3)\n",
      "kfold  1  score: 96.77%\n",
      "kfold  1  score: 99.40%\n",
      "RandomForestClassifier(max_depth=12, n_estimators=10, random_state=3)\n",
      "kfold  1  score: 95.77%\n",
      "kfold  1  score: 99.44%\n",
      "Average score over k-fold 95.89% (+/-0.90%) [Random Forest]\n",
      "[21:46:36] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:46:36] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
      "              gamma=0, gpu_id=-1, importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.300000012,\n",
      "              max_delta_step=0, max_depth=12, max_leaf_nodes=None,\n",
      "              min_child_weight=1, min_samples_split=1, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=10, n_jobs=8,\n",
      "              num_parallel_tree=1, predictor='auto', random_state=3,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)\n",
      "kfold  1  score: 94.97%\n",
      "kfold  1  score: 99.22%\n",
      "[21:46:36] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:46:36] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
      "              gamma=0, gpu_id=-1, importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.300000012,\n",
      "              max_delta_step=0, max_depth=12, max_leaf_nodes=None,\n",
      "              min_child_weight=1, min_samples_split=1, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=10, n_jobs=8,\n",
      "              num_parallel_tree=1, predictor='auto', random_state=3,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)\n",
      "kfold  1  score: 93.76%\n",
      "kfold  1  score: 98.95%\n",
      "[21:46:36] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:46:36] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
      "              gamma=0, gpu_id=-1, importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.300000012,\n",
      "              max_delta_step=0, max_depth=12, max_leaf_nodes=None,\n",
      "              min_child_weight=1, min_samples_split=1, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=10, n_jobs=8,\n",
      "              num_parallel_tree=1, predictor='auto', random_state=3,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)\n",
      "kfold  1  score: 95.57%\n",
      "kfold  1  score: 99.28%\n",
      "[21:46:37] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:46:37] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
      "              gamma=0, gpu_id=-1, importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.300000012,\n",
      "              max_delta_step=0, max_depth=12, max_leaf_nodes=None,\n",
      "              min_child_weight=1, min_samples_split=1, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=10, n_jobs=8,\n",
      "              num_parallel_tree=1, predictor='auto', random_state=3,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)\n",
      "kfold  1  score: 94.77%\n",
      "kfold  1  score: 99.28%\n",
      "[21:46:37] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:46:37] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
      "              gamma=0, gpu_id=-1, importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.300000012,\n",
      "              max_delta_step=0, max_depth=12, max_leaf_nodes=None,\n",
      "              min_child_weight=1, min_samples_split=1, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=10, n_jobs=8,\n",
      "              num_parallel_tree=1, predictor='auto', random_state=3,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)\n",
      "kfold  1  score: 93.36%\n",
      "kfold  1  score: 99.13%\n",
      "[21:46:37] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:46:37] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
      "              gamma=0, gpu_id=-1, importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.300000012,\n",
      "              max_delta_step=0, max_depth=12, max_leaf_nodes=None,\n",
      "              min_child_weight=1, min_samples_split=1, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=10, n_jobs=8,\n",
      "              num_parallel_tree=1, predictor='auto', random_state=3,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)\n",
      "kfold  1  score: 93.96%\n",
      "kfold  1  score: 99.26%\n",
      "[21:46:37] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:46:37] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
      "              gamma=0, gpu_id=-1, importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.300000012,\n",
      "              max_delta_step=0, max_depth=12, max_leaf_nodes=None,\n",
      "              min_child_weight=1, min_samples_split=1, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=10, n_jobs=8,\n",
      "              num_parallel_tree=1, predictor='auto', random_state=3,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)\n",
      "kfold  1  score: 95.97%\n",
      "kfold  1  score: 98.88%\n",
      "[21:46:37] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:46:37] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
      "              gamma=0, gpu_id=-1, importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.300000012,\n",
      "              max_delta_step=0, max_depth=12, max_leaf_nodes=None,\n",
      "              min_child_weight=1, min_samples_split=1, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=10, n_jobs=8,\n",
      "              num_parallel_tree=1, predictor='auto', random_state=3,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)\n",
      "kfold  1  score: 96.57%\n",
      "kfold  1  score: 99.08%\n",
      "[21:46:37] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:46:37] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
      "              gamma=0, gpu_id=-1, importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.300000012,\n",
      "              max_delta_step=0, max_depth=12, max_leaf_nodes=None,\n",
      "              min_child_weight=1, min_samples_split=1, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=10, n_jobs=8,\n",
      "              num_parallel_tree=1, predictor='auto', random_state=3,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)\n",
      "kfold  1  score: 95.36%\n",
      "kfold  1  score: 98.90%\n",
      "[21:46:37] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:46:37] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
      "              gamma=0, gpu_id=-1, importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.300000012,\n",
      "              max_delta_step=0, max_depth=12, max_leaf_nodes=None,\n",
      "              min_child_weight=1, min_samples_split=1, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=10, n_jobs=8,\n",
      "              num_parallel_tree=1, predictor='auto', random_state=3,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)\n",
      "kfold  1  score: 95.36%\n",
      "kfold  1  score: 98.95%\n",
      "Average score over k-fold 94.97% (+/-0.97%) [XGBClassifier]\n",
      "[21:46:38] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:46:38] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "VotingClassifier(estimators=[('gb',\n",
      "                              GradientBoostingClassifier(max_depth=12,\n",
      "                                                         min_samples_split=12,\n",
      "                                                         n_estimators=5,\n",
      "                                                         random_state=3)),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(max_depth=12,\n",
      "                                                     n_estimators=10,\n",
      "                                                     random_state=3)),\n",
      "                             ('xgb',\n",
      "                              XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "                                            colsample_bylevel=1,\n",
      "                                            colsample_bynode=1,\n",
      "                                            colsample_bytree=1,\n",
      "                                            enable_categorical=False, gamma=0,\n",
      "                                            gpu...\n",
      "                                            interaction_constraints='',\n",
      "                                            learning_rate=0.300000012,\n",
      "                                            max_delta_step=0, max_depth=12,\n",
      "                                            max_leaf_nodes=None,\n",
      "                                            min_child_weight=1,\n",
      "                                            min_samples_split=1, missing=nan,\n",
      "                                            monotone_constraints='()',\n",
      "                                            n_estimators=10, n_jobs=8,\n",
      "                                            num_parallel_tree=1,\n",
      "                                            predictor='auto', random_state=3,\n",
      "                                            reg_alpha=0, reg_lambda=1,\n",
      "                                            scale_pos_weight=1, subsample=1,\n",
      "                                            tree_method='exact',\n",
      "                                            validate_parameters=1,\n",
      "                                            verbosity=None))])\n",
      "kfold  1  score: 95.98%\n",
      "kfold  1  score: 99.31%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21:46:38] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:46:38] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "VotingClassifier(estimators=[('gb',\n",
      "                              GradientBoostingClassifier(max_depth=12,\n",
      "                                                         min_samples_split=12,\n",
      "                                                         n_estimators=5,\n",
      "                                                         random_state=3)),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(max_depth=12,\n",
      "                                                     n_estimators=10,\n",
      "                                                     random_state=3)),\n",
      "                             ('xgb',\n",
      "                              XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "                                            colsample_bylevel=1,\n",
      "                                            colsample_bynode=1,\n",
      "                                            colsample_bytree=1,\n",
      "                                            enable_categorical=False, gamma=0,\n",
      "                                            gpu...\n",
      "                                            interaction_constraints='',\n",
      "                                            learning_rate=0.300000012,\n",
      "                                            max_delta_step=0, max_depth=12,\n",
      "                                            max_leaf_nodes=None,\n",
      "                                            min_child_weight=1,\n",
      "                                            min_samples_split=1, missing=nan,\n",
      "                                            monotone_constraints='()',\n",
      "                                            n_estimators=10, n_jobs=8,\n",
      "                                            num_parallel_tree=1,\n",
      "                                            predictor='auto', random_state=3,\n",
      "                                            reg_alpha=0, reg_lambda=1,\n",
      "                                            scale_pos_weight=1, subsample=1,\n",
      "                                            tree_method='exact',\n",
      "                                            validate_parameters=1,\n",
      "                                            verbosity=None))])\n",
      "kfold  1  score: 94.77%\n",
      "kfold  1  score: 99.02%\n",
      "[21:46:39] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:46:39] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "VotingClassifier(estimators=[('gb',\n",
      "                              GradientBoostingClassifier(max_depth=12,\n",
      "                                                         min_samples_split=12,\n",
      "                                                         n_estimators=5,\n",
      "                                                         random_state=3)),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(max_depth=12,\n",
      "                                                     n_estimators=10,\n",
      "                                                     random_state=3)),\n",
      "                             ('xgb',\n",
      "                              XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "                                            colsample_bylevel=1,\n",
      "                                            colsample_bynode=1,\n",
      "                                            colsample_bytree=1,\n",
      "                                            enable_categorical=False, gamma=0,\n",
      "                                            gpu...\n",
      "                                            interaction_constraints='',\n",
      "                                            learning_rate=0.300000012,\n",
      "                                            max_delta_step=0, max_depth=12,\n",
      "                                            max_leaf_nodes=None,\n",
      "                                            min_child_weight=1,\n",
      "                                            min_samples_split=1, missing=nan,\n",
      "                                            monotone_constraints='()',\n",
      "                                            n_estimators=10, n_jobs=8,\n",
      "                                            num_parallel_tree=1,\n",
      "                                            predictor='auto', random_state=3,\n",
      "                                            reg_alpha=0, reg_lambda=1,\n",
      "                                            scale_pos_weight=1, subsample=1,\n",
      "                                            tree_method='exact',\n",
      "                                            validate_parameters=1,\n",
      "                                            verbosity=None))])\n",
      "kfold  1  score: 96.18%\n",
      "kfold  1  score: 99.24%\n",
      "[21:46:39] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:46:39] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "VotingClassifier(estimators=[('gb',\n",
      "                              GradientBoostingClassifier(max_depth=12,\n",
      "                                                         min_samples_split=12,\n",
      "                                                         n_estimators=5,\n",
      "                                                         random_state=3)),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(max_depth=12,\n",
      "                                                     n_estimators=10,\n",
      "                                                     random_state=3)),\n",
      "                             ('xgb',\n",
      "                              XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "                                            colsample_bylevel=1,\n",
      "                                            colsample_bynode=1,\n",
      "                                            colsample_bytree=1,\n",
      "                                            enable_categorical=False, gamma=0,\n",
      "                                            gpu...\n",
      "                                            interaction_constraints='',\n",
      "                                            learning_rate=0.300000012,\n",
      "                                            max_delta_step=0, max_depth=12,\n",
      "                                            max_leaf_nodes=None,\n",
      "                                            min_child_weight=1,\n",
      "                                            min_samples_split=1, missing=nan,\n",
      "                                            monotone_constraints='()',\n",
      "                                            n_estimators=10, n_jobs=8,\n",
      "                                            num_parallel_tree=1,\n",
      "                                            predictor='auto', random_state=3,\n",
      "                                            reg_alpha=0, reg_lambda=1,\n",
      "                                            scale_pos_weight=1, subsample=1,\n",
      "                                            tree_method='exact',\n",
      "                                            validate_parameters=1,\n",
      "                                            verbosity=None))])\n",
      "kfold  1  score: 95.57%\n",
      "kfold  1  score: 99.31%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21:46:40] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:46:40] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "VotingClassifier(estimators=[('gb',\n",
      "                              GradientBoostingClassifier(max_depth=12,\n",
      "                                                         min_samples_split=12,\n",
      "                                                         n_estimators=5,\n",
      "                                                         random_state=3)),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(max_depth=12,\n",
      "                                                     n_estimators=10,\n",
      "                                                     random_state=3)),\n",
      "                             ('xgb',\n",
      "                              XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "                                            colsample_bylevel=1,\n",
      "                                            colsample_bynode=1,\n",
      "                                            colsample_bytree=1,\n",
      "                                            enable_categorical=False, gamma=0,\n",
      "                                            gpu...\n",
      "                                            interaction_constraints='',\n",
      "                                            learning_rate=0.300000012,\n",
      "                                            max_delta_step=0, max_depth=12,\n",
      "                                            max_leaf_nodes=None,\n",
      "                                            min_child_weight=1,\n",
      "                                            min_samples_split=1, missing=nan,\n",
      "                                            monotone_constraints='()',\n",
      "                                            n_estimators=10, n_jobs=8,\n",
      "                                            num_parallel_tree=1,\n",
      "                                            predictor='auto', random_state=3,\n",
      "                                            reg_alpha=0, reg_lambda=1,\n",
      "                                            scale_pos_weight=1, subsample=1,\n",
      "                                            tree_method='exact',\n",
      "                                            validate_parameters=1,\n",
      "                                            verbosity=None))])\n",
      "kfold  1  score: 93.76%\n",
      "kfold  1  score: 98.99%\n",
      "[21:46:40] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:46:40] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "VotingClassifier(estimators=[('gb',\n",
      "                              GradientBoostingClassifier(max_depth=12,\n",
      "                                                         min_samples_split=12,\n",
      "                                                         n_estimators=5,\n",
      "                                                         random_state=3)),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(max_depth=12,\n",
      "                                                     n_estimators=10,\n",
      "                                                     random_state=3)),\n",
      "                             ('xgb',\n",
      "                              XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "                                            colsample_bylevel=1,\n",
      "                                            colsample_bynode=1,\n",
      "                                            colsample_bytree=1,\n",
      "                                            enable_categorical=False, gamma=0,\n",
      "                                            gpu...\n",
      "                                            interaction_constraints='',\n",
      "                                            learning_rate=0.300000012,\n",
      "                                            max_delta_step=0, max_depth=12,\n",
      "                                            max_leaf_nodes=None,\n",
      "                                            min_child_weight=1,\n",
      "                                            min_samples_split=1, missing=nan,\n",
      "                                            monotone_constraints='()',\n",
      "                                            n_estimators=10, n_jobs=8,\n",
      "                                            num_parallel_tree=1,\n",
      "                                            predictor='auto', random_state=3,\n",
      "                                            reg_alpha=0, reg_lambda=1,\n",
      "                                            scale_pos_weight=1, subsample=1,\n",
      "                                            tree_method='exact',\n",
      "                                            validate_parameters=1,\n",
      "                                            verbosity=None))])\n",
      "kfold  1  score: 94.16%\n",
      "kfold  1  score: 99.46%\n",
      "[21:46:40] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:46:40] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "VotingClassifier(estimators=[('gb',\n",
      "                              GradientBoostingClassifier(max_depth=12,\n",
      "                                                         min_samples_split=12,\n",
      "                                                         n_estimators=5,\n",
      "                                                         random_state=3)),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(max_depth=12,\n",
      "                                                     n_estimators=10,\n",
      "                                                     random_state=3)),\n",
      "                             ('xgb',\n",
      "                              XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "                                            colsample_bylevel=1,\n",
      "                                            colsample_bynode=1,\n",
      "                                            colsample_bytree=1,\n",
      "                                            enable_categorical=False, gamma=0,\n",
      "                                            gpu...\n",
      "                                            interaction_constraints='',\n",
      "                                            learning_rate=0.300000012,\n",
      "                                            max_delta_step=0, max_depth=12,\n",
      "                                            max_leaf_nodes=None,\n",
      "                                            min_child_weight=1,\n",
      "                                            min_samples_split=1, missing=nan,\n",
      "                                            monotone_constraints='()',\n",
      "                                            n_estimators=10, n_jobs=8,\n",
      "                                            num_parallel_tree=1,\n",
      "                                            predictor='auto', random_state=3,\n",
      "                                            reg_alpha=0, reg_lambda=1,\n",
      "                                            scale_pos_weight=1, subsample=1,\n",
      "                                            tree_method='exact',\n",
      "                                            validate_parameters=1,\n",
      "                                            verbosity=None))])\n",
      "kfold  1  score: 96.98%\n",
      "kfold  1  score: 98.97%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21:46:41] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:46:41] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "VotingClassifier(estimators=[('gb',\n",
      "                              GradientBoostingClassifier(max_depth=12,\n",
      "                                                         min_samples_split=12,\n",
      "                                                         n_estimators=5,\n",
      "                                                         random_state=3)),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(max_depth=12,\n",
      "                                                     n_estimators=10,\n",
      "                                                     random_state=3)),\n",
      "                             ('xgb',\n",
      "                              XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "                                            colsample_bylevel=1,\n",
      "                                            colsample_bynode=1,\n",
      "                                            colsample_bytree=1,\n",
      "                                            enable_categorical=False, gamma=0,\n",
      "                                            gpu...\n",
      "                                            interaction_constraints='',\n",
      "                                            learning_rate=0.300000012,\n",
      "                                            max_delta_step=0, max_depth=12,\n",
      "                                            max_leaf_nodes=None,\n",
      "                                            min_child_weight=1,\n",
      "                                            min_samples_split=1, missing=nan,\n",
      "                                            monotone_constraints='()',\n",
      "                                            n_estimators=10, n_jobs=8,\n",
      "                                            num_parallel_tree=1,\n",
      "                                            predictor='auto', random_state=3,\n",
      "                                            reg_alpha=0, reg_lambda=1,\n",
      "                                            scale_pos_weight=1, subsample=1,\n",
      "                                            tree_method='exact',\n",
      "                                            validate_parameters=1,\n",
      "                                            verbosity=None))])\n",
      "kfold  1  score: 95.77%\n",
      "kfold  1  score: 99.08%\n",
      "[21:46:41] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:46:41] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "VotingClassifier(estimators=[('gb',\n",
      "                              GradientBoostingClassifier(max_depth=12,\n",
      "                                                         min_samples_split=12,\n",
      "                                                         n_estimators=5,\n",
      "                                                         random_state=3)),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(max_depth=12,\n",
      "                                                     n_estimators=10,\n",
      "                                                     random_state=3)),\n",
      "                             ('xgb',\n",
      "                              XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "                                            colsample_bylevel=1,\n",
      "                                            colsample_bynode=1,\n",
      "                                            colsample_bytree=1,\n",
      "                                            enable_categorical=False, gamma=0,\n",
      "                                            gpu...\n",
      "                                            interaction_constraints='',\n",
      "                                            learning_rate=0.300000012,\n",
      "                                            max_delta_step=0, max_depth=12,\n",
      "                                            max_leaf_nodes=None,\n",
      "                                            min_child_weight=1,\n",
      "                                            min_samples_split=1, missing=nan,\n",
      "                                            monotone_constraints='()',\n",
      "                                            n_estimators=10, n_jobs=8,\n",
      "                                            num_parallel_tree=1,\n",
      "                                            predictor='auto', random_state=3,\n",
      "                                            reg_alpha=0, reg_lambda=1,\n",
      "                                            scale_pos_weight=1, subsample=1,\n",
      "                                            tree_method='exact',\n",
      "                                            validate_parameters=1,\n",
      "                                            verbosity=None))])\n",
      "kfold  1  score: 95.36%\n",
      "kfold  1  score: 98.79%\n",
      "[21:46:42] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:46:42] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "VotingClassifier(estimators=[('gb',\n",
      "                              GradientBoostingClassifier(max_depth=12,\n",
      "                                                         min_samples_split=12,\n",
      "                                                         n_estimators=5,\n",
      "                                                         random_state=3)),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(max_depth=12,\n",
      "                                                     n_estimators=10,\n",
      "                                                     random_state=3)),\n",
      "                             ('xgb',\n",
      "                              XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "                                            colsample_bylevel=1,\n",
      "                                            colsample_bynode=1,\n",
      "                                            colsample_bytree=1,\n",
      "                                            enable_categorical=False, gamma=0,\n",
      "                                            gpu...\n",
      "                                            interaction_constraints='',\n",
      "                                            learning_rate=0.300000012,\n",
      "                                            max_delta_step=0, max_depth=12,\n",
      "                                            max_leaf_nodes=None,\n",
      "                                            min_child_weight=1,\n",
      "                                            min_samples_split=1, missing=nan,\n",
      "                                            monotone_constraints='()',\n",
      "                                            n_estimators=10, n_jobs=8,\n",
      "                                            num_parallel_tree=1,\n",
      "                                            predictor='auto', random_state=3,\n",
      "                                            reg_alpha=0, reg_lambda=1,\n",
      "                                            scale_pos_weight=1, subsample=1,\n",
      "                                            tree_method='exact',\n",
      "                                            validate_parameters=1,\n",
      "                                            verbosity=None))])\n",
      "kfold  1  score: 95.16%\n",
      "kfold  1  score: 99.06%\n",
      "Average score over k-fold 95.37% (+/-0.91%) [Ensemble Majority]\n",
      "./Files_Majority_vote/128_features/filter2.csv\n",
      "n_estimators = 100 \n",
      "\n",
      "10-fold cross validation for file :  2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GradientBoostingClassifier(max_depth=12, min_samples_split=12, n_estimators=5,\n",
      "                           random_state=3)\n",
      "kfold  2  score: 89.74%\n",
      "kfold  2  score: 95.57%\n",
      "GradientBoostingClassifier(max_depth=12, min_samples_split=12, n_estimators=5,\n",
      "                           random_state=3)\n",
      "kfold  2  score: 93.89%\n",
      "kfold  2  score: 94.65%\n",
      "GradientBoostingClassifier(max_depth=12, min_samples_split=12, n_estimators=5,\n",
      "                           random_state=3)\n",
      "kfold  2  score: 90.93%\n",
      "kfold  2  score: 96.10%\n",
      "GradientBoostingClassifier(max_depth=12, min_samples_split=12, n_estimators=5,\n",
      "                           random_state=3)\n",
      "kfold  2  score: 91.91%\n",
      "kfold  2  score: 95.40%\n",
      "GradientBoostingClassifier(max_depth=12, min_samples_split=12, n_estimators=5,\n",
      "                           random_state=3)\n",
      "kfold  2  score: 90.14%\n",
      "kfold  2  score: 95.84%\n",
      "GradientBoostingClassifier(max_depth=12, min_samples_split=12, n_estimators=5,\n",
      "                           random_state=3)\n",
      "kfold  2  score: 90.93%\n",
      "kfold  2  score: 94.74%\n",
      "GradientBoostingClassifier(max_depth=12, min_samples_split=12, n_estimators=5,\n",
      "                           random_state=3)\n",
      "kfold  2  score: 92.50%\n",
      "kfold  2  score: 95.42%\n",
      "GradientBoostingClassifier(max_depth=12, min_samples_split=12, n_estimators=5,\n",
      "                           random_state=3)\n",
      "kfold  2  score: 89.55%\n",
      "kfold  2  score: 95.27%\n",
      "GradientBoostingClassifier(max_depth=12, min_samples_split=12, n_estimators=5,\n",
      "                           random_state=3)\n",
      "kfold  2  score: 92.11%\n",
      "kfold  2  score: 97.00%\n",
      "GradientBoostingClassifier(max_depth=12, min_samples_split=12, n_estimators=5,\n",
      "                           random_state=3)\n",
      "kfold  2  score: 89.92%\n",
      "kfold  2  score: 96.23%\n",
      "Average score over k-fold 91.16% (+/-1.34%) [Gradient Boosting]\n",
      "RandomForestClassifier(max_depth=12, n_estimators=10, random_state=3)\n",
      "kfold  2  score: 94.08%\n",
      "kfold  2  score: 98.93%\n",
      "RandomForestClassifier(max_depth=12, n_estimators=10, random_state=3)\n",
      "kfold  2  score: 96.65%\n",
      "kfold  2  score: 99.04%\n",
      "RandomForestClassifier(max_depth=12, n_estimators=10, random_state=3)\n",
      "kfold  2  score: 92.70%\n",
      "kfold  2  score: 99.21%\n",
      "RandomForestClassifier(max_depth=12, n_estimators=10, random_state=3)\n",
      "kfold  2  score: 93.69%\n",
      "kfold  2  score: 98.99%\n",
      "RandomForestClassifier(max_depth=12, n_estimators=10, random_state=3)\n",
      "kfold  2  score: 92.90%\n",
      "kfold  2  score: 98.73%\n",
      "RandomForestClassifier(max_depth=12, n_estimators=10, random_state=3)\n",
      "kfold  2  score: 93.10%\n",
      "kfold  2  score: 99.12%\n",
      "RandomForestClassifier(max_depth=12, n_estimators=10, random_state=3)\n",
      "kfold  2  score: 93.89%\n",
      "kfold  2  score: 98.64%\n",
      "RandomForestClassifier(max_depth=12, n_estimators=10, random_state=3)\n",
      "kfold  2  score: 95.07%\n",
      "kfold  2  score: 99.04%\n",
      "RandomForestClassifier(max_depth=12, n_estimators=10, random_state=3)\n",
      "kfold  2  score: 93.10%\n",
      "kfold  2  score: 99.17%\n",
      "RandomForestClassifier(max_depth=12, n_estimators=10, random_state=3)\n",
      "kfold  2  score: 91.90%\n",
      "kfold  2  score: 98.84%\n",
      "Average score over k-fold 93.71% (+/-1.28%) [Random Forest]\n",
      "[21:46:45] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:46:45] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
      "              gamma=0, gpu_id=-1, importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.300000012,\n",
      "              max_delta_step=0, max_depth=12, max_leaf_nodes=None,\n",
      "              min_child_weight=1, min_samples_split=1, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=10, n_jobs=8,\n",
      "              num_parallel_tree=1, predictor='auto', random_state=3,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)\n",
      "kfold  2  score: 93.89%\n",
      "kfold  2  score: 99.19%\n",
      "[21:46:45] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:46:45] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
      "              gamma=0, gpu_id=-1, importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.300000012,\n",
      "              max_delta_step=0, max_depth=12, max_leaf_nodes=None,\n",
      "              min_child_weight=1, min_samples_split=1, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=10, n_jobs=8,\n",
      "              num_parallel_tree=1, predictor='auto', random_state=3,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)\n",
      "kfold  2  score: 96.06%\n",
      "kfold  2  score: 99.17%\n",
      "[21:46:45] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:46:45] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
      "              gamma=0, gpu_id=-1, importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.300000012,\n",
      "              max_delta_step=0, max_depth=12, max_leaf_nodes=None,\n",
      "              min_child_weight=1, min_samples_split=1, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=10, n_jobs=8,\n",
      "              num_parallel_tree=1, predictor='auto', random_state=3,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)\n",
      "kfold  2  score: 92.31%\n",
      "kfold  2  score: 99.25%\n",
      "[21:46:45] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:46:45] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
      "              gamma=0, gpu_id=-1, importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.300000012,\n",
      "              max_delta_step=0, max_depth=12, max_leaf_nodes=None,\n",
      "              min_child_weight=1, min_samples_split=1, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=10, n_jobs=8,\n",
      "              num_parallel_tree=1, predictor='auto', random_state=3,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)\n",
      "kfold  2  score: 92.90%\n",
      "kfold  2  score: 99.32%\n",
      "[21:46:45] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:46:45] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
      "              gamma=0, gpu_id=-1, importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.300000012,\n",
      "              max_delta_step=0, max_depth=12, max_leaf_nodes=None,\n",
      "              min_child_weight=1, min_samples_split=1, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=10, n_jobs=8,\n",
      "              num_parallel_tree=1, predictor='auto', random_state=3,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)\n",
      "kfold  2  score: 92.31%\n",
      "kfold  2  score: 99.01%\n",
      "[21:46:45] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:46:45] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
      "              gamma=0, gpu_id=-1, importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.300000012,\n",
      "              max_delta_step=0, max_depth=12, max_leaf_nodes=None,\n",
      "              min_child_weight=1, min_samples_split=1, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=10, n_jobs=8,\n",
      "              num_parallel_tree=1, predictor='auto', random_state=3,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)\n",
      "kfold  2  score: 94.48%\n",
      "kfold  2  score: 99.08%\n",
      "[21:46:46] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:46:46] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
      "              gamma=0, gpu_id=-1, importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.300000012,\n",
      "              max_delta_step=0, max_depth=12, max_leaf_nodes=None,\n",
      "              min_child_weight=1, min_samples_split=1, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=10, n_jobs=8,\n",
      "              num_parallel_tree=1, predictor='auto', random_state=3,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)\n",
      "kfold  2  score: 93.29%\n",
      "kfold  2  score: 99.04%\n",
      "[21:46:46] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:46:46] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
      "              gamma=0, gpu_id=-1, importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.300000012,\n",
      "              max_delta_step=0, max_depth=12, max_leaf_nodes=None,\n",
      "              min_child_weight=1, min_samples_split=1, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=10, n_jobs=8,\n",
      "              num_parallel_tree=1, predictor='auto', random_state=3,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)\n",
      "kfold  2  score: 93.69%\n",
      "kfold  2  score: 99.19%\n",
      "[21:46:46] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:46:46] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
      "              gamma=0, gpu_id=-1, importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.300000012,\n",
      "              max_delta_step=0, max_depth=12, max_leaf_nodes=None,\n",
      "              min_child_weight=1, min_samples_split=1, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=10, n_jobs=8,\n",
      "              num_parallel_tree=1, predictor='auto', random_state=3,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)\n",
      "kfold  2  score: 95.46%\n",
      "kfold  2  score: 99.47%\n",
      "[21:46:46] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:46:46] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
      "              gamma=0, gpu_id=-1, importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.300000012,\n",
      "              max_delta_step=0, max_depth=12, max_leaf_nodes=None,\n",
      "              min_child_weight=1, min_samples_split=1, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=10, n_jobs=8,\n",
      "              num_parallel_tree=1, predictor='auto', random_state=3,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)\n",
      "kfold  2  score: 92.09%\n",
      "kfold  2  score: 99.25%\n",
      "Average score over k-fold 93.65% (+/-1.29%) [XGBClassifier]\n",
      "[21:46:47] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:46:47] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "VotingClassifier(estimators=[('gb',\n",
      "                              GradientBoostingClassifier(max_depth=12,\n",
      "                                                         min_samples_split=12,\n",
      "                                                         n_estimators=5,\n",
      "                                                         random_state=3)),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(max_depth=12,\n",
      "                                                     n_estimators=10,\n",
      "                                                     random_state=3)),\n",
      "                             ('xgb',\n",
      "                              XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "                                            colsample_bylevel=1,\n",
      "                                            colsample_bynode=1,\n",
      "                                            colsample_bytree=1,\n",
      "                                            enable_categorical=False, gamma=0,\n",
      "                                            gpu...\n",
      "                                            interaction_constraints='',\n",
      "                                            learning_rate=0.300000012,\n",
      "                                            max_delta_step=0, max_depth=12,\n",
      "                                            max_leaf_nodes=None,\n",
      "                                            min_child_weight=1,\n",
      "                                            min_samples_split=1, missing=nan,\n",
      "                                            monotone_constraints='()',\n",
      "                                            n_estimators=10, n_jobs=8,\n",
      "                                            num_parallel_tree=1,\n",
      "                                            predictor='auto', random_state=3,\n",
      "                                            reg_alpha=0, reg_lambda=1,\n",
      "                                            scale_pos_weight=1, subsample=1,\n",
      "                                            tree_method='exact',\n",
      "                                            validate_parameters=1,\n",
      "                                            verbosity=None))])\n",
      "kfold  2  score: 93.69%\n",
      "kfold  2  score: 99.01%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21:46:47] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:46:47] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "VotingClassifier(estimators=[('gb',\n",
      "                              GradientBoostingClassifier(max_depth=12,\n",
      "                                                         min_samples_split=12,\n",
      "                                                         n_estimators=5,\n",
      "                                                         random_state=3)),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(max_depth=12,\n",
      "                                                     n_estimators=10,\n",
      "                                                     random_state=3)),\n",
      "                             ('xgb',\n",
      "                              XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "                                            colsample_bylevel=1,\n",
      "                                            colsample_bynode=1,\n",
      "                                            colsample_bytree=1,\n",
      "                                            enable_categorical=False, gamma=0,\n",
      "                                            gpu...\n",
      "                                            interaction_constraints='',\n",
      "                                            learning_rate=0.300000012,\n",
      "                                            max_delta_step=0, max_depth=12,\n",
      "                                            max_leaf_nodes=None,\n",
      "                                            min_child_weight=1,\n",
      "                                            min_samples_split=1, missing=nan,\n",
      "                                            monotone_constraints='()',\n",
      "                                            n_estimators=10, n_jobs=8,\n",
      "                                            num_parallel_tree=1,\n",
      "                                            predictor='auto', random_state=3,\n",
      "                                            reg_alpha=0, reg_lambda=1,\n",
      "                                            scale_pos_weight=1, subsample=1,\n",
      "                                            tree_method='exact',\n",
      "                                            validate_parameters=1,\n",
      "                                            verbosity=None))])\n",
      "kfold  2  score: 96.65%\n",
      "kfold  2  score: 98.86%\n",
      "[21:46:48] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:46:48] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "VotingClassifier(estimators=[('gb',\n",
      "                              GradientBoostingClassifier(max_depth=12,\n",
      "                                                         min_samples_split=12,\n",
      "                                                         n_estimators=5,\n",
      "                                                         random_state=3)),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(max_depth=12,\n",
      "                                                     n_estimators=10,\n",
      "                                                     random_state=3)),\n",
      "                             ('xgb',\n",
      "                              XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "                                            colsample_bylevel=1,\n",
      "                                            colsample_bynode=1,\n",
      "                                            colsample_bytree=1,\n",
      "                                            enable_categorical=False, gamma=0,\n",
      "                                            gpu...\n",
      "                                            interaction_constraints='',\n",
      "                                            learning_rate=0.300000012,\n",
      "                                            max_delta_step=0, max_depth=12,\n",
      "                                            max_leaf_nodes=None,\n",
      "                                            min_child_weight=1,\n",
      "                                            min_samples_split=1, missing=nan,\n",
      "                                            monotone_constraints='()',\n",
      "                                            n_estimators=10, n_jobs=8,\n",
      "                                            num_parallel_tree=1,\n",
      "                                            predictor='auto', random_state=3,\n",
      "                                            reg_alpha=0, reg_lambda=1,\n",
      "                                            scale_pos_weight=1, subsample=1,\n",
      "                                            tree_method='exact',\n",
      "                                            validate_parameters=1,\n",
      "                                            verbosity=None))])\n",
      "kfold  2  score: 92.50%\n",
      "kfold  2  score: 99.28%\n",
      "[21:46:48] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:46:48] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "VotingClassifier(estimators=[('gb',\n",
      "                              GradientBoostingClassifier(max_depth=12,\n",
      "                                                         min_samples_split=12,\n",
      "                                                         n_estimators=5,\n",
      "                                                         random_state=3)),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(max_depth=12,\n",
      "                                                     n_estimators=10,\n",
      "                                                     random_state=3)),\n",
      "                             ('xgb',\n",
      "                              XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "                                            colsample_bylevel=1,\n",
      "                                            colsample_bynode=1,\n",
      "                                            colsample_bytree=1,\n",
      "                                            enable_categorical=False, gamma=0,\n",
      "                                            gpu...\n",
      "                                            interaction_constraints='',\n",
      "                                            learning_rate=0.300000012,\n",
      "                                            max_delta_step=0, max_depth=12,\n",
      "                                            max_leaf_nodes=None,\n",
      "                                            min_child_weight=1,\n",
      "                                            min_samples_split=1, missing=nan,\n",
      "                                            monotone_constraints='()',\n",
      "                                            n_estimators=10, n_jobs=8,\n",
      "                                            num_parallel_tree=1,\n",
      "                                            predictor='auto', random_state=3,\n",
      "                                            reg_alpha=0, reg_lambda=1,\n",
      "                                            scale_pos_weight=1, subsample=1,\n",
      "                                            tree_method='exact',\n",
      "                                            validate_parameters=1,\n",
      "                                            verbosity=None))])\n",
      "kfold  2  score: 93.29%\n",
      "kfold  2  score: 98.97%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21:46:49] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:46:49] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "VotingClassifier(estimators=[('gb',\n",
      "                              GradientBoostingClassifier(max_depth=12,\n",
      "                                                         min_samples_split=12,\n",
      "                                                         n_estimators=5,\n",
      "                                                         random_state=3)),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(max_depth=12,\n",
      "                                                     n_estimators=10,\n",
      "                                                     random_state=3)),\n",
      "                             ('xgb',\n",
      "                              XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "                                            colsample_bylevel=1,\n",
      "                                            colsample_bynode=1,\n",
      "                                            colsample_bytree=1,\n",
      "                                            enable_categorical=False, gamma=0,\n",
      "                                            gpu...\n",
      "                                            interaction_constraints='',\n",
      "                                            learning_rate=0.300000012,\n",
      "                                            max_delta_step=0, max_depth=12,\n",
      "                                            max_leaf_nodes=None,\n",
      "                                            min_child_weight=1,\n",
      "                                            min_samples_split=1, missing=nan,\n",
      "                                            monotone_constraints='()',\n",
      "                                            n_estimators=10, n_jobs=8,\n",
      "                                            num_parallel_tree=1,\n",
      "                                            predictor='auto', random_state=3,\n",
      "                                            reg_alpha=0, reg_lambda=1,\n",
      "                                            scale_pos_weight=1, subsample=1,\n",
      "                                            tree_method='exact',\n",
      "                                            validate_parameters=1,\n",
      "                                            verbosity=None))])\n",
      "kfold  2  score: 92.90%\n",
      "kfold  2  score: 98.73%\n",
      "[21:46:49] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:46:49] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "VotingClassifier(estimators=[('gb',\n",
      "                              GradientBoostingClassifier(max_depth=12,\n",
      "                                                         min_samples_split=12,\n",
      "                                                         n_estimators=5,\n",
      "                                                         random_state=3)),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(max_depth=12,\n",
      "                                                     n_estimators=10,\n",
      "                                                     random_state=3)),\n",
      "                             ('xgb',\n",
      "                              XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "                                            colsample_bylevel=1,\n",
      "                                            colsample_bynode=1,\n",
      "                                            colsample_bytree=1,\n",
      "                                            enable_categorical=False, gamma=0,\n",
      "                                            gpu...\n",
      "                                            interaction_constraints='',\n",
      "                                            learning_rate=0.300000012,\n",
      "                                            max_delta_step=0, max_depth=12,\n",
      "                                            max_leaf_nodes=None,\n",
      "                                            min_child_weight=1,\n",
      "                                            min_samples_split=1, missing=nan,\n",
      "                                            monotone_constraints='()',\n",
      "                                            n_estimators=10, n_jobs=8,\n",
      "                                            num_parallel_tree=1,\n",
      "                                            predictor='auto', random_state=3,\n",
      "                                            reg_alpha=0, reg_lambda=1,\n",
      "                                            scale_pos_weight=1, subsample=1,\n",
      "                                            tree_method='exact',\n",
      "                                            validate_parameters=1,\n",
      "                                            verbosity=None))])\n",
      "kfold  2  score: 94.48%\n",
      "kfold  2  score: 98.79%\n",
      "[21:46:50] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:46:50] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "VotingClassifier(estimators=[('gb',\n",
      "                              GradientBoostingClassifier(max_depth=12,\n",
      "                                                         min_samples_split=12,\n",
      "                                                         n_estimators=5,\n",
      "                                                         random_state=3)),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(max_depth=12,\n",
      "                                                     n_estimators=10,\n",
      "                                                     random_state=3)),\n",
      "                             ('xgb',\n",
      "                              XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "                                            colsample_bylevel=1,\n",
      "                                            colsample_bynode=1,\n",
      "                                            colsample_bytree=1,\n",
      "                                            enable_categorical=False, gamma=0,\n",
      "                                            gpu...\n",
      "                                            interaction_constraints='',\n",
      "                                            learning_rate=0.300000012,\n",
      "                                            max_delta_step=0, max_depth=12,\n",
      "                                            max_leaf_nodes=None,\n",
      "                                            min_child_weight=1,\n",
      "                                            min_samples_split=1, missing=nan,\n",
      "                                            monotone_constraints='()',\n",
      "                                            n_estimators=10, n_jobs=8,\n",
      "                                            num_parallel_tree=1,\n",
      "                                            predictor='auto', random_state=3,\n",
      "                                            reg_alpha=0, reg_lambda=1,\n",
      "                                            scale_pos_weight=1, subsample=1,\n",
      "                                            tree_method='exact',\n",
      "                                            validate_parameters=1,\n",
      "                                            verbosity=None))])\n",
      "kfold  2  score: 93.29%\n",
      "kfold  2  score: 98.68%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21:46:50] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:46:50] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "VotingClassifier(estimators=[('gb',\n",
      "                              GradientBoostingClassifier(max_depth=12,\n",
      "                                                         min_samples_split=12,\n",
      "                                                         n_estimators=5,\n",
      "                                                         random_state=3)),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(max_depth=12,\n",
      "                                                     n_estimators=10,\n",
      "                                                     random_state=3)),\n",
      "                             ('xgb',\n",
      "                              XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "                                            colsample_bylevel=1,\n",
      "                                            colsample_bynode=1,\n",
      "                                            colsample_bytree=1,\n",
      "                                            enable_categorical=False, gamma=0,\n",
      "                                            gpu...\n",
      "                                            interaction_constraints='',\n",
      "                                            learning_rate=0.300000012,\n",
      "                                            max_delta_step=0, max_depth=12,\n",
      "                                            max_leaf_nodes=None,\n",
      "                                            min_child_weight=1,\n",
      "                                            min_samples_split=1, missing=nan,\n",
      "                                            monotone_constraints='()',\n",
      "                                            n_estimators=10, n_jobs=8,\n",
      "                                            num_parallel_tree=1,\n",
      "                                            predictor='auto', random_state=3,\n",
      "                                            reg_alpha=0, reg_lambda=1,\n",
      "                                            scale_pos_weight=1, subsample=1,\n",
      "                                            tree_method='exact',\n",
      "                                            validate_parameters=1,\n",
      "                                            verbosity=None))])\n",
      "kfold  2  score: 94.08%\n",
      "kfold  2  score: 98.95%\n",
      "[21:46:51] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:46:51] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "VotingClassifier(estimators=[('gb',\n",
      "                              GradientBoostingClassifier(max_depth=12,\n",
      "                                                         min_samples_split=12,\n",
      "                                                         n_estimators=5,\n",
      "                                                         random_state=3)),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(max_depth=12,\n",
      "                                                     n_estimators=10,\n",
      "                                                     random_state=3)),\n",
      "                             ('xgb',\n",
      "                              XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "                                            colsample_bylevel=1,\n",
      "                                            colsample_bynode=1,\n",
      "                                            colsample_bytree=1,\n",
      "                                            enable_categorical=False, gamma=0,\n",
      "                                            gpu...\n",
      "                                            interaction_constraints='',\n",
      "                                            learning_rate=0.300000012,\n",
      "                                            max_delta_step=0, max_depth=12,\n",
      "                                            max_leaf_nodes=None,\n",
      "                                            min_child_weight=1,\n",
      "                                            min_samples_split=1, missing=nan,\n",
      "                                            monotone_constraints='()',\n",
      "                                            n_estimators=10, n_jobs=8,\n",
      "                                            num_parallel_tree=1,\n",
      "                                            predictor='auto', random_state=3,\n",
      "                                            reg_alpha=0, reg_lambda=1,\n",
      "                                            scale_pos_weight=1, subsample=1,\n",
      "                                            tree_method='exact',\n",
      "                                            validate_parameters=1,\n",
      "                                            verbosity=None))])\n",
      "kfold  2  score: 94.28%\n",
      "kfold  2  score: 99.21%\n",
      "[21:46:51] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:46:51] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "VotingClassifier(estimators=[('gb',\n",
      "                              GradientBoostingClassifier(max_depth=12,\n",
      "                                                         min_samples_split=12,\n",
      "                                                         n_estimators=5,\n",
      "                                                         random_state=3)),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(max_depth=12,\n",
      "                                                     n_estimators=10,\n",
      "                                                     random_state=3)),\n",
      "                             ('xgb',\n",
      "                              XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "                                            colsample_bylevel=1,\n",
      "                                            colsample_bynode=1,\n",
      "                                            colsample_bytree=1,\n",
      "                                            enable_categorical=False, gamma=0,\n",
      "                                            gpu...\n",
      "                                            interaction_constraints='',\n",
      "                                            learning_rate=0.300000012,\n",
      "                                            max_delta_step=0, max_depth=12,\n",
      "                                            max_leaf_nodes=None,\n",
      "                                            min_child_weight=1,\n",
      "                                            min_samples_split=1, missing=nan,\n",
      "                                            monotone_constraints='()',\n",
      "                                            n_estimators=10, n_jobs=8,\n",
      "                                            num_parallel_tree=1,\n",
      "                                            predictor='auto', random_state=3,\n",
      "                                            reg_alpha=0, reg_lambda=1,\n",
      "                                            scale_pos_weight=1, subsample=1,\n",
      "                                            tree_method='exact',\n",
      "                                            validate_parameters=1,\n",
      "                                            verbosity=None))])\n",
      "kfold  2  score: 92.49%\n",
      "kfold  2  score: 98.84%\n",
      "Average score over k-fold 93.77% (+/-1.17%) [Ensemble Majority]\n",
      "./Files_Majority_vote/128_features/filter3.csv\n",
      "n_estimators = 100 \n",
      "\n",
      "10-fold cross validation for file :  3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GradientBoostingClassifier(max_depth=12, min_samples_split=12, n_estimators=5,\n",
      "                           random_state=3)\n",
      "kfold  3  score: 90.41%\n",
      "kfold  3  score: 93.10%\n",
      "GradientBoostingClassifier(max_depth=12, min_samples_split=12, n_estimators=5,\n",
      "                           random_state=3)\n",
      "kfold  3  score: 88.38%\n",
      "kfold  3  score: 92.49%\n",
      "GradientBoostingClassifier(max_depth=12, min_samples_split=12, n_estimators=5,\n",
      "                           random_state=3)\n",
      "kfold  3  score: 90.77%\n",
      "kfold  3  score: 92.57%\n",
      "GradientBoostingClassifier(max_depth=12, min_samples_split=12, n_estimators=5,\n",
      "                           random_state=3)\n",
      "kfold  3  score: 88.19%\n",
      "kfold  3  score: 92.55%\n",
      "GradientBoostingClassifier(max_depth=12, min_samples_split=12, n_estimators=5,\n",
      "                           random_state=3)\n",
      "kfold  3  score: 89.11%\n",
      "kfold  3  score: 92.45%\n",
      "GradientBoostingClassifier(max_depth=12, min_samples_split=12, n_estimators=5,\n",
      "                           random_state=3)\n",
      "kfold  3  score: 91.68%\n",
      "kfold  3  score: 92.98%\n",
      "GradientBoostingClassifier(max_depth=12, min_samples_split=12, n_estimators=5,\n",
      "                           random_state=3)\n",
      "kfold  3  score: 88.17%\n",
      "kfold  3  score: 93.00%\n",
      "GradientBoostingClassifier(max_depth=12, min_samples_split=12, n_estimators=5,\n",
      "                           random_state=3)\n",
      "kfold  3  score: 89.65%\n",
      "kfold  3  score: 93.07%\n",
      "GradientBoostingClassifier(max_depth=12, min_samples_split=12, n_estimators=5,\n",
      "                           random_state=3)\n",
      "kfold  3  score: 88.35%\n",
      "kfold  3  score: 92.16%\n",
      "GradientBoostingClassifier(max_depth=12, min_samples_split=12, n_estimators=5,\n",
      "                           random_state=3)\n",
      "kfold  3  score: 89.46%\n",
      "kfold  3  score: 91.42%\n",
      "Average score over k-fold 89.42% (+/-1.16%) [Gradient Boosting]\n",
      "RandomForestClassifier(max_depth=12, n_estimators=10, random_state=3)\n",
      "kfold  3  score: 92.62%\n",
      "kfold  3  score: 95.88%\n",
      "RandomForestClassifier(max_depth=12, n_estimators=10, random_state=3)\n",
      "kfold  3  score: 91.51%\n",
      "kfold  3  score: 96.88%\n",
      "RandomForestClassifier(max_depth=12, n_estimators=10, random_state=3)\n",
      "kfold  3  score: 93.54%\n",
      "kfold  3  score: 96.53%\n",
      "RandomForestClassifier(max_depth=12, n_estimators=10, random_state=3)\n",
      "kfold  3  score: 89.85%\n",
      "kfold  3  score: 95.42%\n",
      "RandomForestClassifier(max_depth=12, n_estimators=10, random_state=3)\n",
      "kfold  3  score: 94.28%\n",
      "kfold  3  score: 96.63%\n",
      "RandomForestClassifier(max_depth=12, n_estimators=10, random_state=3)\n",
      "kfold  3  score: 93.90%\n",
      "kfold  3  score: 96.37%\n",
      "RandomForestClassifier(max_depth=12, n_estimators=10, random_state=3)\n",
      "kfold  3  score: 92.05%\n",
      "kfold  3  score: 96.86%\n",
      "RandomForestClassifier(max_depth=12, n_estimators=10, random_state=3)\n",
      "kfold  3  score: 93.16%\n",
      "kfold  3  score: 96.20%\n",
      "RandomForestClassifier(max_depth=12, n_estimators=10, random_state=3)\n",
      "kfold  3  score: 93.16%\n",
      "kfold  3  score: 96.76%\n",
      "RandomForestClassifier(max_depth=12, n_estimators=10, random_state=3)\n",
      "kfold  3  score: 93.53%\n",
      "kfold  3  score: 95.84%\n",
      "Average score over k-fold 92.76% (+/-1.25%) [Random Forest]\n",
      "[21:46:55] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:46:55] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
      "              gamma=0, gpu_id=-1, importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.300000012,\n",
      "              max_delta_step=0, max_depth=12, max_leaf_nodes=None,\n",
      "              min_child_weight=1, min_samples_split=1, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=10, n_jobs=8,\n",
      "              num_parallel_tree=1, predictor='auto', random_state=3,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)\n",
      "kfold  3  score: 95.20%\n",
      "kfold  3  score: 99.49%\n",
      "[21:46:55] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:46:55] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
      "              gamma=0, gpu_id=-1, importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.300000012,\n",
      "              max_delta_step=0, max_depth=12, max_leaf_nodes=None,\n",
      "              min_child_weight=1, min_samples_split=1, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=10, n_jobs=8,\n",
      "              num_parallel_tree=1, predictor='auto', random_state=3,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)\n",
      "kfold  3  score: 95.39%\n",
      "kfold  3  score: 99.55%\n",
      "[21:46:55] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:46:55] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
      "              gamma=0, gpu_id=-1, importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.300000012,\n",
      "              max_delta_step=0, max_depth=12, max_leaf_nodes=None,\n",
      "              min_child_weight=1, min_samples_split=1, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=10, n_jobs=8,\n",
      "              num_parallel_tree=1, predictor='auto', random_state=3,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)\n",
      "kfold  3  score: 95.39%\n",
      "kfold  3  score: 99.51%\n",
      "[21:46:55] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:46:55] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
      "              gamma=0, gpu_id=-1, importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.300000012,\n",
      "              max_delta_step=0, max_depth=12, max_leaf_nodes=None,\n",
      "              min_child_weight=1, min_samples_split=1, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=10, n_jobs=8,\n",
      "              num_parallel_tree=1, predictor='auto', random_state=3,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)\n",
      "kfold  3  score: 92.99%\n",
      "kfold  3  score: 99.22%\n",
      "[21:46:55] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:46:55] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
      "              gamma=0, gpu_id=-1, importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.300000012,\n",
      "              max_delta_step=0, max_depth=12, max_leaf_nodes=None,\n",
      "              min_child_weight=1, min_samples_split=1, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=10, n_jobs=8,\n",
      "              num_parallel_tree=1, predictor='auto', random_state=3,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)\n",
      "kfold  3  score: 95.76%\n",
      "kfold  3  score: 99.53%\n",
      "[21:46:56] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:46:56] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
      "              gamma=0, gpu_id=-1, importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.300000012,\n",
      "              max_delta_step=0, max_depth=12, max_leaf_nodes=None,\n",
      "              min_child_weight=1, min_samples_split=1, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=10, n_jobs=8,\n",
      "              num_parallel_tree=1, predictor='auto', random_state=3,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)\n",
      "kfold  3  score: 96.12%\n",
      "kfold  3  score: 99.38%\n",
      "[21:46:56] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:46:56] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
      "              gamma=0, gpu_id=-1, importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.300000012,\n",
      "              max_delta_step=0, max_depth=12, max_leaf_nodes=None,\n",
      "              min_child_weight=1, min_samples_split=1, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=10, n_jobs=8,\n",
      "              num_parallel_tree=1, predictor='auto', random_state=3,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)\n",
      "kfold  3  score: 94.45%\n",
      "kfold  3  score: 99.55%\n",
      "[21:46:56] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:46:56] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
      "              gamma=0, gpu_id=-1, importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.300000012,\n",
      "              max_delta_step=0, max_depth=12, max_leaf_nodes=None,\n",
      "              min_child_weight=1, min_samples_split=1, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=10, n_jobs=8,\n",
      "              num_parallel_tree=1, predictor='auto', random_state=3,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)\n",
      "kfold  3  score: 95.38%\n",
      "kfold  3  score: 99.41%\n",
      "[21:46:56] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:46:56] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
      "              gamma=0, gpu_id=-1, importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.300000012,\n",
      "              max_delta_step=0, max_depth=12, max_leaf_nodes=None,\n",
      "              min_child_weight=1, min_samples_split=1, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=10, n_jobs=8,\n",
      "              num_parallel_tree=1, predictor='auto', random_state=3,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)\n",
      "kfold  3  score: 95.19%\n",
      "kfold  3  score: 99.06%\n",
      "[21:46:56] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:46:56] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
      "              gamma=0, gpu_id=-1, importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.300000012,\n",
      "              max_delta_step=0, max_depth=12, max_leaf_nodes=None,\n",
      "              min_child_weight=1, min_samples_split=1, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=10, n_jobs=8,\n",
      "              num_parallel_tree=1, predictor='auto', random_state=3,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)\n",
      "kfold  3  score: 96.30%\n",
      "kfold  3  score: 98.87%\n",
      "Average score over k-fold 95.22% (+/-0.89%) [XGBClassifier]\n",
      "[21:46:57] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:46:57] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "VotingClassifier(estimators=[('gb',\n",
      "                              GradientBoostingClassifier(max_depth=12,\n",
      "                                                         min_samples_split=12,\n",
      "                                                         n_estimators=5,\n",
      "                                                         random_state=3)),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(max_depth=12,\n",
      "                                                     n_estimators=10,\n",
      "                                                     random_state=3)),\n",
      "                             ('xgb',\n",
      "                              XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "                                            colsample_bylevel=1,\n",
      "                                            colsample_bynode=1,\n",
      "                                            colsample_bytree=1,\n",
      "                                            enable_categorical=False, gamma=0,\n",
      "                                            gpu...\n",
      "                                            interaction_constraints='',\n",
      "                                            learning_rate=0.300000012,\n",
      "                                            max_delta_step=0, max_depth=12,\n",
      "                                            max_leaf_nodes=None,\n",
      "                                            min_child_weight=1,\n",
      "                                            min_samples_split=1, missing=nan,\n",
      "                                            monotone_constraints='()',\n",
      "                                            n_estimators=10, n_jobs=8,\n",
      "                                            num_parallel_tree=1,\n",
      "                                            predictor='auto', random_state=3,\n",
      "                                            reg_alpha=0, reg_lambda=1,\n",
      "                                            scale_pos_weight=1, subsample=1,\n",
      "                                            tree_method='exact',\n",
      "                                            validate_parameters=1,\n",
      "                                            verbosity=None))])\n",
      "kfold  3  score: 93.17%\n",
      "kfold  3  score: 96.45%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21:46:57] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:46:57] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "VotingClassifier(estimators=[('gb',\n",
      "                              GradientBoostingClassifier(max_depth=12,\n",
      "                                                         min_samples_split=12,\n",
      "                                                         n_estimators=5,\n",
      "                                                         random_state=3)),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(max_depth=12,\n",
      "                                                     n_estimators=10,\n",
      "                                                     random_state=3)),\n",
      "                             ('xgb',\n",
      "                              XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "                                            colsample_bylevel=1,\n",
      "                                            colsample_bynode=1,\n",
      "                                            colsample_bytree=1,\n",
      "                                            enable_categorical=False, gamma=0,\n",
      "                                            gpu...\n",
      "                                            interaction_constraints='',\n",
      "                                            learning_rate=0.300000012,\n",
      "                                            max_delta_step=0, max_depth=12,\n",
      "                                            max_leaf_nodes=None,\n",
      "                                            min_child_weight=1,\n",
      "                                            min_samples_split=1, missing=nan,\n",
      "                                            monotone_constraints='()',\n",
      "                                            n_estimators=10, n_jobs=8,\n",
      "                                            num_parallel_tree=1,\n",
      "                                            predictor='auto', random_state=3,\n",
      "                                            reg_alpha=0, reg_lambda=1,\n",
      "                                            scale_pos_weight=1, subsample=1,\n",
      "                                            tree_method='exact',\n",
      "                                            validate_parameters=1,\n",
      "                                            verbosity=None))])\n",
      "kfold  3  score: 92.99%\n",
      "kfold  3  score: 97.25%\n",
      "[21:46:58] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:46:58] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "VotingClassifier(estimators=[('gb',\n",
      "                              GradientBoostingClassifier(max_depth=12,\n",
      "                                                         min_samples_split=12,\n",
      "                                                         n_estimators=5,\n",
      "                                                         random_state=3)),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(max_depth=12,\n",
      "                                                     n_estimators=10,\n",
      "                                                     random_state=3)),\n",
      "                             ('xgb',\n",
      "                              XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "                                            colsample_bylevel=1,\n",
      "                                            colsample_bynode=1,\n",
      "                                            colsample_bytree=1,\n",
      "                                            enable_categorical=False, gamma=0,\n",
      "                                            gpu...\n",
      "                                            interaction_constraints='',\n",
      "                                            learning_rate=0.300000012,\n",
      "                                            max_delta_step=0, max_depth=12,\n",
      "                                            max_leaf_nodes=None,\n",
      "                                            min_child_weight=1,\n",
      "                                            min_samples_split=1, missing=nan,\n",
      "                                            monotone_constraints='()',\n",
      "                                            n_estimators=10, n_jobs=8,\n",
      "                                            num_parallel_tree=1,\n",
      "                                            predictor='auto', random_state=3,\n",
      "                                            reg_alpha=0, reg_lambda=1,\n",
      "                                            scale_pos_weight=1, subsample=1,\n",
      "                                            tree_method='exact',\n",
      "                                            validate_parameters=1,\n",
      "                                            verbosity=None))])\n",
      "kfold  3  score: 94.46%\n",
      "kfold  3  score: 96.96%\n",
      "[21:46:58] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:46:58] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "VotingClassifier(estimators=[('gb',\n",
      "                              GradientBoostingClassifier(max_depth=12,\n",
      "                                                         min_samples_split=12,\n",
      "                                                         n_estimators=5,\n",
      "                                                         random_state=3)),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(max_depth=12,\n",
      "                                                     n_estimators=10,\n",
      "                                                     random_state=3)),\n",
      "                             ('xgb',\n",
      "                              XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "                                            colsample_bylevel=1,\n",
      "                                            colsample_bynode=1,\n",
      "                                            colsample_bytree=1,\n",
      "                                            enable_categorical=False, gamma=0,\n",
      "                                            gpu...\n",
      "                                            interaction_constraints='',\n",
      "                                            learning_rate=0.300000012,\n",
      "                                            max_delta_step=0, max_depth=12,\n",
      "                                            max_leaf_nodes=None,\n",
      "                                            min_child_weight=1,\n",
      "                                            min_samples_split=1, missing=nan,\n",
      "                                            monotone_constraints='()',\n",
      "                                            n_estimators=10, n_jobs=8,\n",
      "                                            num_parallel_tree=1,\n",
      "                                            predictor='auto', random_state=3,\n",
      "                                            reg_alpha=0, reg_lambda=1,\n",
      "                                            scale_pos_weight=1, subsample=1,\n",
      "                                            tree_method='exact',\n",
      "                                            validate_parameters=1,\n",
      "                                            verbosity=None))])\n",
      "kfold  3  score: 90.96%\n",
      "kfold  3  score: 96.08%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21:46:59] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:46:59] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "VotingClassifier(estimators=[('gb',\n",
      "                              GradientBoostingClassifier(max_depth=12,\n",
      "                                                         min_samples_split=12,\n",
      "                                                         n_estimators=5,\n",
      "                                                         random_state=3)),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(max_depth=12,\n",
      "                                                     n_estimators=10,\n",
      "                                                     random_state=3)),\n",
      "                             ('xgb',\n",
      "                              XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "                                            colsample_bylevel=1,\n",
      "                                            colsample_bynode=1,\n",
      "                                            colsample_bytree=1,\n",
      "                                            enable_categorical=False, gamma=0,\n",
      "                                            gpu...\n",
      "                                            interaction_constraints='',\n",
      "                                            learning_rate=0.300000012,\n",
      "                                            max_delta_step=0, max_depth=12,\n",
      "                                            max_leaf_nodes=None,\n",
      "                                            min_child_weight=1,\n",
      "                                            min_samples_split=1, missing=nan,\n",
      "                                            monotone_constraints='()',\n",
      "                                            n_estimators=10, n_jobs=8,\n",
      "                                            num_parallel_tree=1,\n",
      "                                            predictor='auto', random_state=3,\n",
      "                                            reg_alpha=0, reg_lambda=1,\n",
      "                                            scale_pos_weight=1, subsample=1,\n",
      "                                            tree_method='exact',\n",
      "                                            validate_parameters=1,\n",
      "                                            verbosity=None))])\n",
      "kfold  3  score: 93.73%\n",
      "kfold  3  score: 97.11%\n",
      "[21:46:59] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:46:59] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "VotingClassifier(estimators=[('gb',\n",
      "                              GradientBoostingClassifier(max_depth=12,\n",
      "                                                         min_samples_split=12,\n",
      "                                                         n_estimators=5,\n",
      "                                                         random_state=3)),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(max_depth=12,\n",
      "                                                     n_estimators=10,\n",
      "                                                     random_state=3)),\n",
      "                             ('xgb',\n",
      "                              XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "                                            colsample_bylevel=1,\n",
      "                                            colsample_bynode=1,\n",
      "                                            colsample_bytree=1,\n",
      "                                            enable_categorical=False, gamma=0,\n",
      "                                            gpu...\n",
      "                                            interaction_constraints='',\n",
      "                                            learning_rate=0.300000012,\n",
      "                                            max_delta_step=0, max_depth=12,\n",
      "                                            max_leaf_nodes=None,\n",
      "                                            min_child_weight=1,\n",
      "                                            min_samples_split=1, missing=nan,\n",
      "                                            monotone_constraints='()',\n",
      "                                            n_estimators=10, n_jobs=8,\n",
      "                                            num_parallel_tree=1,\n",
      "                                            predictor='auto', random_state=3,\n",
      "                                            reg_alpha=0, reg_lambda=1,\n",
      "                                            scale_pos_weight=1, subsample=1,\n",
      "                                            tree_method='exact',\n",
      "                                            validate_parameters=1,\n",
      "                                            verbosity=None))])\n",
      "kfold  3  score: 93.90%\n",
      "kfold  3  score: 96.53%\n",
      "[21:47:00] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:47:00] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "VotingClassifier(estimators=[('gb',\n",
      "                              GradientBoostingClassifier(max_depth=12,\n",
      "                                                         min_samples_split=12,\n",
      "                                                         n_estimators=5,\n",
      "                                                         random_state=3)),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(max_depth=12,\n",
      "                                                     n_estimators=10,\n",
      "                                                     random_state=3)),\n",
      "                             ('xgb',\n",
      "                              XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "                                            colsample_bylevel=1,\n",
      "                                            colsample_bynode=1,\n",
      "                                            colsample_bytree=1,\n",
      "                                            enable_categorical=False, gamma=0,\n",
      "                                            gpu...\n",
      "                                            interaction_constraints='',\n",
      "                                            learning_rate=0.300000012,\n",
      "                                            max_delta_step=0, max_depth=12,\n",
      "                                            max_leaf_nodes=None,\n",
      "                                            min_child_weight=1,\n",
      "                                            min_samples_split=1, missing=nan,\n",
      "                                            monotone_constraints='()',\n",
      "                                            n_estimators=10, n_jobs=8,\n",
      "                                            num_parallel_tree=1,\n",
      "                                            predictor='auto', random_state=3,\n",
      "                                            reg_alpha=0, reg_lambda=1,\n",
      "                                            scale_pos_weight=1, subsample=1,\n",
      "                                            tree_method='exact',\n",
      "                                            validate_parameters=1,\n",
      "                                            verbosity=None))])\n",
      "kfold  3  score: 92.79%\n",
      "kfold  3  score: 97.39%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21:47:00] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:47:00] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "VotingClassifier(estimators=[('gb',\n",
      "                              GradientBoostingClassifier(max_depth=12,\n",
      "                                                         min_samples_split=12,\n",
      "                                                         n_estimators=5,\n",
      "                                                         random_state=3)),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(max_depth=12,\n",
      "                                                     n_estimators=10,\n",
      "                                                     random_state=3)),\n",
      "                             ('xgb',\n",
      "                              XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "                                            colsample_bylevel=1,\n",
      "                                            colsample_bynode=1,\n",
      "                                            colsample_bytree=1,\n",
      "                                            enable_categorical=False, gamma=0,\n",
      "                                            gpu...\n",
      "                                            interaction_constraints='',\n",
      "                                            learning_rate=0.300000012,\n",
      "                                            max_delta_step=0, max_depth=12,\n",
      "                                            max_leaf_nodes=None,\n",
      "                                            min_child_weight=1,\n",
      "                                            min_samples_split=1, missing=nan,\n",
      "                                            monotone_constraints='()',\n",
      "                                            n_estimators=10, n_jobs=8,\n",
      "                                            num_parallel_tree=1,\n",
      "                                            predictor='auto', random_state=3,\n",
      "                                            reg_alpha=0, reg_lambda=1,\n",
      "                                            scale_pos_weight=1, subsample=1,\n",
      "                                            tree_method='exact',\n",
      "                                            validate_parameters=1,\n",
      "                                            verbosity=None))])\n",
      "kfold  3  score: 93.53%\n",
      "kfold  3  score: 97.17%\n",
      "[21:47:01] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:47:01] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "VotingClassifier(estimators=[('gb',\n",
      "                              GradientBoostingClassifier(max_depth=12,\n",
      "                                                         min_samples_split=12,\n",
      "                                                         n_estimators=5,\n",
      "                                                         random_state=3)),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(max_depth=12,\n",
      "                                                     n_estimators=10,\n",
      "                                                     random_state=3)),\n",
      "                             ('xgb',\n",
      "                              XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "                                            colsample_bylevel=1,\n",
      "                                            colsample_bynode=1,\n",
      "                                            colsample_bytree=1,\n",
      "                                            enable_categorical=False, gamma=0,\n",
      "                                            gpu...\n",
      "                                            interaction_constraints='',\n",
      "                                            learning_rate=0.300000012,\n",
      "                                            max_delta_step=0, max_depth=12,\n",
      "                                            max_leaf_nodes=None,\n",
      "                                            min_child_weight=1,\n",
      "                                            min_samples_split=1, missing=nan,\n",
      "                                            monotone_constraints='()',\n",
      "                                            n_estimators=10, n_jobs=8,\n",
      "                                            num_parallel_tree=1,\n",
      "                                            predictor='auto', random_state=3,\n",
      "                                            reg_alpha=0, reg_lambda=1,\n",
      "                                            scale_pos_weight=1, subsample=1,\n",
      "                                            tree_method='exact',\n",
      "                                            validate_parameters=1,\n",
      "                                            verbosity=None))])\n",
      "kfold  3  score: 93.90%\n",
      "kfold  3  score: 97.56%\n",
      "[21:47:01] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:47:01] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "VotingClassifier(estimators=[('gb',\n",
      "                              GradientBoostingClassifier(max_depth=12,\n",
      "                                                         min_samples_split=12,\n",
      "                                                         n_estimators=5,\n",
      "                                                         random_state=3)),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(max_depth=12,\n",
      "                                                     n_estimators=10,\n",
      "                                                     random_state=3)),\n",
      "                             ('xgb',\n",
      "                              XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "                                            colsample_bylevel=1,\n",
      "                                            colsample_bynode=1,\n",
      "                                            colsample_bytree=1,\n",
      "                                            enable_categorical=False, gamma=0,\n",
      "                                            gpu...\n",
      "                                            interaction_constraints='',\n",
      "                                            learning_rate=0.300000012,\n",
      "                                            max_delta_step=0, max_depth=12,\n",
      "                                            max_leaf_nodes=None,\n",
      "                                            min_child_weight=1,\n",
      "                                            min_samples_split=1, missing=nan,\n",
      "                                            monotone_constraints='()',\n",
      "                                            n_estimators=10, n_jobs=8,\n",
      "                                            num_parallel_tree=1,\n",
      "                                            predictor='auto', random_state=3,\n",
      "                                            reg_alpha=0, reg_lambda=1,\n",
      "                                            scale_pos_weight=1, subsample=1,\n",
      "                                            tree_method='exact',\n",
      "                                            validate_parameters=1,\n",
      "                                            verbosity=None))])\n",
      "kfold  3  score: 94.64%\n",
      "kfold  3  score: 96.29%\n",
      "Average score over k-fold 93.41% (+/-0.99%) [Ensemble Majority]\n",
      "./Files_Majority_vote/128_features/filter4.csv\n",
      "n_estimators = 100 \n",
      "\n",
      "10-fold cross validation for file :  4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GradientBoostingClassifier(max_depth=12, min_samples_split=12, n_estimators=5,\n",
      "                           random_state=3)\n",
      "kfold  4  score: 85.80%\n",
      "kfold  4  score: 92.03%\n",
      "GradientBoostingClassifier(max_depth=12, min_samples_split=12, n_estimators=5,\n",
      "                           random_state=3)\n",
      "kfold  4  score: 94.24%\n",
      "kfold  4  score: 97.35%\n",
      "GradientBoostingClassifier(max_depth=12, min_samples_split=12, n_estimators=5,\n",
      "                           random_state=3)\n",
      "kfold  4  score: 91.73%\n",
      "kfold  4  score: 97.69%\n",
      "GradientBoostingClassifier(max_depth=12, min_samples_split=12, n_estimators=5,\n",
      "                           random_state=3)\n",
      "kfold  4  score: 93.08%\n",
      "kfold  4  score: 97.27%\n",
      "GradientBoostingClassifier(max_depth=12, min_samples_split=12, n_estimators=5,\n",
      "                           random_state=3)\n",
      "kfold  4  score: 93.46%\n",
      "kfold  4  score: 97.63%\n",
      "GradientBoostingClassifier(max_depth=12, min_samples_split=12, n_estimators=5,\n",
      "                           random_state=3)\n",
      "kfold  4  score: 91.92%\n",
      "kfold  4  score: 97.69%\n",
      "GradientBoostingClassifier(max_depth=12, min_samples_split=12, n_estimators=5,\n",
      "                           random_state=3)\n",
      "kfold  4  score: 91.54%\n",
      "kfold  4  score: 96.90%\n",
      "GradientBoostingClassifier(max_depth=12, min_samples_split=12, n_estimators=5,\n",
      "                           random_state=3)\n",
      "kfold  4  score: 91.54%\n",
      "kfold  4  score: 97.63%\n",
      "GradientBoostingClassifier(max_depth=12, min_samples_split=12, n_estimators=5,\n",
      "                           random_state=3)\n",
      "kfold  4  score: 95.00%\n",
      "kfold  4  score: 97.48%\n",
      "GradientBoostingClassifier(max_depth=12, min_samples_split=12, n_estimators=5,\n",
      "                           random_state=3)\n",
      "kfold  4  score: 91.35%\n",
      "kfold  4  score: 97.48%\n",
      "Average score over k-fold 91.97% (+/-2.38%) [Gradient Boosting]\n",
      "RandomForestClassifier(max_depth=12, n_estimators=10, random_state=3)\n",
      "kfold  4  score: 93.09%\n",
      "kfold  4  score: 99.19%\n",
      "RandomForestClassifier(max_depth=12, n_estimators=10, random_state=3)\n",
      "kfold  4  score: 96.35%\n",
      "kfold  4  score: 99.06%\n",
      "RandomForestClassifier(max_depth=12, n_estimators=10, random_state=3)\n",
      "kfold  4  score: 92.31%\n",
      "kfold  4  score: 99.17%\n",
      "RandomForestClassifier(max_depth=12, n_estimators=10, random_state=3)\n",
      "kfold  4  score: 94.62%\n",
      "kfold  4  score: 98.55%\n",
      "RandomForestClassifier(max_depth=12, n_estimators=10, random_state=3)\n",
      "kfold  4  score: 93.08%\n",
      "kfold  4  score: 99.10%\n",
      "RandomForestClassifier(max_depth=12, n_estimators=10, random_state=3)\n",
      "kfold  4  score: 93.08%\n",
      "kfold  4  score: 99.27%\n",
      "RandomForestClassifier(max_depth=12, n_estimators=10, random_state=3)\n",
      "kfold  4  score: 95.19%\n",
      "kfold  4  score: 99.02%\n",
      "RandomForestClassifier(max_depth=12, n_estimators=10, random_state=3)\n",
      "kfold  4  score: 94.62%\n",
      "kfold  4  score: 98.95%\n",
      "RandomForestClassifier(max_depth=12, n_estimators=10, random_state=3)\n",
      "kfold  4  score: 95.58%\n",
      "kfold  4  score: 98.72%\n",
      "RandomForestClassifier(max_depth=12, n_estimators=10, random_state=3)\n",
      "kfold  4  score: 93.08%\n",
      "kfold  4  score: 98.78%\n",
      "Average score over k-fold 94.10% (+/-1.28%) [Random Forest]\n",
      "[21:47:05] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:47:05] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
      "              gamma=0, gpu_id=-1, importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.300000012,\n",
      "              max_delta_step=0, max_depth=12, max_leaf_nodes=None,\n",
      "              min_child_weight=1, min_samples_split=1, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=10, n_jobs=8,\n",
      "              num_parallel_tree=1, predictor='auto', random_state=3,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)\n",
      "kfold  4  score: 94.05%\n",
      "kfold  4  score: 99.36%\n",
      "[21:47:05] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:47:05] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
      "              gamma=0, gpu_id=-1, importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.300000012,\n",
      "              max_delta_step=0, max_depth=12, max_leaf_nodes=None,\n",
      "              min_child_weight=1, min_samples_split=1, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=10, n_jobs=8,\n",
      "              num_parallel_tree=1, predictor='auto', random_state=3,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)\n",
      "kfold  4  score: 96.35%\n",
      "kfold  4  score: 99.47%\n",
      "[21:47:05] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:47:05] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
      "              gamma=0, gpu_id=-1, importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.300000012,\n",
      "              max_delta_step=0, max_depth=12, max_leaf_nodes=None,\n",
      "              min_child_weight=1, min_samples_split=1, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=10, n_jobs=8,\n",
      "              num_parallel_tree=1, predictor='auto', random_state=3,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)\n",
      "kfold  4  score: 93.65%\n",
      "kfold  4  score: 99.68%\n",
      "[21:47:05] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:47:05] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
      "              gamma=0, gpu_id=-1, importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.300000012,\n",
      "              max_delta_step=0, max_depth=12, max_leaf_nodes=None,\n",
      "              min_child_weight=1, min_samples_split=1, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=10, n_jobs=8,\n",
      "              num_parallel_tree=1, predictor='auto', random_state=3,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)\n",
      "kfold  4  score: 96.92%\n",
      "kfold  4  score: 99.59%\n",
      "[21:47:06] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:47:06] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
      "              gamma=0, gpu_id=-1, importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.300000012,\n",
      "              max_delta_step=0, max_depth=12, max_leaf_nodes=None,\n",
      "              min_child_weight=1, min_samples_split=1, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=10, n_jobs=8,\n",
      "              num_parallel_tree=1, predictor='auto', random_state=3,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)\n",
      "kfold  4  score: 95.00%\n",
      "kfold  4  score: 99.74%\n",
      "[21:47:06] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:47:06] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
      "              gamma=0, gpu_id=-1, importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.300000012,\n",
      "              max_delta_step=0, max_depth=12, max_leaf_nodes=None,\n",
      "              min_child_weight=1, min_samples_split=1, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=10, n_jobs=8,\n",
      "              num_parallel_tree=1, predictor='auto', random_state=3,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)\n",
      "kfold  4  score: 93.08%\n",
      "kfold  4  score: 99.59%\n",
      "[21:47:06] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:47:06] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
      "              gamma=0, gpu_id=-1, importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.300000012,\n",
      "              max_delta_step=0, max_depth=12, max_leaf_nodes=None,\n",
      "              min_child_weight=1, min_samples_split=1, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=10, n_jobs=8,\n",
      "              num_parallel_tree=1, predictor='auto', random_state=3,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)\n",
      "kfold  4  score: 94.62%\n",
      "kfold  4  score: 99.66%\n",
      "[21:47:06] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:47:06] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
      "              gamma=0, gpu_id=-1, importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.300000012,\n",
      "              max_delta_step=0, max_depth=12, max_leaf_nodes=None,\n",
      "              min_child_weight=1, min_samples_split=1, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=10, n_jobs=8,\n",
      "              num_parallel_tree=1, predictor='auto', random_state=3,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)\n",
      "kfold  4  score: 95.19%\n",
      "kfold  4  score: 99.66%\n",
      "[21:47:06] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:47:06] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
      "              gamma=0, gpu_id=-1, importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.300000012,\n",
      "              max_delta_step=0, max_depth=12, max_leaf_nodes=None,\n",
      "              min_child_weight=1, min_samples_split=1, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=10, n_jobs=8,\n",
      "              num_parallel_tree=1, predictor='auto', random_state=3,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)\n",
      "kfold  4  score: 95.77%\n",
      "kfold  4  score: 99.64%\n",
      "[21:47:06] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:47:06] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
      "              gamma=0, gpu_id=-1, importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.300000012,\n",
      "              max_delta_step=0, max_depth=12, max_leaf_nodes=None,\n",
      "              min_child_weight=1, min_samples_split=1, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=10, n_jobs=8,\n",
      "              num_parallel_tree=1, predictor='auto', random_state=3,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)\n",
      "kfold  4  score: 94.42%\n",
      "kfold  4  score: 99.08%\n",
      "Average score over k-fold 94.91% (+/-1.14%) [XGBClassifier]\n",
      "[21:47:07] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:47:07] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "VotingClassifier(estimators=[('gb',\n",
      "                              GradientBoostingClassifier(max_depth=12,\n",
      "                                                         min_samples_split=12,\n",
      "                                                         n_estimators=5,\n",
      "                                                         random_state=3)),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(max_depth=12,\n",
      "                                                     n_estimators=10,\n",
      "                                                     random_state=3)),\n",
      "                             ('xgb',\n",
      "                              XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "                                            colsample_bylevel=1,\n",
      "                                            colsample_bynode=1,\n",
      "                                            colsample_bytree=1,\n",
      "                                            enable_categorical=False, gamma=0,\n",
      "                                            gpu...\n",
      "                                            interaction_constraints='',\n",
      "                                            learning_rate=0.300000012,\n",
      "                                            max_delta_step=0, max_depth=12,\n",
      "                                            max_leaf_nodes=None,\n",
      "                                            min_child_weight=1,\n",
      "                                            min_samples_split=1, missing=nan,\n",
      "                                            monotone_constraints='()',\n",
      "                                            n_estimators=10, n_jobs=8,\n",
      "                                            num_parallel_tree=1,\n",
      "                                            predictor='auto', random_state=3,\n",
      "                                            reg_alpha=0, reg_lambda=1,\n",
      "                                            scale_pos_weight=1, subsample=1,\n",
      "                                            tree_method='exact',\n",
      "                                            validate_parameters=1,\n",
      "                                            verbosity=None))])\n",
      "kfold  4  score: 94.05%\n",
      "kfold  4  score: 99.19%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21:47:07] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:47:07] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "VotingClassifier(estimators=[('gb',\n",
      "                              GradientBoostingClassifier(max_depth=12,\n",
      "                                                         min_samples_split=12,\n",
      "                                                         n_estimators=5,\n",
      "                                                         random_state=3)),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(max_depth=12,\n",
      "                                                     n_estimators=10,\n",
      "                                                     random_state=3)),\n",
      "                             ('xgb',\n",
      "                              XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "                                            colsample_bylevel=1,\n",
      "                                            colsample_bynode=1,\n",
      "                                            colsample_bytree=1,\n",
      "                                            enable_categorical=False, gamma=0,\n",
      "                                            gpu...\n",
      "                                            interaction_constraints='',\n",
      "                                            learning_rate=0.300000012,\n",
      "                                            max_delta_step=0, max_depth=12,\n",
      "                                            max_leaf_nodes=None,\n",
      "                                            min_child_weight=1,\n",
      "                                            min_samples_split=1, missing=nan,\n",
      "                                            monotone_constraints='()',\n",
      "                                            n_estimators=10, n_jobs=8,\n",
      "                                            num_parallel_tree=1,\n",
      "                                            predictor='auto', random_state=3,\n",
      "                                            reg_alpha=0, reg_lambda=1,\n",
      "                                            scale_pos_weight=1, subsample=1,\n",
      "                                            tree_method='exact',\n",
      "                                            validate_parameters=1,\n",
      "                                            verbosity=None))])\n",
      "kfold  4  score: 96.93%\n",
      "kfold  4  score: 99.44%\n",
      "[21:47:08] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:47:08] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "VotingClassifier(estimators=[('gb',\n",
      "                              GradientBoostingClassifier(max_depth=12,\n",
      "                                                         min_samples_split=12,\n",
      "                                                         n_estimators=5,\n",
      "                                                         random_state=3)),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(max_depth=12,\n",
      "                                                     n_estimators=10,\n",
      "                                                     random_state=3)),\n",
      "                             ('xgb',\n",
      "                              XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "                                            colsample_bylevel=1,\n",
      "                                            colsample_bynode=1,\n",
      "                                            colsample_bytree=1,\n",
      "                                            enable_categorical=False, gamma=0,\n",
      "                                            gpu...\n",
      "                                            interaction_constraints='',\n",
      "                                            learning_rate=0.300000012,\n",
      "                                            max_delta_step=0, max_depth=12,\n",
      "                                            max_leaf_nodes=None,\n",
      "                                            min_child_weight=1,\n",
      "                                            min_samples_split=1, missing=nan,\n",
      "                                            monotone_constraints='()',\n",
      "                                            n_estimators=10, n_jobs=8,\n",
      "                                            num_parallel_tree=1,\n",
      "                                            predictor='auto', random_state=3,\n",
      "                                            reg_alpha=0, reg_lambda=1,\n",
      "                                            scale_pos_weight=1, subsample=1,\n",
      "                                            tree_method='exact',\n",
      "                                            validate_parameters=1,\n",
      "                                            verbosity=None))])\n",
      "kfold  4  score: 93.08%\n",
      "kfold  4  score: 99.57%\n",
      "[21:47:08] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:47:08] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "VotingClassifier(estimators=[('gb',\n",
      "                              GradientBoostingClassifier(max_depth=12,\n",
      "                                                         min_samples_split=12,\n",
      "                                                         n_estimators=5,\n",
      "                                                         random_state=3)),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(max_depth=12,\n",
      "                                                     n_estimators=10,\n",
      "                                                     random_state=3)),\n",
      "                             ('xgb',\n",
      "                              XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "                                            colsample_bylevel=1,\n",
      "                                            colsample_bynode=1,\n",
      "                                            colsample_bytree=1,\n",
      "                                            enable_categorical=False, gamma=0,\n",
      "                                            gpu...\n",
      "                                            interaction_constraints='',\n",
      "                                            learning_rate=0.300000012,\n",
      "                                            max_delta_step=0, max_depth=12,\n",
      "                                            max_leaf_nodes=None,\n",
      "                                            min_child_weight=1,\n",
      "                                            min_samples_split=1, missing=nan,\n",
      "                                            monotone_constraints='()',\n",
      "                                            n_estimators=10, n_jobs=8,\n",
      "                                            num_parallel_tree=1,\n",
      "                                            predictor='auto', random_state=3,\n",
      "                                            reg_alpha=0, reg_lambda=1,\n",
      "                                            scale_pos_weight=1, subsample=1,\n",
      "                                            tree_method='exact',\n",
      "                                            validate_parameters=1,\n",
      "                                            verbosity=None))])\n",
      "kfold  4  score: 95.77%\n",
      "kfold  4  score: 99.49%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21:47:09] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:47:09] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "VotingClassifier(estimators=[('gb',\n",
      "                              GradientBoostingClassifier(max_depth=12,\n",
      "                                                         min_samples_split=12,\n",
      "                                                         n_estimators=5,\n",
      "                                                         random_state=3)),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(max_depth=12,\n",
      "                                                     n_estimators=10,\n",
      "                                                     random_state=3)),\n",
      "                             ('xgb',\n",
      "                              XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "                                            colsample_bylevel=1,\n",
      "                                            colsample_bynode=1,\n",
      "                                            colsample_bytree=1,\n",
      "                                            enable_categorical=False, gamma=0,\n",
      "                                            gpu...\n",
      "                                            interaction_constraints='',\n",
      "                                            learning_rate=0.300000012,\n",
      "                                            max_delta_step=0, max_depth=12,\n",
      "                                            max_leaf_nodes=None,\n",
      "                                            min_child_weight=1,\n",
      "                                            min_samples_split=1, missing=nan,\n",
      "                                            monotone_constraints='()',\n",
      "                                            n_estimators=10, n_jobs=8,\n",
      "                                            num_parallel_tree=1,\n",
      "                                            predictor='auto', random_state=3,\n",
      "                                            reg_alpha=0, reg_lambda=1,\n",
      "                                            scale_pos_weight=1, subsample=1,\n",
      "                                            tree_method='exact',\n",
      "                                            validate_parameters=1,\n",
      "                                            verbosity=None))])\n",
      "kfold  4  score: 94.81%\n",
      "kfold  4  score: 99.57%\n",
      "[21:47:10] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:47:10] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "VotingClassifier(estimators=[('gb',\n",
      "                              GradientBoostingClassifier(max_depth=12,\n",
      "                                                         min_samples_split=12,\n",
      "                                                         n_estimators=5,\n",
      "                                                         random_state=3)),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(max_depth=12,\n",
      "                                                     n_estimators=10,\n",
      "                                                     random_state=3)),\n",
      "                             ('xgb',\n",
      "                              XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "                                            colsample_bylevel=1,\n",
      "                                            colsample_bynode=1,\n",
      "                                            colsample_bytree=1,\n",
      "                                            enable_categorical=False, gamma=0,\n",
      "                                            gpu...\n",
      "                                            interaction_constraints='',\n",
      "                                            learning_rate=0.300000012,\n",
      "                                            max_delta_step=0, max_depth=12,\n",
      "                                            max_leaf_nodes=None,\n",
      "                                            min_child_weight=1,\n",
      "                                            min_samples_split=1, missing=nan,\n",
      "                                            monotone_constraints='()',\n",
      "                                            n_estimators=10, n_jobs=8,\n",
      "                                            num_parallel_tree=1,\n",
      "                                            predictor='auto', random_state=3,\n",
      "                                            reg_alpha=0, reg_lambda=1,\n",
      "                                            scale_pos_weight=1, subsample=1,\n",
      "                                            tree_method='exact',\n",
      "                                            validate_parameters=1,\n",
      "                                            verbosity=None))])\n",
      "kfold  4  score: 92.88%\n",
      "kfold  4  score: 99.57%\n",
      "[21:47:10] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:47:10] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "VotingClassifier(estimators=[('gb',\n",
      "                              GradientBoostingClassifier(max_depth=12,\n",
      "                                                         min_samples_split=12,\n",
      "                                                         n_estimators=5,\n",
      "                                                         random_state=3)),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(max_depth=12,\n",
      "                                                     n_estimators=10,\n",
      "                                                     random_state=3)),\n",
      "                             ('xgb',\n",
      "                              XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "                                            colsample_bylevel=1,\n",
      "                                            colsample_bynode=1,\n",
      "                                            colsample_bytree=1,\n",
      "                                            enable_categorical=False, gamma=0,\n",
      "                                            gpu...\n",
      "                                            interaction_constraints='',\n",
      "                                            learning_rate=0.300000012,\n",
      "                                            max_delta_step=0, max_depth=12,\n",
      "                                            max_leaf_nodes=None,\n",
      "                                            min_child_weight=1,\n",
      "                                            min_samples_split=1, missing=nan,\n",
      "                                            monotone_constraints='()',\n",
      "                                            n_estimators=10, n_jobs=8,\n",
      "                                            num_parallel_tree=1,\n",
      "                                            predictor='auto', random_state=3,\n",
      "                                            reg_alpha=0, reg_lambda=1,\n",
      "                                            scale_pos_weight=1, subsample=1,\n",
      "                                            tree_method='exact',\n",
      "                                            validate_parameters=1,\n",
      "                                            verbosity=None))])\n",
      "kfold  4  score: 95.58%\n",
      "kfold  4  score: 99.57%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21:47:11] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:47:11] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "VotingClassifier(estimators=[('gb',\n",
      "                              GradientBoostingClassifier(max_depth=12,\n",
      "                                                         min_samples_split=12,\n",
      "                                                         n_estimators=5,\n",
      "                                                         random_state=3)),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(max_depth=12,\n",
      "                                                     n_estimators=10,\n",
      "                                                     random_state=3)),\n",
      "                             ('xgb',\n",
      "                              XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "                                            colsample_bylevel=1,\n",
      "                                            colsample_bynode=1,\n",
      "                                            colsample_bytree=1,\n",
      "                                            enable_categorical=False, gamma=0,\n",
      "                                            gpu...\n",
      "                                            interaction_constraints='',\n",
      "                                            learning_rate=0.300000012,\n",
      "                                            max_delta_step=0, max_depth=12,\n",
      "                                            max_leaf_nodes=None,\n",
      "                                            min_child_weight=1,\n",
      "                                            min_samples_split=1, missing=nan,\n",
      "                                            monotone_constraints='()',\n",
      "                                            n_estimators=10, n_jobs=8,\n",
      "                                            num_parallel_tree=1,\n",
      "                                            predictor='auto', random_state=3,\n",
      "                                            reg_alpha=0, reg_lambda=1,\n",
      "                                            scale_pos_weight=1, subsample=1,\n",
      "                                            tree_method='exact',\n",
      "                                            validate_parameters=1,\n",
      "                                            verbosity=None))])\n",
      "kfold  4  score: 93.85%\n",
      "kfold  4  score: 99.44%\n",
      "[21:47:11] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:47:11] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "VotingClassifier(estimators=[('gb',\n",
      "                              GradientBoostingClassifier(max_depth=12,\n",
      "                                                         min_samples_split=12,\n",
      "                                                         n_estimators=5,\n",
      "                                                         random_state=3)),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(max_depth=12,\n",
      "                                                     n_estimators=10,\n",
      "                                                     random_state=3)),\n",
      "                             ('xgb',\n",
      "                              XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "                                            colsample_bylevel=1,\n",
      "                                            colsample_bynode=1,\n",
      "                                            colsample_bytree=1,\n",
      "                                            enable_categorical=False, gamma=0,\n",
      "                                            gpu...\n",
      "                                            interaction_constraints='',\n",
      "                                            learning_rate=0.300000012,\n",
      "                                            max_delta_step=0, max_depth=12,\n",
      "                                            max_leaf_nodes=None,\n",
      "                                            min_child_weight=1,\n",
      "                                            min_samples_split=1, missing=nan,\n",
      "                                            monotone_constraints='()',\n",
      "                                            n_estimators=10, n_jobs=8,\n",
      "                                            num_parallel_tree=1,\n",
      "                                            predictor='auto', random_state=3,\n",
      "                                            reg_alpha=0, reg_lambda=1,\n",
      "                                            scale_pos_weight=1, subsample=1,\n",
      "                                            tree_method='exact',\n",
      "                                            validate_parameters=1,\n",
      "                                            verbosity=None))])\n",
      "kfold  4  score: 96.35%\n",
      "kfold  4  score: 99.53%\n",
      "[21:47:12] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:47:12] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "VotingClassifier(estimators=[('gb',\n",
      "                              GradientBoostingClassifier(max_depth=12,\n",
      "                                                         min_samples_split=12,\n",
      "                                                         n_estimators=5,\n",
      "                                                         random_state=3)),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(max_depth=12,\n",
      "                                                     n_estimators=10,\n",
      "                                                     random_state=3)),\n",
      "                             ('xgb',\n",
      "                              XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "                                            colsample_bylevel=1,\n",
      "                                            colsample_bynode=1,\n",
      "                                            colsample_bytree=1,\n",
      "                                            enable_categorical=False, gamma=0,\n",
      "                                            gpu...\n",
      "                                            interaction_constraints='',\n",
      "                                            learning_rate=0.300000012,\n",
      "                                            max_delta_step=0, max_depth=12,\n",
      "                                            max_leaf_nodes=None,\n",
      "                                            min_child_weight=1,\n",
      "                                            min_samples_split=1, missing=nan,\n",
      "                                            monotone_constraints='()',\n",
      "                                            n_estimators=10, n_jobs=8,\n",
      "                                            num_parallel_tree=1,\n",
      "                                            predictor='auto', random_state=3,\n",
      "                                            reg_alpha=0, reg_lambda=1,\n",
      "                                            scale_pos_weight=1, subsample=1,\n",
      "                                            tree_method='exact',\n",
      "                                            validate_parameters=1,\n",
      "                                            verbosity=None))])\n",
      "kfold  4  score: 94.23%\n",
      "kfold  4  score: 99.30%\n",
      "Average score over k-fold 94.75% (+/-1.30%) [Ensemble Majority]\n",
      "./Files_Majority_vote/128_features/filter5.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_estimators = 100 \n",
      "\n",
      "10-fold cross validation for file :  5\n",
      "GradientBoostingClassifier(max_depth=12, min_samples_split=12, n_estimators=5,\n",
      "                           random_state=3)\n",
      "kfold  5  score: 90.14%\n",
      "kfold  5  score: 93.54%\n",
      "GradientBoostingClassifier(max_depth=12, min_samples_split=12, n_estimators=5,\n",
      "                           random_state=3)\n",
      "kfold  5  score: 87.98%\n",
      "kfold  5  score: 90.05%\n",
      "GradientBoostingClassifier(max_depth=12, min_samples_split=12, n_estimators=5,\n",
      "                           random_state=3)\n",
      "kfold  5  score: 90.50%\n",
      "kfold  5  score: 94.49%\n",
      "GradientBoostingClassifier(max_depth=12, min_samples_split=12, n_estimators=5,\n",
      "                           random_state=3)\n",
      "kfold  5  score: 87.40%\n",
      "kfold  5  score: 91.13%\n",
      "GradientBoostingClassifier(max_depth=12, min_samples_split=12, n_estimators=5,\n",
      "                           random_state=3)\n",
      "kfold  5  score: 87.60%\n",
      "kfold  5  score: 90.46%\n",
      "GradientBoostingClassifier(max_depth=12, min_samples_split=12, n_estimators=5,\n",
      "                           random_state=3)\n",
      "kfold  5  score: 91.09%\n",
      "kfold  5  score: 94.12%\n",
      "GradientBoostingClassifier(max_depth=12, min_samples_split=12, n_estimators=5,\n",
      "                           random_state=3)\n",
      "kfold  5  score: 89.92%\n",
      "kfold  5  score: 91.97%\n",
      "GradientBoostingClassifier(max_depth=12, min_samples_split=12, n_estimators=5,\n",
      "                           random_state=3)\n",
      "kfold  5  score: 85.66%\n",
      "kfold  5  score: 87.45%\n",
      "GradientBoostingClassifier(max_depth=12, min_samples_split=12, n_estimators=5,\n",
      "                           random_state=3)\n",
      "kfold  5  score: 86.05%\n",
      "kfold  5  score: 89.95%\n",
      "GradientBoostingClassifier(max_depth=12, min_samples_split=12, n_estimators=5,\n",
      "                           random_state=3)\n",
      "kfold  5  score: 88.57%\n",
      "kfold  5  score: 92.38%\n",
      "Average score over k-fold 88.49% (+/-1.78%) [Gradient Boosting]\n",
      "RandomForestClassifier(max_depth=12, n_estimators=10, random_state=3)\n",
      "kfold  5  score: 94.78%\n",
      "kfold  5  score: 98.11%\n",
      "RandomForestClassifier(max_depth=12, n_estimators=10, random_state=3)\n",
      "kfold  5  score: 94.38%\n",
      "kfold  5  score: 97.50%\n",
      "RandomForestClassifier(max_depth=12, n_estimators=10, random_state=3)\n",
      "kfold  5  score: 93.60%\n",
      "kfold  5  score: 96.94%\n",
      "RandomForestClassifier(max_depth=12, n_estimators=10, random_state=3)\n",
      "kfold  5  score: 94.38%\n",
      "kfold  5  score: 97.42%\n",
      "RandomForestClassifier(max_depth=12, n_estimators=10, random_state=3)\n",
      "kfold  5  score: 94.19%\n",
      "kfold  5  score: 97.89%\n",
      "RandomForestClassifier(max_depth=12, n_estimators=10, random_state=3)\n",
      "kfold  5  score: 93.22%\n",
      "kfold  5  score: 98.15%\n",
      "RandomForestClassifier(max_depth=12, n_estimators=10, random_state=3)\n",
      "kfold  5  score: 92.83%\n",
      "kfold  5  score: 98.58%\n",
      "RandomForestClassifier(max_depth=12, n_estimators=10, random_state=3)\n",
      "kfold  5  score: 94.57%\n",
      "kfold  5  score: 98.67%\n",
      "RandomForestClassifier(max_depth=12, n_estimators=10, random_state=3)\n",
      "kfold  5  score: 93.99%\n",
      "kfold  5  score: 98.69%\n",
      "RandomForestClassifier(max_depth=12, n_estimators=10, random_state=3)\n",
      "kfold  5  score: 91.67%\n",
      "kfold  5  score: 97.50%\n",
      "Average score over k-fold 93.76% (+/-0.91%) [Random Forest]\n",
      "[21:47:15] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:47:15] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
      "              gamma=0, gpu_id=-1, importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.300000012,\n",
      "              max_delta_step=0, max_depth=12, max_leaf_nodes=None,\n",
      "              min_child_weight=1, min_samples_split=1, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=10, n_jobs=8,\n",
      "              num_parallel_tree=1, predictor='auto', random_state=3,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)\n",
      "kfold  5  score: 96.91%\n",
      "kfold  5  score: 99.31%\n",
      "[21:47:15] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:47:15] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
      "              gamma=0, gpu_id=-1, importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.300000012,\n",
      "              max_delta_step=0, max_depth=12, max_leaf_nodes=None,\n",
      "              min_child_weight=1, min_samples_split=1, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=10, n_jobs=8,\n",
      "              num_parallel_tree=1, predictor='auto', random_state=3,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)\n",
      "kfold  5  score: 94.57%\n",
      "kfold  5  score: 99.48%\n",
      "[21:47:16] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:47:16] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
      "              gamma=0, gpu_id=-1, importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.300000012,\n",
      "              max_delta_step=0, max_depth=12, max_leaf_nodes=None,\n",
      "              min_child_weight=1, min_samples_split=1, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=10, n_jobs=8,\n",
      "              num_parallel_tree=1, predictor='auto', random_state=3,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)\n",
      "kfold  5  score: 94.38%\n",
      "kfold  5  score: 99.20%\n",
      "[21:47:16] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:47:16] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
      "              gamma=0, gpu_id=-1, importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.300000012,\n",
      "              max_delta_step=0, max_depth=12, max_leaf_nodes=None,\n",
      "              min_child_weight=1, min_samples_split=1, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=10, n_jobs=8,\n",
      "              num_parallel_tree=1, predictor='auto', random_state=3,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)\n",
      "kfold  5  score: 95.35%\n",
      "kfold  5  score: 99.48%\n",
      "[21:47:16] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:47:16] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
      "              gamma=0, gpu_id=-1, importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.300000012,\n",
      "              max_delta_step=0, max_depth=12, max_leaf_nodes=None,\n",
      "              min_child_weight=1, min_samples_split=1, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=10, n_jobs=8,\n",
      "              num_parallel_tree=1, predictor='auto', random_state=3,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)\n",
      "kfold  5  score: 95.35%\n",
      "kfold  5  score: 99.50%\n",
      "[21:47:16] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:47:16] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
      "              gamma=0, gpu_id=-1, importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.300000012,\n",
      "              max_delta_step=0, max_depth=12, max_leaf_nodes=None,\n",
      "              min_child_weight=1, min_samples_split=1, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=10, n_jobs=8,\n",
      "              num_parallel_tree=1, predictor='auto', random_state=3,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)\n",
      "kfold  5  score: 94.38%\n",
      "kfold  5  score: 99.59%\n",
      "[21:47:16] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:47:16] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
      "              gamma=0, gpu_id=-1, importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.300000012,\n",
      "              max_delta_step=0, max_depth=12, max_leaf_nodes=None,\n",
      "              min_child_weight=1, min_samples_split=1, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=10, n_jobs=8,\n",
      "              num_parallel_tree=1, predictor='auto', random_state=3,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)\n",
      "kfold  5  score: 93.99%\n",
      "kfold  5  score: 99.35%\n",
      "[21:47:16] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:47:16] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
      "              gamma=0, gpu_id=-1, importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.300000012,\n",
      "              max_delta_step=0, max_depth=12, max_leaf_nodes=None,\n",
      "              min_child_weight=1, min_samples_split=1, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=10, n_jobs=8,\n",
      "              num_parallel_tree=1, predictor='auto', random_state=3,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)\n",
      "kfold  5  score: 95.93%\n",
      "kfold  5  score: 99.29%\n",
      "[21:47:16] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:47:16] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
      "              gamma=0, gpu_id=-1, importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.300000012,\n",
      "              max_delta_step=0, max_depth=12, max_leaf_nodes=None,\n",
      "              min_child_weight=1, min_samples_split=1, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=10, n_jobs=8,\n",
      "              num_parallel_tree=1, predictor='auto', random_state=3,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)\n",
      "kfold  5  score: 95.35%\n",
      "kfold  5  score: 99.61%\n",
      "[21:47:16] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:47:16] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
      "              gamma=0, gpu_id=-1, importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.300000012,\n",
      "              max_delta_step=0, max_depth=12, max_leaf_nodes=None,\n",
      "              min_child_weight=1, min_samples_split=1, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=10, n_jobs=8,\n",
      "              num_parallel_tree=1, predictor='auto', random_state=3,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)\n",
      "kfold  5  score: 93.99%\n",
      "kfold  5  score: 99.61%\n",
      "Average score over k-fold 95.02% (+/-0.89%) [XGBClassifier]\n",
      "[21:47:17] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:47:17] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "VotingClassifier(estimators=[('gb',\n",
      "                              GradientBoostingClassifier(max_depth=12,\n",
      "                                                         min_samples_split=12,\n",
      "                                                         n_estimators=5,\n",
      "                                                         random_state=3)),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(max_depth=12,\n",
      "                                                     n_estimators=10,\n",
      "                                                     random_state=3)),\n",
      "                             ('xgb',\n",
      "                              XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "                                            colsample_bylevel=1,\n",
      "                                            colsample_bynode=1,\n",
      "                                            colsample_bytree=1,\n",
      "                                            enable_categorical=False, gamma=0,\n",
      "                                            gpu...\n",
      "                                            interaction_constraints='',\n",
      "                                            learning_rate=0.300000012,\n",
      "                                            max_delta_step=0, max_depth=12,\n",
      "                                            max_leaf_nodes=None,\n",
      "                                            min_child_weight=1,\n",
      "                                            min_samples_split=1, missing=nan,\n",
      "                                            monotone_constraints='()',\n",
      "                                            n_estimators=10, n_jobs=8,\n",
      "                                            num_parallel_tree=1,\n",
      "                                            predictor='auto', random_state=3,\n",
      "                                            reg_alpha=0, reg_lambda=1,\n",
      "                                            scale_pos_weight=1, subsample=1,\n",
      "                                            tree_method='exact',\n",
      "                                            validate_parameters=1,\n",
      "                                            verbosity=None))])\n",
      "kfold  5  score: 95.55%\n",
      "kfold  5  score: 98.45%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21:47:17] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:47:17] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "VotingClassifier(estimators=[('gb',\n",
      "                              GradientBoostingClassifier(max_depth=12,\n",
      "                                                         min_samples_split=12,\n",
      "                                                         n_estimators=5,\n",
      "                                                         random_state=3)),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(max_depth=12,\n",
      "                                                     n_estimators=10,\n",
      "                                                     random_state=3)),\n",
      "                             ('xgb',\n",
      "                              XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "                                            colsample_bylevel=1,\n",
      "                                            colsample_bynode=1,\n",
      "                                            colsample_bytree=1,\n",
      "                                            enable_categorical=False, gamma=0,\n",
      "                                            gpu...\n",
      "                                            interaction_constraints='',\n",
      "                                            learning_rate=0.300000012,\n",
      "                                            max_delta_step=0, max_depth=12,\n",
      "                                            max_leaf_nodes=None,\n",
      "                                            min_child_weight=1,\n",
      "                                            min_samples_split=1, missing=nan,\n",
      "                                            monotone_constraints='()',\n",
      "                                            n_estimators=10, n_jobs=8,\n",
      "                                            num_parallel_tree=1,\n",
      "                                            predictor='auto', random_state=3,\n",
      "                                            reg_alpha=0, reg_lambda=1,\n",
      "                                            scale_pos_weight=1, subsample=1,\n",
      "                                            tree_method='exact',\n",
      "                                            validate_parameters=1,\n",
      "                                            verbosity=None))])\n",
      "kfold  5  score: 94.77%\n",
      "kfold  5  score: 97.93%\n",
      "[21:47:18] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:47:18] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "VotingClassifier(estimators=[('gb',\n",
      "                              GradientBoostingClassifier(max_depth=12,\n",
      "                                                         min_samples_split=12,\n",
      "                                                         n_estimators=5,\n",
      "                                                         random_state=3)),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(max_depth=12,\n",
      "                                                     n_estimators=10,\n",
      "                                                     random_state=3)),\n",
      "                             ('xgb',\n",
      "                              XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "                                            colsample_bylevel=1,\n",
      "                                            colsample_bynode=1,\n",
      "                                            colsample_bytree=1,\n",
      "                                            enable_categorical=False, gamma=0,\n",
      "                                            gpu...\n",
      "                                            interaction_constraints='',\n",
      "                                            learning_rate=0.300000012,\n",
      "                                            max_delta_step=0, max_depth=12,\n",
      "                                            max_leaf_nodes=None,\n",
      "                                            min_child_weight=1,\n",
      "                                            min_samples_split=1, missing=nan,\n",
      "                                            monotone_constraints='()',\n",
      "                                            n_estimators=10, n_jobs=8,\n",
      "                                            num_parallel_tree=1,\n",
      "                                            predictor='auto', random_state=3,\n",
      "                                            reg_alpha=0, reg_lambda=1,\n",
      "                                            scale_pos_weight=1, subsample=1,\n",
      "                                            tree_method='exact',\n",
      "                                            validate_parameters=1,\n",
      "                                            verbosity=None))])\n",
      "kfold  5  score: 93.80%\n",
      "kfold  5  score: 98.04%\n",
      "[21:47:18] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:47:18] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "VotingClassifier(estimators=[('gb',\n",
      "                              GradientBoostingClassifier(max_depth=12,\n",
      "                                                         min_samples_split=12,\n",
      "                                                         n_estimators=5,\n",
      "                                                         random_state=3)),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(max_depth=12,\n",
      "                                                     n_estimators=10,\n",
      "                                                     random_state=3)),\n",
      "                             ('xgb',\n",
      "                              XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "                                            colsample_bylevel=1,\n",
      "                                            colsample_bynode=1,\n",
      "                                            colsample_bytree=1,\n",
      "                                            enable_categorical=False, gamma=0,\n",
      "                                            gpu...\n",
      "                                            interaction_constraints='',\n",
      "                                            learning_rate=0.300000012,\n",
      "                                            max_delta_step=0, max_depth=12,\n",
      "                                            max_leaf_nodes=None,\n",
      "                                            min_child_weight=1,\n",
      "                                            min_samples_split=1, missing=nan,\n",
      "                                            monotone_constraints='()',\n",
      "                                            n_estimators=10, n_jobs=8,\n",
      "                                            num_parallel_tree=1,\n",
      "                                            predictor='auto', random_state=3,\n",
      "                                            reg_alpha=0, reg_lambda=1,\n",
      "                                            scale_pos_weight=1, subsample=1,\n",
      "                                            tree_method='exact',\n",
      "                                            validate_parameters=1,\n",
      "                                            verbosity=None))])\n",
      "kfold  5  score: 94.38%\n",
      "kfold  5  score: 97.55%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21:47:19] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:47:19] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "VotingClassifier(estimators=[('gb',\n",
      "                              GradientBoostingClassifier(max_depth=12,\n",
      "                                                         min_samples_split=12,\n",
      "                                                         n_estimators=5,\n",
      "                                                         random_state=3)),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(max_depth=12,\n",
      "                                                     n_estimators=10,\n",
      "                                                     random_state=3)),\n",
      "                             ('xgb',\n",
      "                              XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "                                            colsample_bylevel=1,\n",
      "                                            colsample_bynode=1,\n",
      "                                            colsample_bytree=1,\n",
      "                                            enable_categorical=False, gamma=0,\n",
      "                                            gpu...\n",
      "                                            interaction_constraints='',\n",
      "                                            learning_rate=0.300000012,\n",
      "                                            max_delta_step=0, max_depth=12,\n",
      "                                            max_leaf_nodes=None,\n",
      "                                            min_child_weight=1,\n",
      "                                            min_samples_split=1, missing=nan,\n",
      "                                            monotone_constraints='()',\n",
      "                                            n_estimators=10, n_jobs=8,\n",
      "                                            num_parallel_tree=1,\n",
      "                                            predictor='auto', random_state=3,\n",
      "                                            reg_alpha=0, reg_lambda=1,\n",
      "                                            scale_pos_weight=1, subsample=1,\n",
      "                                            tree_method='exact',\n",
      "                                            validate_parameters=1,\n",
      "                                            verbosity=None))])\n",
      "kfold  5  score: 94.19%\n",
      "kfold  5  score: 98.11%\n",
      "[21:47:19] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:47:19] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "VotingClassifier(estimators=[('gb',\n",
      "                              GradientBoostingClassifier(max_depth=12,\n",
      "                                                         min_samples_split=12,\n",
      "                                                         n_estimators=5,\n",
      "                                                         random_state=3)),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(max_depth=12,\n",
      "                                                     n_estimators=10,\n",
      "                                                     random_state=3)),\n",
      "                             ('xgb',\n",
      "                              XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "                                            colsample_bylevel=1,\n",
      "                                            colsample_bynode=1,\n",
      "                                            colsample_bytree=1,\n",
      "                                            enable_categorical=False, gamma=0,\n",
      "                                            gpu...\n",
      "                                            interaction_constraints='',\n",
      "                                            learning_rate=0.300000012,\n",
      "                                            max_delta_step=0, max_depth=12,\n",
      "                                            max_leaf_nodes=None,\n",
      "                                            min_child_weight=1,\n",
      "                                            min_samples_split=1, missing=nan,\n",
      "                                            monotone_constraints='()',\n",
      "                                            n_estimators=10, n_jobs=8,\n",
      "                                            num_parallel_tree=1,\n",
      "                                            predictor='auto', random_state=3,\n",
      "                                            reg_alpha=0, reg_lambda=1,\n",
      "                                            scale_pos_weight=1, subsample=1,\n",
      "                                            tree_method='exact',\n",
      "                                            validate_parameters=1,\n",
      "                                            verbosity=None))])\n",
      "kfold  5  score: 94.19%\n",
      "kfold  5  score: 98.69%\n",
      "[21:47:20] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:47:20] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "VotingClassifier(estimators=[('gb',\n",
      "                              GradientBoostingClassifier(max_depth=12,\n",
      "                                                         min_samples_split=12,\n",
      "                                                         n_estimators=5,\n",
      "                                                         random_state=3)),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(max_depth=12,\n",
      "                                                     n_estimators=10,\n",
      "                                                     random_state=3)),\n",
      "                             ('xgb',\n",
      "                              XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "                                            colsample_bylevel=1,\n",
      "                                            colsample_bynode=1,\n",
      "                                            colsample_bytree=1,\n",
      "                                            enable_categorical=False, gamma=0,\n",
      "                                            gpu...\n",
      "                                            interaction_constraints='',\n",
      "                                            learning_rate=0.300000012,\n",
      "                                            max_delta_step=0, max_depth=12,\n",
      "                                            max_leaf_nodes=None,\n",
      "                                            min_child_weight=1,\n",
      "                                            min_samples_split=1, missing=nan,\n",
      "                                            monotone_constraints='()',\n",
      "                                            n_estimators=10, n_jobs=8,\n",
      "                                            num_parallel_tree=1,\n",
      "                                            predictor='auto', random_state=3,\n",
      "                                            reg_alpha=0, reg_lambda=1,\n",
      "                                            scale_pos_weight=1, subsample=1,\n",
      "                                            tree_method='exact',\n",
      "                                            validate_parameters=1,\n",
      "                                            verbosity=None))])\n",
      "kfold  5  score: 93.22%\n",
      "kfold  5  score: 98.69%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21:47:20] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:47:20] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "VotingClassifier(estimators=[('gb',\n",
      "                              GradientBoostingClassifier(max_depth=12,\n",
      "                                                         min_samples_split=12,\n",
      "                                                         n_estimators=5,\n",
      "                                                         random_state=3)),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(max_depth=12,\n",
      "                                                     n_estimators=10,\n",
      "                                                     random_state=3)),\n",
      "                             ('xgb',\n",
      "                              XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "                                            colsample_bylevel=1,\n",
      "                                            colsample_bynode=1,\n",
      "                                            colsample_bytree=1,\n",
      "                                            enable_categorical=False, gamma=0,\n",
      "                                            gpu...\n",
      "                                            interaction_constraints='',\n",
      "                                            learning_rate=0.300000012,\n",
      "                                            max_delta_step=0, max_depth=12,\n",
      "                                            max_leaf_nodes=None,\n",
      "                                            min_child_weight=1,\n",
      "                                            min_samples_split=1, missing=nan,\n",
      "                                            monotone_constraints='()',\n",
      "                                            n_estimators=10, n_jobs=8,\n",
      "                                            num_parallel_tree=1,\n",
      "                                            predictor='auto', random_state=3,\n",
      "                                            reg_alpha=0, reg_lambda=1,\n",
      "                                            scale_pos_weight=1, subsample=1,\n",
      "                                            tree_method='exact',\n",
      "                                            validate_parameters=1,\n",
      "                                            verbosity=None))])\n",
      "kfold  5  score: 94.57%\n",
      "kfold  5  score: 98.67%\n",
      "[21:47:20] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:47:20] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "VotingClassifier(estimators=[('gb',\n",
      "                              GradientBoostingClassifier(max_depth=12,\n",
      "                                                         min_samples_split=12,\n",
      "                                                         n_estimators=5,\n",
      "                                                         random_state=3)),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(max_depth=12,\n",
      "                                                     n_estimators=10,\n",
      "                                                     random_state=3)),\n",
      "                             ('xgb',\n",
      "                              XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "                                            colsample_bylevel=1,\n",
      "                                            colsample_bynode=1,\n",
      "                                            colsample_bytree=1,\n",
      "                                            enable_categorical=False, gamma=0,\n",
      "                                            gpu...\n",
      "                                            interaction_constraints='',\n",
      "                                            learning_rate=0.300000012,\n",
      "                                            max_delta_step=0, max_depth=12,\n",
      "                                            max_leaf_nodes=None,\n",
      "                                            min_child_weight=1,\n",
      "                                            min_samples_split=1, missing=nan,\n",
      "                                            monotone_constraints='()',\n",
      "                                            n_estimators=10, n_jobs=8,\n",
      "                                            num_parallel_tree=1,\n",
      "                                            predictor='auto', random_state=3,\n",
      "                                            reg_alpha=0, reg_lambda=1,\n",
      "                                            scale_pos_weight=1, subsample=1,\n",
      "                                            tree_method='exact',\n",
      "                                            validate_parameters=1,\n",
      "                                            verbosity=None))])\n",
      "kfold  5  score: 94.57%\n",
      "kfold  5  score: 98.67%\n",
      "[21:47:21] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:47:21] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "VotingClassifier(estimators=[('gb',\n",
      "                              GradientBoostingClassifier(max_depth=12,\n",
      "                                                         min_samples_split=12,\n",
      "                                                         n_estimators=5,\n",
      "                                                         random_state=3)),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(max_depth=12,\n",
      "                                                     n_estimators=10,\n",
      "                                                     random_state=3)),\n",
      "                             ('xgb',\n",
      "                              XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "                                            colsample_bylevel=1,\n",
      "                                            colsample_bynode=1,\n",
      "                                            colsample_bytree=1,\n",
      "                                            enable_categorical=False, gamma=0,\n",
      "                                            gpu...\n",
      "                                            interaction_constraints='',\n",
      "                                            learning_rate=0.300000012,\n",
      "                                            max_delta_step=0, max_depth=12,\n",
      "                                            max_leaf_nodes=None,\n",
      "                                            min_child_weight=1,\n",
      "                                            min_samples_split=1, missing=nan,\n",
      "                                            monotone_constraints='()',\n",
      "                                            n_estimators=10, n_jobs=8,\n",
      "                                            num_parallel_tree=1,\n",
      "                                            predictor='auto', random_state=3,\n",
      "                                            reg_alpha=0, reg_lambda=1,\n",
      "                                            scale_pos_weight=1, subsample=1,\n",
      "                                            tree_method='exact',\n",
      "                                            validate_parameters=1,\n",
      "                                            verbosity=None))])\n",
      "kfold  5  score: 92.44%\n",
      "kfold  5  score: 98.30%\n",
      "Average score over k-fold 94.17% (+/-0.82%) [Ensemble Majority]\n",
      "./Files_Majority_vote/128_features/filter6.csv\n",
      "n_estimators = 100 \n",
      "\n",
      "10-fold cross validation for file :  6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GradientBoostingClassifier(max_depth=12, min_samples_split=12, n_estimators=5,\n",
      "                           random_state=3)\n",
      "kfold  6  score: 81.49%\n",
      "kfold  6  score: 83.33%\n",
      "GradientBoostingClassifier(max_depth=12, min_samples_split=12, n_estimators=5,\n",
      "                           random_state=3)\n",
      "kfold  6  score: 85.31%\n",
      "kfold  6  score: 89.98%\n",
      "GradientBoostingClassifier(max_depth=12, min_samples_split=12, n_estimators=5,\n",
      "                           random_state=3)\n",
      "kfold  6  score: 83.90%\n",
      "kfold  6  score: 85.35%\n",
      "GradientBoostingClassifier(max_depth=12, min_samples_split=12, n_estimators=5,\n",
      "                           random_state=3)\n",
      "kfold  6  score: 79.88%\n",
      "kfold  6  score: 84.27%\n",
      "GradientBoostingClassifier(max_depth=12, min_samples_split=12, n_estimators=5,\n",
      "                           random_state=3)\n",
      "kfold  6  score: 84.71%\n",
      "kfold  6  score: 88.61%\n",
      "GradientBoostingClassifier(max_depth=12, min_samples_split=12, n_estimators=5,\n",
      "                           random_state=3)\n",
      "kfold  6  score: 80.48%\n",
      "kfold  6  score: 82.93%\n",
      "GradientBoostingClassifier(max_depth=12, min_samples_split=12, n_estimators=5,\n",
      "                           random_state=3)\n",
      "kfold  6  score: 83.50%\n",
      "kfold  6  score: 84.07%\n",
      "GradientBoostingClassifier(max_depth=12, min_samples_split=12, n_estimators=5,\n",
      "                           random_state=3)\n",
      "kfold  6  score: 81.65%\n",
      "kfold  6  score: 84.72%\n",
      "GradientBoostingClassifier(max_depth=12, min_samples_split=12, n_estimators=5,\n",
      "                           random_state=3)\n",
      "kfold  6  score: 83.27%\n",
      "kfold  6  score: 88.57%\n",
      "GradientBoostingClassifier(max_depth=12, min_samples_split=12, n_estimators=5,\n",
      "                           random_state=3)\n",
      "kfold  6  score: 81.25%\n",
      "kfold  6  score: 85.10%\n",
      "Average score over k-fold 82.54% (+/-1.75%) [Gradient Boosting]\n",
      "RandomForestClassifier(max_depth=12, n_estimators=10, random_state=3)\n",
      "kfold  6  score: 92.15%\n",
      "kfold  6  score: 96.55%\n",
      "RandomForestClassifier(max_depth=12, n_estimators=10, random_state=3)\n",
      "kfold  6  score: 90.34%\n",
      "kfold  6  score: 95.64%\n",
      "RandomForestClassifier(max_depth=12, n_estimators=10, random_state=3)\n",
      "kfold  6  score: 89.74%\n",
      "kfold  6  score: 93.58%\n",
      "RandomForestClassifier(max_depth=12, n_estimators=10, random_state=3)\n",
      "kfold  6  score: 91.15%\n",
      "kfold  6  score: 95.88%\n",
      "RandomForestClassifier(max_depth=12, n_estimators=10, random_state=3)\n",
      "kfold  6  score: 90.95%\n",
      "kfold  6  score: 95.68%\n",
      "RandomForestClassifier(max_depth=12, n_estimators=10, random_state=3)\n",
      "kfold  6  score: 91.55%\n",
      "kfold  6  score: 96.20%\n",
      "RandomForestClassifier(max_depth=12, n_estimators=10, random_state=3)\n",
      "kfold  6  score: 90.54%\n",
      "kfold  6  score: 94.92%\n",
      "RandomForestClassifier(max_depth=12, n_estimators=10, random_state=3)\n",
      "kfold  6  score: 92.74%\n",
      "kfold  6  score: 97.81%\n",
      "RandomForestClassifier(max_depth=12, n_estimators=10, random_state=3)\n",
      "kfold  6  score: 88.71%\n",
      "kfold  6  score: 95.24%\n",
      "RandomForestClassifier(max_depth=12, n_estimators=10, random_state=3)\n",
      "kfold  6  score: 91.94%\n",
      "kfold  6  score: 96.31%\n",
      "Average score over k-fold 90.98% (+/-1.14%) [Random Forest]\n",
      "[21:47:25] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:47:25] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
      "              gamma=0, gpu_id=-1, importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.300000012,\n",
      "              max_delta_step=0, max_depth=12, max_leaf_nodes=None,\n",
      "              min_child_weight=1, min_samples_split=1, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=10, n_jobs=8,\n",
      "              num_parallel_tree=1, predictor='auto', random_state=3,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)\n",
      "kfold  6  score: 95.17%\n",
      "kfold  6  score: 99.04%\n",
      "[21:47:25] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:47:25] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
      "              gamma=0, gpu_id=-1, importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.300000012,\n",
      "              max_delta_step=0, max_depth=12, max_leaf_nodes=None,\n",
      "              min_child_weight=1, min_samples_split=1, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=10, n_jobs=8,\n",
      "              num_parallel_tree=1, predictor='auto', random_state=3,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)\n",
      "kfold  6  score: 94.16%\n",
      "kfold  6  score: 98.66%\n",
      "[21:47:25] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:47:25] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
      "              gamma=0, gpu_id=-1, importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.300000012,\n",
      "              max_delta_step=0, max_depth=12, max_leaf_nodes=None,\n",
      "              min_child_weight=1, min_samples_split=1, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=10, n_jobs=8,\n",
      "              num_parallel_tree=1, predictor='auto', random_state=3,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)\n",
      "kfold  6  score: 92.56%\n",
      "kfold  6  score: 97.45%\n",
      "[21:47:25] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:47:25] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
      "              gamma=0, gpu_id=-1, importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.300000012,\n",
      "              max_delta_step=0, max_depth=12, max_leaf_nodes=None,\n",
      "              min_child_weight=1, min_samples_split=1, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=10, n_jobs=8,\n",
      "              num_parallel_tree=1, predictor='auto', random_state=3,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)\n",
      "kfold  6  score: 91.95%\n",
      "kfold  6  score: 98.26%\n",
      "[21:47:25] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:47:25] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
      "              gamma=0, gpu_id=-1, importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.300000012,\n",
      "              max_delta_step=0, max_depth=12, max_leaf_nodes=None,\n",
      "              min_child_weight=1, min_samples_split=1, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=10, n_jobs=8,\n",
      "              num_parallel_tree=1, predictor='auto', random_state=3,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)\n",
      "kfold  6  score: 93.56%\n",
      "kfold  6  score: 98.19%\n",
      "[21:47:25] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:47:25] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
      "              gamma=0, gpu_id=-1, importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.300000012,\n",
      "              max_delta_step=0, max_depth=12, max_leaf_nodes=None,\n",
      "              min_child_weight=1, min_samples_split=1, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=10, n_jobs=8,\n",
      "              num_parallel_tree=1, predictor='auto', random_state=3,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)\n",
      "kfold  6  score: 93.96%\n",
      "kfold  6  score: 98.88%\n",
      "[21:47:25] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:47:25] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
      "              gamma=0, gpu_id=-1, importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.300000012,\n",
      "              max_delta_step=0, max_depth=12, max_leaf_nodes=None,\n",
      "              min_child_weight=1, min_samples_split=1, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=10, n_jobs=8,\n",
      "              num_parallel_tree=1, predictor='auto', random_state=3,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)\n",
      "kfold  6  score: 95.57%\n",
      "kfold  6  score: 98.84%\n",
      "[21:47:25] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:47:25] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
      "              gamma=0, gpu_id=-1, importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.300000012,\n",
      "              max_delta_step=0, max_depth=12, max_leaf_nodes=None,\n",
      "              min_child_weight=1, min_samples_split=1, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=10, n_jobs=8,\n",
      "              num_parallel_tree=1, predictor='auto', random_state=3,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)\n",
      "kfold  6  score: 92.54%\n",
      "kfold  6  score: 97.47%\n",
      "[21:47:25] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:47:25] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
      "              gamma=0, gpu_id=-1, importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.300000012,\n",
      "              max_delta_step=0, max_depth=12, max_leaf_nodes=None,\n",
      "              min_child_weight=1, min_samples_split=1, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=10, n_jobs=8,\n",
      "              num_parallel_tree=1, predictor='auto', random_state=3,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)\n",
      "kfold  6  score: 92.54%\n",
      "kfold  6  score: 98.37%\n",
      "[21:47:26] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:47:26] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
      "              gamma=0, gpu_id=-1, importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.300000012,\n",
      "              max_delta_step=0, max_depth=12, max_leaf_nodes=None,\n",
      "              min_child_weight=1, min_samples_split=1, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=10, n_jobs=8,\n",
      "              num_parallel_tree=1, predictor='auto', random_state=3,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)\n",
      "kfold  6  score: 94.76%\n",
      "kfold  6  score: 98.41%\n",
      "Average score over k-fold 93.68% (+/-1.19%) [XGBClassifier]\n",
      "[21:47:26] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:47:26] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "VotingClassifier(estimators=[('gb',\n",
      "                              GradientBoostingClassifier(max_depth=12,\n",
      "                                                         min_samples_split=12,\n",
      "                                                         n_estimators=5,\n",
      "                                                         random_state=3)),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(max_depth=12,\n",
      "                                                     n_estimators=10,\n",
      "                                                     random_state=3)),\n",
      "                             ('xgb',\n",
      "                              XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "                                            colsample_bylevel=1,\n",
      "                                            colsample_bynode=1,\n",
      "                                            colsample_bytree=1,\n",
      "                                            enable_categorical=False, gamma=0,\n",
      "                                            gpu...\n",
      "                                            interaction_constraints='',\n",
      "                                            learning_rate=0.300000012,\n",
      "                                            max_delta_step=0, max_depth=12,\n",
      "                                            max_leaf_nodes=None,\n",
      "                                            min_child_weight=1,\n",
      "                                            min_samples_split=1, missing=nan,\n",
      "                                            monotone_constraints='()',\n",
      "                                            n_estimators=10, n_jobs=8,\n",
      "                                            num_parallel_tree=1,\n",
      "                                            predictor='auto', random_state=3,\n",
      "                                            reg_alpha=0, reg_lambda=1,\n",
      "                                            scale_pos_weight=1, subsample=1,\n",
      "                                            tree_method='exact',\n",
      "                                            validate_parameters=1,\n",
      "                                            verbosity=None))])\n",
      "kfold  6  score: 93.56%\n",
      "kfold  6  score: 97.16%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21:47:27] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:47:27] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "VotingClassifier(estimators=[('gb',\n",
      "                              GradientBoostingClassifier(max_depth=12,\n",
      "                                                         min_samples_split=12,\n",
      "                                                         n_estimators=5,\n",
      "                                                         random_state=3)),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(max_depth=12,\n",
      "                                                     n_estimators=10,\n",
      "                                                     random_state=3)),\n",
      "                             ('xgb',\n",
      "                              XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "                                            colsample_bylevel=1,\n",
      "                                            colsample_bynode=1,\n",
      "                                            colsample_bytree=1,\n",
      "                                            enable_categorical=False, gamma=0,\n",
      "                                            gpu...\n",
      "                                            interaction_constraints='',\n",
      "                                            learning_rate=0.300000012,\n",
      "                                            max_delta_step=0, max_depth=12,\n",
      "                                            max_leaf_nodes=None,\n",
      "                                            min_child_weight=1,\n",
      "                                            min_samples_split=1, missing=nan,\n",
      "                                            monotone_constraints='()',\n",
      "                                            n_estimators=10, n_jobs=8,\n",
      "                                            num_parallel_tree=1,\n",
      "                                            predictor='auto', random_state=3,\n",
      "                                            reg_alpha=0, reg_lambda=1,\n",
      "                                            scale_pos_weight=1, subsample=1,\n",
      "                                            tree_method='exact',\n",
      "                                            validate_parameters=1,\n",
      "                                            verbosity=None))])\n",
      "kfold  6  score: 91.15%\n",
      "kfold  6  score: 96.73%\n",
      "[21:47:27] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:47:27] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "VotingClassifier(estimators=[('gb',\n",
      "                              GradientBoostingClassifier(max_depth=12,\n",
      "                                                         min_samples_split=12,\n",
      "                                                         n_estimators=5,\n",
      "                                                         random_state=3)),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(max_depth=12,\n",
      "                                                     n_estimators=10,\n",
      "                                                     random_state=3)),\n",
      "                             ('xgb',\n",
      "                              XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "                                            colsample_bylevel=1,\n",
      "                                            colsample_bynode=1,\n",
      "                                            colsample_bytree=1,\n",
      "                                            enable_categorical=False, gamma=0,\n",
      "                                            gpu...\n",
      "                                            interaction_constraints='',\n",
      "                                            learning_rate=0.300000012,\n",
      "                                            max_delta_step=0, max_depth=12,\n",
      "                                            max_leaf_nodes=None,\n",
      "                                            min_child_weight=1,\n",
      "                                            min_samples_split=1, missing=nan,\n",
      "                                            monotone_constraints='()',\n",
      "                                            n_estimators=10, n_jobs=8,\n",
      "                                            num_parallel_tree=1,\n",
      "                                            predictor='auto', random_state=3,\n",
      "                                            reg_alpha=0, reg_lambda=1,\n",
      "                                            scale_pos_weight=1, subsample=1,\n",
      "                                            tree_method='exact',\n",
      "                                            validate_parameters=1,\n",
      "                                            verbosity=None))])\n",
      "kfold  6  score: 90.34%\n",
      "kfold  6  score: 93.96%\n",
      "[21:47:27] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:47:28] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "VotingClassifier(estimators=[('gb',\n",
      "                              GradientBoostingClassifier(max_depth=12,\n",
      "                                                         min_samples_split=12,\n",
      "                                                         n_estimators=5,\n",
      "                                                         random_state=3)),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(max_depth=12,\n",
      "                                                     n_estimators=10,\n",
      "                                                     random_state=3)),\n",
      "                             ('xgb',\n",
      "                              XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "                                            colsample_bylevel=1,\n",
      "                                            colsample_bynode=1,\n",
      "                                            colsample_bytree=1,\n",
      "                                            enable_categorical=False, gamma=0,\n",
      "                                            gpu...\n",
      "                                            interaction_constraints='',\n",
      "                                            learning_rate=0.300000012,\n",
      "                                            max_delta_step=0, max_depth=12,\n",
      "                                            max_leaf_nodes=None,\n",
      "                                            min_child_weight=1,\n",
      "                                            min_samples_split=1, missing=nan,\n",
      "                                            monotone_constraints='()',\n",
      "                                            n_estimators=10, n_jobs=8,\n",
      "                                            num_parallel_tree=1,\n",
      "                                            predictor='auto', random_state=3,\n",
      "                                            reg_alpha=0, reg_lambda=1,\n",
      "                                            scale_pos_weight=1, subsample=1,\n",
      "                                            tree_method='exact',\n",
      "                                            validate_parameters=1,\n",
      "                                            verbosity=None))])\n",
      "kfold  6  score: 90.14%\n",
      "kfold  6  score: 95.88%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21:47:28] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:47:28] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "VotingClassifier(estimators=[('gb',\n",
      "                              GradientBoostingClassifier(max_depth=12,\n",
      "                                                         min_samples_split=12,\n",
      "                                                         n_estimators=5,\n",
      "                                                         random_state=3)),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(max_depth=12,\n",
      "                                                     n_estimators=10,\n",
      "                                                     random_state=3)),\n",
      "                             ('xgb',\n",
      "                              XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "                                            colsample_bylevel=1,\n",
      "                                            colsample_bynode=1,\n",
      "                                            colsample_bytree=1,\n",
      "                                            enable_categorical=False, gamma=0,\n",
      "                                            gpu...\n",
      "                                            interaction_constraints='',\n",
      "                                            learning_rate=0.300000012,\n",
      "                                            max_delta_step=0, max_depth=12,\n",
      "                                            max_leaf_nodes=None,\n",
      "                                            min_child_weight=1,\n",
      "                                            min_samples_split=1, missing=nan,\n",
      "                                            monotone_constraints='()',\n",
      "                                            n_estimators=10, n_jobs=8,\n",
      "                                            num_parallel_tree=1,\n",
      "                                            predictor='auto', random_state=3,\n",
      "                                            reg_alpha=0, reg_lambda=1,\n",
      "                                            scale_pos_weight=1, subsample=1,\n",
      "                                            tree_method='exact',\n",
      "                                            validate_parameters=1,\n",
      "                                            verbosity=None))])\n",
      "kfold  6  score: 90.74%\n",
      "kfold  6  score: 96.06%\n",
      "[21:47:28] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:47:28] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "VotingClassifier(estimators=[('gb',\n",
      "                              GradientBoostingClassifier(max_depth=12,\n",
      "                                                         min_samples_split=12,\n",
      "                                                         n_estimators=5,\n",
      "                                                         random_state=3)),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(max_depth=12,\n",
      "                                                     n_estimators=10,\n",
      "                                                     random_state=3)),\n",
      "                             ('xgb',\n",
      "                              XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "                                            colsample_bylevel=1,\n",
      "                                            colsample_bynode=1,\n",
      "                                            colsample_bytree=1,\n",
      "                                            enable_categorical=False, gamma=0,\n",
      "                                            gpu...\n",
      "                                            interaction_constraints='',\n",
      "                                            learning_rate=0.300000012,\n",
      "                                            max_delta_step=0, max_depth=12,\n",
      "                                            max_leaf_nodes=None,\n",
      "                                            min_child_weight=1,\n",
      "                                            min_samples_split=1, missing=nan,\n",
      "                                            monotone_constraints='()',\n",
      "                                            n_estimators=10, n_jobs=8,\n",
      "                                            num_parallel_tree=1,\n",
      "                                            predictor='auto', random_state=3,\n",
      "                                            reg_alpha=0, reg_lambda=1,\n",
      "                                            scale_pos_weight=1, subsample=1,\n",
      "                                            tree_method='exact',\n",
      "                                            validate_parameters=1,\n",
      "                                            verbosity=None))])\n",
      "kfold  6  score: 91.15%\n",
      "kfold  6  score: 96.71%\n",
      "[21:47:29] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:47:29] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "VotingClassifier(estimators=[('gb',\n",
      "                              GradientBoostingClassifier(max_depth=12,\n",
      "                                                         min_samples_split=12,\n",
      "                                                         n_estimators=5,\n",
      "                                                         random_state=3)),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(max_depth=12,\n",
      "                                                     n_estimators=10,\n",
      "                                                     random_state=3)),\n",
      "                             ('xgb',\n",
      "                              XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "                                            colsample_bylevel=1,\n",
      "                                            colsample_bynode=1,\n",
      "                                            colsample_bytree=1,\n",
      "                                            enable_categorical=False, gamma=0,\n",
      "                                            gpu...\n",
      "                                            interaction_constraints='',\n",
      "                                            learning_rate=0.300000012,\n",
      "                                            max_delta_step=0, max_depth=12,\n",
      "                                            max_leaf_nodes=None,\n",
      "                                            min_child_weight=1,\n",
      "                                            min_samples_split=1, missing=nan,\n",
      "                                            monotone_constraints='()',\n",
      "                                            n_estimators=10, n_jobs=8,\n",
      "                                            num_parallel_tree=1,\n",
      "                                            predictor='auto', random_state=3,\n",
      "                                            reg_alpha=0, reg_lambda=1,\n",
      "                                            scale_pos_weight=1, subsample=1,\n",
      "                                            tree_method='exact',\n",
      "                                            validate_parameters=1,\n",
      "                                            verbosity=None))])\n",
      "kfold  6  score: 92.56%\n",
      "kfold  6  score: 95.55%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21:47:29] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:47:29] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "VotingClassifier(estimators=[('gb',\n",
      "                              GradientBoostingClassifier(max_depth=12,\n",
      "                                                         min_samples_split=12,\n",
      "                                                         n_estimators=5,\n",
      "                                                         random_state=3)),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(max_depth=12,\n",
      "                                                     n_estimators=10,\n",
      "                                                     random_state=3)),\n",
      "                             ('xgb',\n",
      "                              XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "                                            colsample_bylevel=1,\n",
      "                                            colsample_bynode=1,\n",
      "                                            colsample_bytree=1,\n",
      "                                            enable_categorical=False, gamma=0,\n",
      "                                            gpu...\n",
      "                                            interaction_constraints='',\n",
      "                                            learning_rate=0.300000012,\n",
      "                                            max_delta_step=0, max_depth=12,\n",
      "                                            max_leaf_nodes=None,\n",
      "                                            min_child_weight=1,\n",
      "                                            min_samples_split=1, missing=nan,\n",
      "                                            monotone_constraints='()',\n",
      "                                            n_estimators=10, n_jobs=8,\n",
      "                                            num_parallel_tree=1,\n",
      "                                            predictor='auto', random_state=3,\n",
      "                                            reg_alpha=0, reg_lambda=1,\n",
      "                                            scale_pos_weight=1, subsample=1,\n",
      "                                            tree_method='exact',\n",
      "                                            validate_parameters=1,\n",
      "                                            verbosity=None))])\n",
      "kfold  6  score: 91.73%\n",
      "kfold  6  score: 96.73%\n",
      "[21:47:30] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:47:30] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "VotingClassifier(estimators=[('gb',\n",
      "                              GradientBoostingClassifier(max_depth=12,\n",
      "                                                         min_samples_split=12,\n",
      "                                                         n_estimators=5,\n",
      "                                                         random_state=3)),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(max_depth=12,\n",
      "                                                     n_estimators=10,\n",
      "                                                     random_state=3)),\n",
      "                             ('xgb',\n",
      "                              XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "                                            colsample_bylevel=1,\n",
      "                                            colsample_bynode=1,\n",
      "                                            colsample_bytree=1,\n",
      "                                            enable_categorical=False, gamma=0,\n",
      "                                            gpu...\n",
      "                                            interaction_constraints='',\n",
      "                                            learning_rate=0.300000012,\n",
      "                                            max_delta_step=0, max_depth=12,\n",
      "                                            max_leaf_nodes=None,\n",
      "                                            min_child_weight=1,\n",
      "                                            min_samples_split=1, missing=nan,\n",
      "                                            monotone_constraints='()',\n",
      "                                            n_estimators=10, n_jobs=8,\n",
      "                                            num_parallel_tree=1,\n",
      "                                            predictor='auto', random_state=3,\n",
      "                                            reg_alpha=0, reg_lambda=1,\n",
      "                                            scale_pos_weight=1, subsample=1,\n",
      "                                            tree_method='exact',\n",
      "                                            validate_parameters=1,\n",
      "                                            verbosity=None))])\n",
      "kfold  6  score: 89.52%\n",
      "kfold  6  score: 95.88%\n",
      "[21:47:30] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:47:30] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "VotingClassifier(estimators=[('gb',\n",
      "                              GradientBoostingClassifier(max_depth=12,\n",
      "                                                         min_samples_split=12,\n",
      "                                                         n_estimators=5,\n",
      "                                                         random_state=3)),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(max_depth=12,\n",
      "                                                     n_estimators=10,\n",
      "                                                     random_state=3)),\n",
      "                             ('xgb',\n",
      "                              XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "                                            colsample_bylevel=1,\n",
      "                                            colsample_bynode=1,\n",
      "                                            colsample_bytree=1,\n",
      "                                            enable_categorical=False, gamma=0,\n",
      "                                            gpu...\n",
      "                                            interaction_constraints='',\n",
      "                                            learning_rate=0.300000012,\n",
      "                                            max_delta_step=0, max_depth=12,\n",
      "                                            max_leaf_nodes=None,\n",
      "                                            min_child_weight=1,\n",
      "                                            min_samples_split=1, missing=nan,\n",
      "                                            monotone_constraints='()',\n",
      "                                            n_estimators=10, n_jobs=8,\n",
      "                                            num_parallel_tree=1,\n",
      "                                            predictor='auto', random_state=3,\n",
      "                                            reg_alpha=0, reg_lambda=1,\n",
      "                                            scale_pos_weight=1, subsample=1,\n",
      "                                            tree_method='exact',\n",
      "                                            validate_parameters=1,\n",
      "                                            verbosity=None))])\n",
      "kfold  6  score: 91.94%\n",
      "kfold  6  score: 96.22%\n",
      "Average score over k-fold 91.28% (+/-1.14%) [Ensemble Majority]\n",
      "./Files_Majority_vote/128_features/filter7.csv\n",
      "n_estimators = 100 \n",
      "\n",
      "10-fold cross validation for file :  7\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GradientBoostingClassifier(max_depth=12, min_samples_split=12, n_estimators=5,\n",
      "                           random_state=3)\n",
      "kfold  7  score: 89.12%\n",
      "kfold  7  score: 93.89%\n",
      "GradientBoostingClassifier(max_depth=12, min_samples_split=12, n_estimators=5,\n",
      "                           random_state=3)\n",
      "kfold  7  score: 91.79%\n",
      "kfold  7  score: 94.42%\n",
      "GradientBoostingClassifier(max_depth=12, min_samples_split=12, n_estimators=5,\n",
      "                           random_state=3)\n",
      "kfold  7  score: 92.56%\n",
      "kfold  7  score: 93.95%\n",
      "GradientBoostingClassifier(max_depth=12, min_samples_split=12, n_estimators=5,\n",
      "                           random_state=3)\n",
      "kfold  7  score: 87.60%\n",
      "kfold  7  score: 92.42%\n",
      "GradientBoostingClassifier(max_depth=12, min_samples_split=12, n_estimators=5,\n",
      "                           random_state=3)\n",
      "kfold  7  score: 87.79%\n",
      "kfold  7  score: 93.55%\n",
      "GradientBoostingClassifier(max_depth=12, min_samples_split=12, n_estimators=5,\n",
      "                           random_state=3)\n",
      "kfold  7  score: 91.79%\n",
      "kfold  7  score: 94.40%\n",
      "GradientBoostingClassifier(max_depth=12, min_samples_split=12, n_estimators=5,\n",
      "                           random_state=3)\n",
      "kfold  7  score: 91.78%\n",
      "kfold  7  score: 92.83%\n",
      "GradientBoostingClassifier(max_depth=12, min_samples_split=12, n_estimators=5,\n",
      "                           random_state=3)\n",
      "kfold  7  score: 90.25%\n",
      "kfold  7  score: 94.44%\n",
      "GradientBoostingClassifier(max_depth=12, min_samples_split=12, n_estimators=5,\n",
      "                           random_state=3)\n",
      "kfold  7  score: 90.25%\n",
      "kfold  7  score: 93.89%\n",
      "GradientBoostingClassifier(max_depth=12, min_samples_split=12, n_estimators=5,\n",
      "                           random_state=3)\n",
      "kfold  7  score: 88.34%\n",
      "kfold  7  score: 92.93%\n",
      "Average score over k-fold 90.13% (+/-1.74%) [Gradient Boosting]\n",
      "RandomForestClassifier(max_depth=12, n_estimators=10, random_state=3)\n",
      "kfold  7  score: 91.98%\n",
      "kfold  7  score: 97.30%\n",
      "RandomForestClassifier(max_depth=12, n_estimators=10, random_state=3)\n",
      "kfold  7  score: 94.08%\n",
      "kfold  7  score: 98.49%\n",
      "RandomForestClassifier(max_depth=12, n_estimators=10, random_state=3)\n",
      "kfold  7  score: 92.56%\n",
      "kfold  7  score: 96.48%\n",
      "RandomForestClassifier(max_depth=12, n_estimators=10, random_state=3)\n",
      "kfold  7  score: 91.98%\n",
      "kfold  7  score: 97.30%\n",
      "RandomForestClassifier(max_depth=12, n_estimators=10, random_state=3)\n",
      "kfold  7  score: 93.13%\n",
      "kfold  7  score: 98.49%\n",
      "RandomForestClassifier(max_depth=12, n_estimators=10, random_state=3)\n",
      "kfold  7  score: 94.08%\n",
      "kfold  7  score: 96.97%\n",
      "RandomForestClassifier(max_depth=12, n_estimators=10, random_state=3)\n",
      "kfold  7  score: 94.65%\n",
      "kfold  7  score: 97.52%\n",
      "RandomForestClassifier(max_depth=12, n_estimators=10, random_state=3)\n",
      "kfold  7  score: 91.97%\n",
      "kfold  7  score: 96.97%\n",
      "RandomForestClassifier(max_depth=12, n_estimators=10, random_state=3)\n",
      "kfold  7  score: 92.54%\n",
      "kfold  7  score: 98.75%\n",
      "RandomForestClassifier(max_depth=12, n_estimators=10, random_state=3)\n",
      "kfold  7  score: 93.12%\n",
      "kfold  7  score: 97.67%\n",
      "Average score over k-fold 93.01% (+/-0.93%) [Random Forest]\n",
      "[21:47:34] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:47:34] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
      "              gamma=0, gpu_id=-1, importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.300000012,\n",
      "              max_delta_step=0, max_depth=12, max_leaf_nodes=None,\n",
      "              min_child_weight=1, min_samples_split=1, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=10, n_jobs=8,\n",
      "              num_parallel_tree=1, predictor='auto', random_state=3,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)\n",
      "kfold  7  score: 93.51%\n",
      "kfold  7  score: 99.13%\n",
      "[21:47:34] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:47:34] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
      "              gamma=0, gpu_id=-1, importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.300000012,\n",
      "              max_delta_step=0, max_depth=12, max_leaf_nodes=None,\n",
      "              min_child_weight=1, min_samples_split=1, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=10, n_jobs=8,\n",
      "              num_parallel_tree=1, predictor='auto', random_state=3,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)\n",
      "kfold  7  score: 95.04%\n",
      "kfold  7  score: 99.28%\n",
      "[21:47:34] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:47:34] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
      "              gamma=0, gpu_id=-1, importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.300000012,\n",
      "              max_delta_step=0, max_depth=12, max_leaf_nodes=None,\n",
      "              min_child_weight=1, min_samples_split=1, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=10, n_jobs=8,\n",
      "              num_parallel_tree=1, predictor='auto', random_state=3,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)\n",
      "kfold  7  score: 94.85%\n",
      "kfold  7  score: 99.32%\n",
      "[21:47:34] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:47:34] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
      "              gamma=0, gpu_id=-1, importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.300000012,\n",
      "              max_delta_step=0, max_depth=12, max_leaf_nodes=None,\n",
      "              min_child_weight=1, min_samples_split=1, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=10, n_jobs=8,\n",
      "              num_parallel_tree=1, predictor='auto', random_state=3,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)\n",
      "kfold  7  score: 92.94%\n",
      "kfold  7  score: 99.45%\n",
      "[21:47:34] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:47:34] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
      "              gamma=0, gpu_id=-1, importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.300000012,\n",
      "              max_delta_step=0, max_depth=12, max_leaf_nodes=None,\n",
      "              min_child_weight=1, min_samples_split=1, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=10, n_jobs=8,\n",
      "              num_parallel_tree=1, predictor='auto', random_state=3,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)\n",
      "kfold  7  score: 92.75%\n",
      "kfold  7  score: 99.49%\n",
      "[21:47:34] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:47:34] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
      "              gamma=0, gpu_id=-1, importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.300000012,\n",
      "              max_delta_step=0, max_depth=12, max_leaf_nodes=None,\n",
      "              min_child_weight=1, min_samples_split=1, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=10, n_jobs=8,\n",
      "              num_parallel_tree=1, predictor='auto', random_state=3,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)\n",
      "kfold  7  score: 94.85%\n",
      "kfold  7  score: 99.51%\n",
      "[21:47:34] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:47:34] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
      "              gamma=0, gpu_id=-1, importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.300000012,\n",
      "              max_delta_step=0, max_depth=12, max_leaf_nodes=None,\n",
      "              min_child_weight=1, min_samples_split=1, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=10, n_jobs=8,\n",
      "              num_parallel_tree=1, predictor='auto', random_state=3,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)\n",
      "kfold  7  score: 96.56%\n",
      "kfold  7  score: 99.19%\n",
      "[21:47:35] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:47:35] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
      "              gamma=0, gpu_id=-1, importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.300000012,\n",
      "              max_delta_step=0, max_depth=12, max_leaf_nodes=None,\n",
      "              min_child_weight=1, min_samples_split=1, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=10, n_jobs=8,\n",
      "              num_parallel_tree=1, predictor='auto', random_state=3,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)\n",
      "kfold  7  score: 95.22%\n",
      "kfold  7  score: 99.30%\n",
      "[21:47:35] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:47:35] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
      "              gamma=0, gpu_id=-1, importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.300000012,\n",
      "              max_delta_step=0, max_depth=12, max_leaf_nodes=None,\n",
      "              min_child_weight=1, min_samples_split=1, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=10, n_jobs=8,\n",
      "              num_parallel_tree=1, predictor='auto', random_state=3,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)\n",
      "kfold  7  score: 92.93%\n",
      "kfold  7  score: 99.53%\n",
      "[21:47:35] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:47:35] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
      "              gamma=0, gpu_id=-1, importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.300000012,\n",
      "              max_delta_step=0, max_depth=12, max_leaf_nodes=None,\n",
      "              min_child_weight=1, min_samples_split=1, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=10, n_jobs=8,\n",
      "              num_parallel_tree=1, predictor='auto', random_state=3,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)\n",
      "kfold  7  score: 93.88%\n",
      "kfold  7  score: 99.43%\n",
      "Average score over k-fold 94.25% (+/-1.18%) [XGBClassifier]\n",
      "[21:47:35] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:47:35] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "VotingClassifier(estimators=[('gb',\n",
      "                              GradientBoostingClassifier(max_depth=12,\n",
      "                                                         min_samples_split=12,\n",
      "                                                         n_estimators=5,\n",
      "                                                         random_state=3)),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(max_depth=12,\n",
      "                                                     n_estimators=10,\n",
      "                                                     random_state=3)),\n",
      "                             ('xgb',\n",
      "                              XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "                                            colsample_bylevel=1,\n",
      "                                            colsample_bynode=1,\n",
      "                                            colsample_bytree=1,\n",
      "                                            enable_categorical=False, gamma=0,\n",
      "                                            gpu...\n",
      "                                            interaction_constraints='',\n",
      "                                            learning_rate=0.300000012,\n",
      "                                            max_delta_step=0, max_depth=12,\n",
      "                                            max_leaf_nodes=None,\n",
      "                                            min_child_weight=1,\n",
      "                                            min_samples_split=1, missing=nan,\n",
      "                                            monotone_constraints='()',\n",
      "                                            n_estimators=10, n_jobs=8,\n",
      "                                            num_parallel_tree=1,\n",
      "                                            predictor='auto', random_state=3,\n",
      "                                            reg_alpha=0, reg_lambda=1,\n",
      "                                            scale_pos_weight=1, subsample=1,\n",
      "                                            tree_method='exact',\n",
      "                                            validate_parameters=1,\n",
      "                                            verbosity=None))])\n",
      "kfold  7  score: 92.18%\n",
      "kfold  7  score: 97.62%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21:47:36] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:47:36] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "VotingClassifier(estimators=[('gb',\n",
      "                              GradientBoostingClassifier(max_depth=12,\n",
      "                                                         min_samples_split=12,\n",
      "                                                         n_estimators=5,\n",
      "                                                         random_state=3)),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(max_depth=12,\n",
      "                                                     n_estimators=10,\n",
      "                                                     random_state=3)),\n",
      "                             ('xgb',\n",
      "                              XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "                                            colsample_bylevel=1,\n",
      "                                            colsample_bynode=1,\n",
      "                                            colsample_bytree=1,\n",
      "                                            enable_categorical=False, gamma=0,\n",
      "                                            gpu...\n",
      "                                            interaction_constraints='',\n",
      "                                            learning_rate=0.300000012,\n",
      "                                            max_delta_step=0, max_depth=12,\n",
      "                                            max_leaf_nodes=None,\n",
      "                                            min_child_weight=1,\n",
      "                                            min_samples_split=1, missing=nan,\n",
      "                                            monotone_constraints='()',\n",
      "                                            n_estimators=10, n_jobs=8,\n",
      "                                            num_parallel_tree=1,\n",
      "                                            predictor='auto', random_state=3,\n",
      "                                            reg_alpha=0, reg_lambda=1,\n",
      "                                            scale_pos_weight=1, subsample=1,\n",
      "                                            tree_method='exact',\n",
      "                                            validate_parameters=1,\n",
      "                                            verbosity=None))])\n",
      "kfold  7  score: 95.23%\n",
      "kfold  7  score: 98.68%\n",
      "[21:47:36] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:47:36] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "VotingClassifier(estimators=[('gb',\n",
      "                              GradientBoostingClassifier(max_depth=12,\n",
      "                                                         min_samples_split=12,\n",
      "                                                         n_estimators=5,\n",
      "                                                         random_state=3)),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(max_depth=12,\n",
      "                                                     n_estimators=10,\n",
      "                                                     random_state=3)),\n",
      "                             ('xgb',\n",
      "                              XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "                                            colsample_bylevel=1,\n",
      "                                            colsample_bynode=1,\n",
      "                                            colsample_bytree=1,\n",
      "                                            enable_categorical=False, gamma=0,\n",
      "                                            gpu...\n",
      "                                            interaction_constraints='',\n",
      "                                            learning_rate=0.300000012,\n",
      "                                            max_delta_step=0, max_depth=12,\n",
      "                                            max_leaf_nodes=None,\n",
      "                                            min_child_weight=1,\n",
      "                                            min_samples_split=1, missing=nan,\n",
      "                                            monotone_constraints='()',\n",
      "                                            n_estimators=10, n_jobs=8,\n",
      "                                            num_parallel_tree=1,\n",
      "                                            predictor='auto', random_state=3,\n",
      "                                            reg_alpha=0, reg_lambda=1,\n",
      "                                            scale_pos_weight=1, subsample=1,\n",
      "                                            tree_method='exact',\n",
      "                                            validate_parameters=1,\n",
      "                                            verbosity=None))])\n",
      "kfold  7  score: 95.23%\n",
      "kfold  7  score: 97.84%\n",
      "[21:47:37] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:47:37] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "VotingClassifier(estimators=[('gb',\n",
      "                              GradientBoostingClassifier(max_depth=12,\n",
      "                                                         min_samples_split=12,\n",
      "                                                         n_estimators=5,\n",
      "                                                         random_state=3)),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(max_depth=12,\n",
      "                                                     n_estimators=10,\n",
      "                                                     random_state=3)),\n",
      "                             ('xgb',\n",
      "                              XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "                                            colsample_bylevel=1,\n",
      "                                            colsample_bynode=1,\n",
      "                                            colsample_bytree=1,\n",
      "                                            enable_categorical=False, gamma=0,\n",
      "                                            gpu...\n",
      "                                            interaction_constraints='',\n",
      "                                            learning_rate=0.300000012,\n",
      "                                            max_delta_step=0, max_depth=12,\n",
      "                                            max_leaf_nodes=None,\n",
      "                                            min_child_weight=1,\n",
      "                                            min_samples_split=1, missing=nan,\n",
      "                                            monotone_constraints='()',\n",
      "                                            n_estimators=10, n_jobs=8,\n",
      "                                            num_parallel_tree=1,\n",
      "                                            predictor='auto', random_state=3,\n",
      "                                            reg_alpha=0, reg_lambda=1,\n",
      "                                            scale_pos_weight=1, subsample=1,\n",
      "                                            tree_method='exact',\n",
      "                                            validate_parameters=1,\n",
      "                                            verbosity=None))])\n",
      "kfold  7  score: 91.22%\n",
      "kfold  7  score: 97.28%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21:47:37] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:47:37] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "VotingClassifier(estimators=[('gb',\n",
      "                              GradientBoostingClassifier(max_depth=12,\n",
      "                                                         min_samples_split=12,\n",
      "                                                         n_estimators=5,\n",
      "                                                         random_state=3)),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(max_depth=12,\n",
      "                                                     n_estimators=10,\n",
      "                                                     random_state=3)),\n",
      "                             ('xgb',\n",
      "                              XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "                                            colsample_bylevel=1,\n",
      "                                            colsample_bynode=1,\n",
      "                                            colsample_bytree=1,\n",
      "                                            enable_categorical=False, gamma=0,\n",
      "                                            gpu...\n",
      "                                            interaction_constraints='',\n",
      "                                            learning_rate=0.300000012,\n",
      "                                            max_delta_step=0, max_depth=12,\n",
      "                                            max_leaf_nodes=None,\n",
      "                                            min_child_weight=1,\n",
      "                                            min_samples_split=1, missing=nan,\n",
      "                                            monotone_constraints='()',\n",
      "                                            n_estimators=10, n_jobs=8,\n",
      "                                            num_parallel_tree=1,\n",
      "                                            predictor='auto', random_state=3,\n",
      "                                            reg_alpha=0, reg_lambda=1,\n",
      "                                            scale_pos_weight=1, subsample=1,\n",
      "                                            tree_method='exact',\n",
      "                                            validate_parameters=1,\n",
      "                                            verbosity=None))])\n",
      "kfold  7  score: 92.75%\n",
      "kfold  7  score: 98.60%\n",
      "[21:47:38] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:47:38] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "VotingClassifier(estimators=[('gb',\n",
      "                              GradientBoostingClassifier(max_depth=12,\n",
      "                                                         min_samples_split=12,\n",
      "                                                         n_estimators=5,\n",
      "                                                         random_state=3)),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(max_depth=12,\n",
      "                                                     n_estimators=10,\n",
      "                                                     random_state=3)),\n",
      "                             ('xgb',\n",
      "                              XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "                                            colsample_bylevel=1,\n",
      "                                            colsample_bynode=1,\n",
      "                                            colsample_bytree=1,\n",
      "                                            enable_categorical=False, gamma=0,\n",
      "                                            gpu...\n",
      "                                            interaction_constraints='',\n",
      "                                            learning_rate=0.300000012,\n",
      "                                            max_delta_step=0, max_depth=12,\n",
      "                                            max_leaf_nodes=None,\n",
      "                                            min_child_weight=1,\n",
      "                                            min_samples_split=1, missing=nan,\n",
      "                                            monotone_constraints='()',\n",
      "                                            n_estimators=10, n_jobs=8,\n",
      "                                            num_parallel_tree=1,\n",
      "                                            predictor='auto', random_state=3,\n",
      "                                            reg_alpha=0, reg_lambda=1,\n",
      "                                            scale_pos_weight=1, subsample=1,\n",
      "                                            tree_method='exact',\n",
      "                                            validate_parameters=1,\n",
      "                                            verbosity=None))])\n",
      "kfold  7  score: 94.27%\n",
      "kfold  7  score: 97.77%\n",
      "[21:47:38] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:47:38] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "VotingClassifier(estimators=[('gb',\n",
      "                              GradientBoostingClassifier(max_depth=12,\n",
      "                                                         min_samples_split=12,\n",
      "                                                         n_estimators=5,\n",
      "                                                         random_state=3)),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(max_depth=12,\n",
      "                                                     n_estimators=10,\n",
      "                                                     random_state=3)),\n",
      "                             ('xgb',\n",
      "                              XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "                                            colsample_bylevel=1,\n",
      "                                            colsample_bynode=1,\n",
      "                                            colsample_bytree=1,\n",
      "                                            enable_categorical=False, gamma=0,\n",
      "                                            gpu...\n",
      "                                            interaction_constraints='',\n",
      "                                            learning_rate=0.300000012,\n",
      "                                            max_delta_step=0, max_depth=12,\n",
      "                                            max_leaf_nodes=None,\n",
      "                                            min_child_weight=1,\n",
      "                                            min_samples_split=1, missing=nan,\n",
      "                                            monotone_constraints='()',\n",
      "                                            n_estimators=10, n_jobs=8,\n",
      "                                            num_parallel_tree=1,\n",
      "                                            predictor='auto', random_state=3,\n",
      "                                            reg_alpha=0, reg_lambda=1,\n",
      "                                            scale_pos_weight=1, subsample=1,\n",
      "                                            tree_method='exact',\n",
      "                                            validate_parameters=1,\n",
      "                                            verbosity=None))])\n",
      "kfold  7  score: 94.65%\n",
      "kfold  7  score: 97.56%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21:47:39] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:47:39] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "VotingClassifier(estimators=[('gb',\n",
      "                              GradientBoostingClassifier(max_depth=12,\n",
      "                                                         min_samples_split=12,\n",
      "                                                         n_estimators=5,\n",
      "                                                         random_state=3)),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(max_depth=12,\n",
      "                                                     n_estimators=10,\n",
      "                                                     random_state=3)),\n",
      "                             ('xgb',\n",
      "                              XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "                                            colsample_bylevel=1,\n",
      "                                            colsample_bynode=1,\n",
      "                                            colsample_bytree=1,\n",
      "                                            enable_categorical=False, gamma=0,\n",
      "                                            gpu...\n",
      "                                            interaction_constraints='',\n",
      "                                            learning_rate=0.300000012,\n",
      "                                            max_delta_step=0, max_depth=12,\n",
      "                                            max_leaf_nodes=None,\n",
      "                                            min_child_weight=1,\n",
      "                                            min_samples_split=1, missing=nan,\n",
      "                                            monotone_constraints='()',\n",
      "                                            n_estimators=10, n_jobs=8,\n",
      "                                            num_parallel_tree=1,\n",
      "                                            predictor='auto', random_state=3,\n",
      "                                            reg_alpha=0, reg_lambda=1,\n",
      "                                            scale_pos_weight=1, subsample=1,\n",
      "                                            tree_method='exact',\n",
      "                                            validate_parameters=1,\n",
      "                                            verbosity=None))])\n",
      "kfold  7  score: 93.12%\n",
      "kfold  7  score: 97.69%\n",
      "[21:47:39] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:47:39] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "VotingClassifier(estimators=[('gb',\n",
      "                              GradientBoostingClassifier(max_depth=12,\n",
      "                                                         min_samples_split=12,\n",
      "                                                         n_estimators=5,\n",
      "                                                         random_state=3)),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(max_depth=12,\n",
      "                                                     n_estimators=10,\n",
      "                                                     random_state=3)),\n",
      "                             ('xgb',\n",
      "                              XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "                                            colsample_bylevel=1,\n",
      "                                            colsample_bynode=1,\n",
      "                                            colsample_bytree=1,\n",
      "                                            enable_categorical=False, gamma=0,\n",
      "                                            gpu...\n",
      "                                            interaction_constraints='',\n",
      "                                            learning_rate=0.300000012,\n",
      "                                            max_delta_step=0, max_depth=12,\n",
      "                                            max_leaf_nodes=None,\n",
      "                                            min_child_weight=1,\n",
      "                                            min_samples_split=1, missing=nan,\n",
      "                                            monotone_constraints='()',\n",
      "                                            n_estimators=10, n_jobs=8,\n",
      "                                            num_parallel_tree=1,\n",
      "                                            predictor='auto', random_state=3,\n",
      "                                            reg_alpha=0, reg_lambda=1,\n",
      "                                            scale_pos_weight=1, subsample=1,\n",
      "                                            tree_method='exact',\n",
      "                                            validate_parameters=1,\n",
      "                                            verbosity=None))])\n",
      "kfold  7  score: 92.93%\n",
      "kfold  7  score: 98.81%\n",
      "[21:47:40] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:47:40] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "VotingClassifier(estimators=[('gb',\n",
      "                              GradientBoostingClassifier(max_depth=12,\n",
      "                                                         min_samples_split=12,\n",
      "                                                         n_estimators=5,\n",
      "                                                         random_state=3)),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(max_depth=12,\n",
      "                                                     n_estimators=10,\n",
      "                                                     random_state=3)),\n",
      "                             ('xgb',\n",
      "                              XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "                                            colsample_bylevel=1,\n",
      "                                            colsample_bynode=1,\n",
      "                                            colsample_bytree=1,\n",
      "                                            enable_categorical=False, gamma=0,\n",
      "                                            gpu...\n",
      "                                            interaction_constraints='',\n",
      "                                            learning_rate=0.300000012,\n",
      "                                            max_delta_step=0, max_depth=12,\n",
      "                                            max_leaf_nodes=None,\n",
      "                                            min_child_weight=1,\n",
      "                                            min_samples_split=1, missing=nan,\n",
      "                                            monotone_constraints='()',\n",
      "                                            n_estimators=10, n_jobs=8,\n",
      "                                            num_parallel_tree=1,\n",
      "                                            predictor='auto', random_state=3,\n",
      "                                            reg_alpha=0, reg_lambda=1,\n",
      "                                            scale_pos_weight=1, subsample=1,\n",
      "                                            tree_method='exact',\n",
      "                                            validate_parameters=1,\n",
      "                                            verbosity=None))])\n",
      "kfold  7  score: 92.93%\n",
      "kfold  7  score: 97.71%\n",
      "Average score over k-fold 93.45% (+/-1.27%) [Ensemble Majority]\n",
      "./Files_Majority_vote/128_features/filter8.csv\n",
      "n_estimators = 100 \n",
      "\n",
      "10-fold cross validation for file :  8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GradientBoostingClassifier(max_depth=12, min_samples_split=12, n_estimators=5,\n",
      "                           random_state=3)\n",
      "kfold  8  score: 93.16%\n",
      "kfold  8  score: 97.76%\n",
      "GradientBoostingClassifier(max_depth=12, min_samples_split=12, n_estimators=5,\n",
      "                           random_state=3)\n",
      "kfold  8  score: 94.16%\n",
      "kfold  8  score: 97.83%\n",
      "GradientBoostingClassifier(max_depth=12, min_samples_split=12, n_estimators=5,\n",
      "                           random_state=3)\n",
      "kfold  8  score: 94.97%\n",
      "kfold  8  score: 97.67%\n",
      "GradientBoostingClassifier(max_depth=12, min_samples_split=12, n_estimators=5,\n",
      "                           random_state=3)\n",
      "kfold  8  score: 93.36%\n",
      "kfold  8  score: 97.76%\n",
      "GradientBoostingClassifier(max_depth=12, min_samples_split=12, n_estimators=5,\n",
      "                           random_state=3)\n",
      "kfold  8  score: 94.97%\n",
      "kfold  8  score: 98.26%\n",
      "GradientBoostingClassifier(max_depth=12, min_samples_split=12, n_estimators=5,\n",
      "                           random_state=3)\n",
      "kfold  8  score: 95.17%\n",
      "kfold  8  score: 97.18%\n",
      "GradientBoostingClassifier(max_depth=12, min_samples_split=12, n_estimators=5,\n",
      "                           random_state=3)\n",
      "kfold  8  score: 93.16%\n",
      "kfold  8  score: 97.99%\n",
      "GradientBoostingClassifier(max_depth=12, min_samples_split=12, n_estimators=5,\n",
      "                           random_state=3)\n",
      "kfold  8  score: 94.76%\n",
      "kfold  8  score: 98.28%\n",
      "GradientBoostingClassifier(max_depth=12, min_samples_split=12, n_estimators=5,\n",
      "                           random_state=3)\n",
      "kfold  8  score: 95.97%\n",
      "kfold  8  score: 97.74%\n",
      "GradientBoostingClassifier(max_depth=12, min_samples_split=12, n_estimators=5,\n",
      "                           random_state=3)\n",
      "kfold  8  score: 95.56%\n",
      "kfold  8  score: 97.14%\n",
      "Average score over k-fold 94.52% (+/-0.96%) [Gradient Boosting]\n",
      "RandomForestClassifier(max_depth=12, n_estimators=10, random_state=3)\n",
      "kfold  8  score: 95.77%\n",
      "kfold  8  score: 99.60%\n",
      "RandomForestClassifier(max_depth=12, n_estimators=10, random_state=3)\n",
      "kfold  8  score: 96.58%\n",
      "kfold  8  score: 99.49%\n",
      "RandomForestClassifier(max_depth=12, n_estimators=10, random_state=3)\n",
      "kfold  8  score: 96.98%\n",
      "kfold  8  score: 99.46%\n",
      "RandomForestClassifier(max_depth=12, n_estimators=10, random_state=3)\n",
      "kfold  8  score: 96.78%\n",
      "kfold  8  score: 99.49%\n",
      "RandomForestClassifier(max_depth=12, n_estimators=10, random_state=3)\n",
      "kfold  8  score: 95.57%\n",
      "kfold  8  score: 99.17%\n",
      "RandomForestClassifier(max_depth=12, n_estimators=10, random_state=3)\n",
      "kfold  8  score: 96.38%\n",
      "kfold  8  score: 99.19%\n",
      "RandomForestClassifier(max_depth=12, n_estimators=10, random_state=3)\n",
      "kfold  8  score: 95.77%\n",
      "kfold  8  score: 99.42%\n",
      "RandomForestClassifier(max_depth=12, n_estimators=10, random_state=3)\n",
      "kfold  8  score: 96.37%\n",
      "kfold  8  score: 99.37%\n",
      "RandomForestClassifier(max_depth=12, n_estimators=10, random_state=3)\n",
      "kfold  8  score: 97.98%\n",
      "kfold  8  score: 99.60%\n",
      "RandomForestClassifier(max_depth=12, n_estimators=10, random_state=3)\n",
      "kfold  8  score: 97.58%\n",
      "kfold  8  score: 99.55%\n",
      "Average score over k-fold 96.58% (+/-0.75%) [Random Forest]\n",
      "[21:47:43] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:47:43] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
      "              gamma=0, gpu_id=-1, importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.300000012,\n",
      "              max_delta_step=0, max_depth=12, max_leaf_nodes=None,\n",
      "              min_child_weight=1, min_samples_split=1, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=10, n_jobs=8,\n",
      "              num_parallel_tree=1, predictor='auto', random_state=3,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)\n",
      "kfold  8  score: 96.18%\n",
      "kfold  8  score: 99.62%\n",
      "[21:47:43] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:47:43] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
      "              gamma=0, gpu_id=-1, importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.300000012,\n",
      "              max_delta_step=0, max_depth=12, max_leaf_nodes=None,\n",
      "              min_child_weight=1, min_samples_split=1, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=10, n_jobs=8,\n",
      "              num_parallel_tree=1, predictor='auto', random_state=3,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)\n",
      "kfold  8  score: 96.98%\n",
      "kfold  8  score: 99.71%\n",
      "[21:47:43] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:47:43] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
      "              gamma=0, gpu_id=-1, importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.300000012,\n",
      "              max_delta_step=0, max_depth=12, max_leaf_nodes=None,\n",
      "              min_child_weight=1, min_samples_split=1, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=10, n_jobs=8,\n",
      "              num_parallel_tree=1, predictor='auto', random_state=3,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)\n",
      "kfold  8  score: 98.19%\n",
      "kfold  8  score: 99.66%\n",
      "[21:47:43] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:47:43] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
      "              gamma=0, gpu_id=-1, importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.300000012,\n",
      "              max_delta_step=0, max_depth=12, max_leaf_nodes=None,\n",
      "              min_child_weight=1, min_samples_split=1, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=10, n_jobs=8,\n",
      "              num_parallel_tree=1, predictor='auto', random_state=3,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)\n",
      "kfold  8  score: 97.59%\n",
      "kfold  8  score: 99.69%\n",
      "[21:47:43] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:47:43] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
      "              gamma=0, gpu_id=-1, importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.300000012,\n",
      "              max_delta_step=0, max_depth=12, max_leaf_nodes=None,\n",
      "              min_child_weight=1, min_samples_split=1, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=10, n_jobs=8,\n",
      "              num_parallel_tree=1, predictor='auto', random_state=3,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)\n",
      "kfold  8  score: 95.57%\n",
      "kfold  8  score: 99.66%\n",
      "[21:47:43] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:47:43] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
      "              gamma=0, gpu_id=-1, importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.300000012,\n",
      "              max_delta_step=0, max_depth=12, max_leaf_nodes=None,\n",
      "              min_child_weight=1, min_samples_split=1, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=10, n_jobs=8,\n",
      "              num_parallel_tree=1, predictor='auto', random_state=3,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)\n",
      "kfold  8  score: 97.38%\n",
      "kfold  8  score: 99.78%\n",
      "[21:47:44] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:47:44] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
      "              gamma=0, gpu_id=-1, importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.300000012,\n",
      "              max_delta_step=0, max_depth=12, max_leaf_nodes=None,\n",
      "              min_child_weight=1, min_samples_split=1, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=10, n_jobs=8,\n",
      "              num_parallel_tree=1, predictor='auto', random_state=3,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)\n",
      "kfold  8  score: 96.38%\n",
      "kfold  8  score: 99.69%\n",
      "[21:47:44] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:47:44] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
      "              gamma=0, gpu_id=-1, importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.300000012,\n",
      "              max_delta_step=0, max_depth=12, max_leaf_nodes=None,\n",
      "              min_child_weight=1, min_samples_split=1, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=10, n_jobs=8,\n",
      "              num_parallel_tree=1, predictor='auto', random_state=3,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)\n",
      "kfold  8  score: 96.57%\n",
      "kfold  8  score: 99.66%\n",
      "[21:47:44] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:47:44] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
      "              gamma=0, gpu_id=-1, importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.300000012,\n",
      "              max_delta_step=0, max_depth=12, max_leaf_nodes=None,\n",
      "              min_child_weight=1, min_samples_split=1, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=10, n_jobs=8,\n",
      "              num_parallel_tree=1, predictor='auto', random_state=3,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)\n",
      "kfold  8  score: 98.19%\n",
      "kfold  8  score: 99.82%\n",
      "[21:47:44] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:47:44] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
      "              gamma=0, gpu_id=-1, importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.300000012,\n",
      "              max_delta_step=0, max_depth=12, max_leaf_nodes=None,\n",
      "              min_child_weight=1, min_samples_split=1, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=10, n_jobs=8,\n",
      "              num_parallel_tree=1, predictor='auto', random_state=3,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)\n",
      "kfold  8  score: 98.79%\n",
      "kfold  8  score: 99.80%\n",
      "Average score over k-fold 97.18% (+/-0.97%) [XGBClassifier]\n",
      "[21:47:44] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:47:44] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "VotingClassifier(estimators=[('gb',\n",
      "                              GradientBoostingClassifier(max_depth=12,\n",
      "                                                         min_samples_split=12,\n",
      "                                                         n_estimators=5,\n",
      "                                                         random_state=3)),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(max_depth=12,\n",
      "                                                     n_estimators=10,\n",
      "                                                     random_state=3)),\n",
      "                             ('xgb',\n",
      "                              XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "                                            colsample_bylevel=1,\n",
      "                                            colsample_bynode=1,\n",
      "                                            colsample_bytree=1,\n",
      "                                            enable_categorical=False, gamma=0,\n",
      "                                            gpu...\n",
      "                                            interaction_constraints='',\n",
      "                                            learning_rate=0.300000012,\n",
      "                                            max_delta_step=0, max_depth=12,\n",
      "                                            max_leaf_nodes=None,\n",
      "                                            min_child_weight=1,\n",
      "                                            min_samples_split=1, missing=nan,\n",
      "                                            monotone_constraints='()',\n",
      "                                            n_estimators=10, n_jobs=8,\n",
      "                                            num_parallel_tree=1,\n",
      "                                            predictor='auto', random_state=3,\n",
      "                                            reg_alpha=0, reg_lambda=1,\n",
      "                                            scale_pos_weight=1, subsample=1,\n",
      "                                            tree_method='exact',\n",
      "                                            validate_parameters=1,\n",
      "                                            verbosity=None))])\n",
      "kfold  8  score: 95.98%\n",
      "kfold  8  score: 99.55%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21:47:45] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:47:45] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "VotingClassifier(estimators=[('gb',\n",
      "                              GradientBoostingClassifier(max_depth=12,\n",
      "                                                         min_samples_split=12,\n",
      "                                                         n_estimators=5,\n",
      "                                                         random_state=3)),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(max_depth=12,\n",
      "                                                     n_estimators=10,\n",
      "                                                     random_state=3)),\n",
      "                             ('xgb',\n",
      "                              XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "                                            colsample_bylevel=1,\n",
      "                                            colsample_bynode=1,\n",
      "                                            colsample_bytree=1,\n",
      "                                            enable_categorical=False, gamma=0,\n",
      "                                            gpu...\n",
      "                                            interaction_constraints='',\n",
      "                                            learning_rate=0.300000012,\n",
      "                                            max_delta_step=0, max_depth=12,\n",
      "                                            max_leaf_nodes=None,\n",
      "                                            min_child_weight=1,\n",
      "                                            min_samples_split=1, missing=nan,\n",
      "                                            monotone_constraints='()',\n",
      "                                            n_estimators=10, n_jobs=8,\n",
      "                                            num_parallel_tree=1,\n",
      "                                            predictor='auto', random_state=3,\n",
      "                                            reg_alpha=0, reg_lambda=1,\n",
      "                                            scale_pos_weight=1, subsample=1,\n",
      "                                            tree_method='exact',\n",
      "                                            validate_parameters=1,\n",
      "                                            verbosity=None))])\n",
      "kfold  8  score: 96.78%\n",
      "kfold  8  score: 99.62%\n",
      "[21:47:45] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:47:45] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "VotingClassifier(estimators=[('gb',\n",
      "                              GradientBoostingClassifier(max_depth=12,\n",
      "                                                         min_samples_split=12,\n",
      "                                                         n_estimators=5,\n",
      "                                                         random_state=3)),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(max_depth=12,\n",
      "                                                     n_estimators=10,\n",
      "                                                     random_state=3)),\n",
      "                             ('xgb',\n",
      "                              XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "                                            colsample_bylevel=1,\n",
      "                                            colsample_bynode=1,\n",
      "                                            colsample_bytree=1,\n",
      "                                            enable_categorical=False, gamma=0,\n",
      "                                            gpu...\n",
      "                                            interaction_constraints='',\n",
      "                                            learning_rate=0.300000012,\n",
      "                                            max_delta_step=0, max_depth=12,\n",
      "                                            max_leaf_nodes=None,\n",
      "                                            min_child_weight=1,\n",
      "                                            min_samples_split=1, missing=nan,\n",
      "                                            monotone_constraints='()',\n",
      "                                            n_estimators=10, n_jobs=8,\n",
      "                                            num_parallel_tree=1,\n",
      "                                            predictor='auto', random_state=3,\n",
      "                                            reg_alpha=0, reg_lambda=1,\n",
      "                                            scale_pos_weight=1, subsample=1,\n",
      "                                            tree_method='exact',\n",
      "                                            validate_parameters=1,\n",
      "                                            verbosity=None))])\n",
      "kfold  8  score: 97.38%\n",
      "kfold  8  score: 99.51%\n",
      "[21:47:46] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:47:46] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "VotingClassifier(estimators=[('gb',\n",
      "                              GradientBoostingClassifier(max_depth=12,\n",
      "                                                         min_samples_split=12,\n",
      "                                                         n_estimators=5,\n",
      "                                                         random_state=3)),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(max_depth=12,\n",
      "                                                     n_estimators=10,\n",
      "                                                     random_state=3)),\n",
      "                             ('xgb',\n",
      "                              XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "                                            colsample_bylevel=1,\n",
      "                                            colsample_bynode=1,\n",
      "                                            colsample_bytree=1,\n",
      "                                            enable_categorical=False, gamma=0,\n",
      "                                            gpu...\n",
      "                                            interaction_constraints='',\n",
      "                                            learning_rate=0.300000012,\n",
      "                                            max_delta_step=0, max_depth=12,\n",
      "                                            max_leaf_nodes=None,\n",
      "                                            min_child_weight=1,\n",
      "                                            min_samples_split=1, missing=nan,\n",
      "                                            monotone_constraints='()',\n",
      "                                            n_estimators=10, n_jobs=8,\n",
      "                                            num_parallel_tree=1,\n",
      "                                            predictor='auto', random_state=3,\n",
      "                                            reg_alpha=0, reg_lambda=1,\n",
      "                                            scale_pos_weight=1, subsample=1,\n",
      "                                            tree_method='exact',\n",
      "                                            validate_parameters=1,\n",
      "                                            verbosity=None))])\n",
      "kfold  8  score: 97.18%\n",
      "kfold  8  score: 99.57%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21:47:46] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:47:46] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "VotingClassifier(estimators=[('gb',\n",
      "                              GradientBoostingClassifier(max_depth=12,\n",
      "                                                         min_samples_split=12,\n",
      "                                                         n_estimators=5,\n",
      "                                                         random_state=3)),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(max_depth=12,\n",
      "                                                     n_estimators=10,\n",
      "                                                     random_state=3)),\n",
      "                             ('xgb',\n",
      "                              XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "                                            colsample_bylevel=1,\n",
      "                                            colsample_bynode=1,\n",
      "                                            colsample_bytree=1,\n",
      "                                            enable_categorical=False, gamma=0,\n",
      "                                            gpu...\n",
      "                                            interaction_constraints='',\n",
      "                                            learning_rate=0.300000012,\n",
      "                                            max_delta_step=0, max_depth=12,\n",
      "                                            max_leaf_nodes=None,\n",
      "                                            min_child_weight=1,\n",
      "                                            min_samples_split=1, missing=nan,\n",
      "                                            monotone_constraints='()',\n",
      "                                            n_estimators=10, n_jobs=8,\n",
      "                                            num_parallel_tree=1,\n",
      "                                            predictor='auto', random_state=3,\n",
      "                                            reg_alpha=0, reg_lambda=1,\n",
      "                                            scale_pos_weight=1, subsample=1,\n",
      "                                            tree_method='exact',\n",
      "                                            validate_parameters=1,\n",
      "                                            verbosity=None))])\n",
      "kfold  8  score: 95.98%\n",
      "kfold  8  score: 99.40%\n",
      "[21:47:47] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:47:47] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "VotingClassifier(estimators=[('gb',\n",
      "                              GradientBoostingClassifier(max_depth=12,\n",
      "                                                         min_samples_split=12,\n",
      "                                                         n_estimators=5,\n",
      "                                                         random_state=3)),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(max_depth=12,\n",
      "                                                     n_estimators=10,\n",
      "                                                     random_state=3)),\n",
      "                             ('xgb',\n",
      "                              XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "                                            colsample_bylevel=1,\n",
      "                                            colsample_bynode=1,\n",
      "                                            colsample_bytree=1,\n",
      "                                            enable_categorical=False, gamma=0,\n",
      "                                            gpu...\n",
      "                                            interaction_constraints='',\n",
      "                                            learning_rate=0.300000012,\n",
      "                                            max_delta_step=0, max_depth=12,\n",
      "                                            max_leaf_nodes=None,\n",
      "                                            min_child_weight=1,\n",
      "                                            min_samples_split=1, missing=nan,\n",
      "                                            monotone_constraints='()',\n",
      "                                            n_estimators=10, n_jobs=8,\n",
      "                                            num_parallel_tree=1,\n",
      "                                            predictor='auto', random_state=3,\n",
      "                                            reg_alpha=0, reg_lambda=1,\n",
      "                                            scale_pos_weight=1, subsample=1,\n",
      "                                            tree_method='exact',\n",
      "                                            validate_parameters=1,\n",
      "                                            verbosity=None))])\n",
      "kfold  8  score: 96.38%\n",
      "kfold  8  score: 99.40%\n",
      "[21:47:47] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:47:47] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "VotingClassifier(estimators=[('gb',\n",
      "                              GradientBoostingClassifier(max_depth=12,\n",
      "                                                         min_samples_split=12,\n",
      "                                                         n_estimators=5,\n",
      "                                                         random_state=3)),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(max_depth=12,\n",
      "                                                     n_estimators=10,\n",
      "                                                     random_state=3)),\n",
      "                             ('xgb',\n",
      "                              XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "                                            colsample_bylevel=1,\n",
      "                                            colsample_bynode=1,\n",
      "                                            colsample_bytree=1,\n",
      "                                            enable_categorical=False, gamma=0,\n",
      "                                            gpu...\n",
      "                                            interaction_constraints='',\n",
      "                                            learning_rate=0.300000012,\n",
      "                                            max_delta_step=0, max_depth=12,\n",
      "                                            max_leaf_nodes=None,\n",
      "                                            min_child_weight=1,\n",
      "                                            min_samples_split=1, missing=nan,\n",
      "                                            monotone_constraints='()',\n",
      "                                            n_estimators=10, n_jobs=8,\n",
      "                                            num_parallel_tree=1,\n",
      "                                            predictor='auto', random_state=3,\n",
      "                                            reg_alpha=0, reg_lambda=1,\n",
      "                                            scale_pos_weight=1, subsample=1,\n",
      "                                            tree_method='exact',\n",
      "                                            validate_parameters=1,\n",
      "                                            verbosity=None))])\n",
      "kfold  8  score: 95.57%\n",
      "kfold  8  score: 99.57%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21:47:48] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:47:48] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "VotingClassifier(estimators=[('gb',\n",
      "                              GradientBoostingClassifier(max_depth=12,\n",
      "                                                         min_samples_split=12,\n",
      "                                                         n_estimators=5,\n",
      "                                                         random_state=3)),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(max_depth=12,\n",
      "                                                     n_estimators=10,\n",
      "                                                     random_state=3)),\n",
      "                             ('xgb',\n",
      "                              XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "                                            colsample_bylevel=1,\n",
      "                                            colsample_bynode=1,\n",
      "                                            colsample_bytree=1,\n",
      "                                            enable_categorical=False, gamma=0,\n",
      "                                            gpu...\n",
      "                                            interaction_constraints='',\n",
      "                                            learning_rate=0.300000012,\n",
      "                                            max_delta_step=0, max_depth=12,\n",
      "                                            max_leaf_nodes=None,\n",
      "                                            min_child_weight=1,\n",
      "                                            min_samples_split=1, missing=nan,\n",
      "                                            monotone_constraints='()',\n",
      "                                            n_estimators=10, n_jobs=8,\n",
      "                                            num_parallel_tree=1,\n",
      "                                            predictor='auto', random_state=3,\n",
      "                                            reg_alpha=0, reg_lambda=1,\n",
      "                                            scale_pos_weight=1, subsample=1,\n",
      "                                            tree_method='exact',\n",
      "                                            validate_parameters=1,\n",
      "                                            verbosity=None))])\n",
      "kfold  8  score: 96.37%\n",
      "kfold  8  score: 99.60%\n",
      "[21:47:48] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:47:48] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "VotingClassifier(estimators=[('gb',\n",
      "                              GradientBoostingClassifier(max_depth=12,\n",
      "                                                         min_samples_split=12,\n",
      "                                                         n_estimators=5,\n",
      "                                                         random_state=3)),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(max_depth=12,\n",
      "                                                     n_estimators=10,\n",
      "                                                     random_state=3)),\n",
      "                             ('xgb',\n",
      "                              XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "                                            colsample_bylevel=1,\n",
      "                                            colsample_bynode=1,\n",
      "                                            colsample_bytree=1,\n",
      "                                            enable_categorical=False, gamma=0,\n",
      "                                            gpu...\n",
      "                                            interaction_constraints='',\n",
      "                                            learning_rate=0.300000012,\n",
      "                                            max_delta_step=0, max_depth=12,\n",
      "                                            max_leaf_nodes=None,\n",
      "                                            min_child_weight=1,\n",
      "                                            min_samples_split=1, missing=nan,\n",
      "                                            monotone_constraints='()',\n",
      "                                            n_estimators=10, n_jobs=8,\n",
      "                                            num_parallel_tree=1,\n",
      "                                            predictor='auto', random_state=3,\n",
      "                                            reg_alpha=0, reg_lambda=1,\n",
      "                                            scale_pos_weight=1, subsample=1,\n",
      "                                            tree_method='exact',\n",
      "                                            validate_parameters=1,\n",
      "                                            verbosity=None))])\n",
      "kfold  8  score: 98.19%\n",
      "kfold  8  score: 99.62%\n",
      "[21:47:48] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:47:48] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "VotingClassifier(estimators=[('gb',\n",
      "                              GradientBoostingClassifier(max_depth=12,\n",
      "                                                         min_samples_split=12,\n",
      "                                                         n_estimators=5,\n",
      "                                                         random_state=3)),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(max_depth=12,\n",
      "                                                     n_estimators=10,\n",
      "                                                     random_state=3)),\n",
      "                             ('xgb',\n",
      "                              XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "                                            colsample_bylevel=1,\n",
      "                                            colsample_bynode=1,\n",
      "                                            colsample_bytree=1,\n",
      "                                            enable_categorical=False, gamma=0,\n",
      "                                            gpu...\n",
      "                                            interaction_constraints='',\n",
      "                                            learning_rate=0.300000012,\n",
      "                                            max_delta_step=0, max_depth=12,\n",
      "                                            max_leaf_nodes=None,\n",
      "                                            min_child_weight=1,\n",
      "                                            min_samples_split=1, missing=nan,\n",
      "                                            monotone_constraints='()',\n",
      "                                            n_estimators=10, n_jobs=8,\n",
      "                                            num_parallel_tree=1,\n",
      "                                            predictor='auto', random_state=3,\n",
      "                                            reg_alpha=0, reg_lambda=1,\n",
      "                                            scale_pos_weight=1, subsample=1,\n",
      "                                            tree_method='exact',\n",
      "                                            validate_parameters=1,\n",
      "                                            verbosity=None))])\n",
      "kfold  8  score: 97.78%\n",
      "kfold  8  score: 99.58%\n",
      "Average score over k-fold 96.76% (+/-0.81%) [Ensemble Majority]\n",
      "./Files_Majority_vote/128_features/filter9.csv\n",
      "n_estimators = 100 \n",
      "\n",
      "10-fold cross validation for file :  9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GradientBoostingClassifier(max_depth=12, min_samples_split=12, n_estimators=5,\n",
      "                           random_state=3)\n",
      "kfold  9  score: 89.70%\n",
      "kfold  9  score: 92.16%\n",
      "GradientBoostingClassifier(max_depth=12, min_samples_split=12, n_estimators=5,\n",
      "                           random_state=3)\n",
      "kfold  9  score: 87.83%\n",
      "kfold  9  score: 90.76%\n",
      "GradientBoostingClassifier(max_depth=12, min_samples_split=12, n_estimators=5,\n",
      "                           random_state=3)\n",
      "kfold  9  score: 82.21%\n",
      "kfold  9  score: 86.37%\n",
      "GradientBoostingClassifier(max_depth=12, min_samples_split=12, n_estimators=5,\n",
      "                           random_state=3)\n",
      "kfold  9  score: 85.58%\n",
      "kfold  9  score: 90.20%\n",
      "GradientBoostingClassifier(max_depth=12, min_samples_split=12, n_estimators=5,\n",
      "                           random_state=3)\n",
      "kfold  9  score: 87.08%\n",
      "kfold  9  score: 90.03%\n",
      "GradientBoostingClassifier(max_depth=12, min_samples_split=12, n_estimators=5,\n",
      "                           random_state=3)\n",
      "kfold  9  score: 88.76%\n",
      "kfold  9  score: 92.88%\n",
      "GradientBoostingClassifier(max_depth=12, min_samples_split=12, n_estimators=5,\n",
      "                           random_state=3)\n",
      "kfold  9  score: 84.64%\n",
      "kfold  9  score: 87.08%\n",
      "GradientBoostingClassifier(max_depth=12, min_samples_split=12, n_estimators=5,\n",
      "                           random_state=3)\n",
      "kfold  9  score: 86.70%\n",
      "kfold  9  score: 90.57%\n",
      "GradientBoostingClassifier(max_depth=12, min_samples_split=12, n_estimators=5,\n",
      "                           random_state=3)\n",
      "kfold  9  score: 86.70%\n",
      "kfold  9  score: 89.06%\n",
      "GradientBoostingClassifier(max_depth=12, min_samples_split=12, n_estimators=5,\n",
      "                           random_state=3)\n",
      "kfold  9  score: 89.89%\n",
      "kfold  9  score: 92.41%\n",
      "Average score over k-fold 86.91% (+/-2.24%) [Gradient Boosting]\n",
      "RandomForestClassifier(max_depth=12, n_estimators=10, random_state=3)\n",
      "kfold  9  score: 89.51%\n",
      "kfold  9  score: 94.65%\n",
      "RandomForestClassifier(max_depth=12, n_estimators=10, random_state=3)\n",
      "kfold  9  score: 92.32%\n",
      "kfold  9  score: 96.84%\n",
      "RandomForestClassifier(max_depth=12, n_estimators=10, random_state=3)\n",
      "kfold  9  score: 87.27%\n",
      "kfold  9  score: 95.82%\n",
      "RandomForestClassifier(max_depth=12, n_estimators=10, random_state=3)\n",
      "kfold  9  score: 89.51%\n",
      "kfold  9  score: 96.09%\n",
      "RandomForestClassifier(max_depth=12, n_estimators=10, random_state=3)\n",
      "kfold  9  score: 91.76%\n",
      "kfold  9  score: 95.96%\n",
      "RandomForestClassifier(max_depth=12, n_estimators=10, random_state=3)\n",
      "kfold  9  score: 91.01%\n",
      "kfold  9  score: 96.03%\n",
      "RandomForestClassifier(max_depth=12, n_estimators=10, random_state=3)\n",
      "kfold  9  score: 89.33%\n",
      "kfold  9  score: 95.76%\n",
      "RandomForestClassifier(max_depth=12, n_estimators=10, random_state=3)\n",
      "kfold  9  score: 90.45%\n",
      "kfold  9  score: 96.42%\n",
      "RandomForestClassifier(max_depth=12, n_estimators=10, random_state=3)\n",
      "kfold  9  score: 89.89%\n",
      "kfold  9  score: 96.38%\n",
      "RandomForestClassifier(max_depth=12, n_estimators=10, random_state=3)\n",
      "kfold  9  score: 89.51%\n",
      "kfold  9  score: 95.76%\n",
      "Average score over k-fold 90.06% (+/-1.35%) [Random Forest]\n",
      "[21:47:52] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:47:52] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
      "              gamma=0, gpu_id=-1, importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.300000012,\n",
      "              max_delta_step=0, max_depth=12, max_leaf_nodes=None,\n",
      "              min_child_weight=1, min_samples_split=1, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=10, n_jobs=8,\n",
      "              num_parallel_tree=1, predictor='auto', random_state=3,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)\n",
      "kfold  9  score: 93.45%\n",
      "kfold  9  score: 98.88%\n",
      "[21:47:52] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:47:52] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
      "              gamma=0, gpu_id=-1, importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.300000012,\n",
      "              max_delta_step=0, max_depth=12, max_leaf_nodes=None,\n",
      "              min_child_weight=1, min_samples_split=1, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=10, n_jobs=8,\n",
      "              num_parallel_tree=1, predictor='auto', random_state=3,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)\n",
      "kfold  9  score: 92.88%\n",
      "kfold  9  score: 98.88%\n",
      "[21:47:53] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:47:53] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
      "              gamma=0, gpu_id=-1, importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.300000012,\n",
      "              max_delta_step=0, max_depth=12, max_leaf_nodes=None,\n",
      "              min_child_weight=1, min_samples_split=1, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=10, n_jobs=8,\n",
      "              num_parallel_tree=1, predictor='auto', random_state=3,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)\n",
      "kfold  9  score: 88.39%\n",
      "kfold  9  score: 97.96%\n",
      "[21:47:53] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:47:53] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
      "              gamma=0, gpu_id=-1, importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.300000012,\n",
      "              max_delta_step=0, max_depth=12, max_leaf_nodes=None,\n",
      "              min_child_weight=1, min_samples_split=1, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=10, n_jobs=8,\n",
      "              num_parallel_tree=1, predictor='auto', random_state=3,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)\n",
      "kfold  9  score: 91.39%\n",
      "kfold  9  score: 98.56%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21:47:53] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:47:53] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
      "              gamma=0, gpu_id=-1, importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.300000012,\n",
      "              max_delta_step=0, max_depth=12, max_leaf_nodes=None,\n",
      "              min_child_weight=1, min_samples_split=1, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=10, n_jobs=8,\n",
      "              num_parallel_tree=1, predictor='auto', random_state=3,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)\n",
      "kfold  9  score: 95.32%\n",
      "kfold  9  score: 98.77%\n",
      "[21:47:53] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:47:53] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
      "              gamma=0, gpu_id=-1, importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.300000012,\n",
      "              max_delta_step=0, max_depth=12, max_leaf_nodes=None,\n",
      "              min_child_weight=1, min_samples_split=1, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=10, n_jobs=8,\n",
      "              num_parallel_tree=1, predictor='auto', random_state=3,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)\n",
      "kfold  9  score: 91.39%\n",
      "kfold  9  score: 98.36%\n",
      "[21:47:53] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:47:53] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
      "              gamma=0, gpu_id=-1, importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.300000012,\n",
      "              max_delta_step=0, max_depth=12, max_leaf_nodes=None,\n",
      "              min_child_weight=1, min_samples_split=1, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=10, n_jobs=8,\n",
      "              num_parallel_tree=1, predictor='auto', random_state=3,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)\n",
      "kfold  9  score: 91.39%\n",
      "kfold  9  score: 98.73%\n",
      "[21:47:53] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:47:53] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
      "              gamma=0, gpu_id=-1, importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.300000012,\n",
      "              max_delta_step=0, max_depth=12, max_leaf_nodes=None,\n",
      "              min_child_weight=1, min_samples_split=1, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=10, n_jobs=8,\n",
      "              num_parallel_tree=1, predictor='auto', random_state=3,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)\n",
      "kfold  9  score: 92.32%\n",
      "kfold  9  score: 98.38%\n",
      "[21:47:53] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:47:53] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
      "              gamma=0, gpu_id=-1, importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.300000012,\n",
      "              max_delta_step=0, max_depth=12, max_leaf_nodes=None,\n",
      "              min_child_weight=1, min_samples_split=1, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=10, n_jobs=8,\n",
      "              num_parallel_tree=1, predictor='auto', random_state=3,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)\n",
      "kfold  9  score: 91.57%\n",
      "kfold  9  score: 98.98%\n",
      "[21:47:53] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:47:53] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
      "              gamma=0, gpu_id=-1, importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.300000012,\n",
      "              max_delta_step=0, max_depth=12, max_leaf_nodes=None,\n",
      "              min_child_weight=1, min_samples_split=1, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=10, n_jobs=8,\n",
      "              num_parallel_tree=1, predictor='auto', random_state=3,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)\n",
      "kfold  9  score: 91.39%\n",
      "kfold  9  score: 98.79%\n",
      "Average score over k-fold 91.95% (+/-1.69%) [XGBClassifier]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21:47:54] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:47:54] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "VotingClassifier(estimators=[('gb',\n",
      "                              GradientBoostingClassifier(max_depth=12,\n",
      "                                                         min_samples_split=12,\n",
      "                                                         n_estimators=5,\n",
      "                                                         random_state=3)),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(max_depth=12,\n",
      "                                                     n_estimators=10,\n",
      "                                                     random_state=3)),\n",
      "                             ('xgb',\n",
      "                              XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "                                            colsample_bylevel=1,\n",
      "                                            colsample_bynode=1,\n",
      "                                            colsample_bytree=1,\n",
      "                                            enable_categorical=False, gamma=0,\n",
      "                                            gpu...\n",
      "                                            interaction_constraints='',\n",
      "                                            learning_rate=0.300000012,\n",
      "                                            max_delta_step=0, max_depth=12,\n",
      "                                            max_leaf_nodes=None,\n",
      "                                            min_child_weight=1,\n",
      "                                            min_samples_split=1, missing=nan,\n",
      "                                            monotone_constraints='()',\n",
      "                                            n_estimators=10, n_jobs=8,\n",
      "                                            num_parallel_tree=1,\n",
      "                                            predictor='auto', random_state=3,\n",
      "                                            reg_alpha=0, reg_lambda=1,\n",
      "                                            scale_pos_weight=1, subsample=1,\n",
      "                                            tree_method='exact',\n",
      "                                            validate_parameters=1,\n",
      "                                            verbosity=None))])\n",
      "kfold  9  score: 92.13%\n",
      "kfold  9  score: 96.75%\n",
      "[21:47:54] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:47:54] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "VotingClassifier(estimators=[('gb',\n",
      "                              GradientBoostingClassifier(max_depth=12,\n",
      "                                                         min_samples_split=12,\n",
      "                                                         n_estimators=5,\n",
      "                                                         random_state=3)),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(max_depth=12,\n",
      "                                                     n_estimators=10,\n",
      "                                                     random_state=3)),\n",
      "                             ('xgb',\n",
      "                              XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "                                            colsample_bylevel=1,\n",
      "                                            colsample_bynode=1,\n",
      "                                            colsample_bytree=1,\n",
      "                                            enable_categorical=False, gamma=0,\n",
      "                                            gpu...\n",
      "                                            interaction_constraints='',\n",
      "                                            learning_rate=0.300000012,\n",
      "                                            max_delta_step=0, max_depth=12,\n",
      "                                            max_leaf_nodes=None,\n",
      "                                            min_child_weight=1,\n",
      "                                            min_samples_split=1, missing=nan,\n",
      "                                            monotone_constraints='()',\n",
      "                                            n_estimators=10, n_jobs=8,\n",
      "                                            num_parallel_tree=1,\n",
      "                                            predictor='auto', random_state=3,\n",
      "                                            reg_alpha=0, reg_lambda=1,\n",
      "                                            scale_pos_weight=1, subsample=1,\n",
      "                                            tree_method='exact',\n",
      "                                            validate_parameters=1,\n",
      "                                            verbosity=None))])\n",
      "kfold  9  score: 93.07%\n",
      "kfold  9  score: 97.69%\n",
      "[21:47:55] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:47:55] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "VotingClassifier(estimators=[('gb',\n",
      "                              GradientBoostingClassifier(max_depth=12,\n",
      "                                                         min_samples_split=12,\n",
      "                                                         n_estimators=5,\n",
      "                                                         random_state=3)),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(max_depth=12,\n",
      "                                                     n_estimators=10,\n",
      "                                                     random_state=3)),\n",
      "                             ('xgb',\n",
      "                              XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "                                            colsample_bylevel=1,\n",
      "                                            colsample_bynode=1,\n",
      "                                            colsample_bytree=1,\n",
      "                                            enable_categorical=False, gamma=0,\n",
      "                                            gpu...\n",
      "                                            interaction_constraints='',\n",
      "                                            learning_rate=0.300000012,\n",
      "                                            max_delta_step=0, max_depth=12,\n",
      "                                            max_leaf_nodes=None,\n",
      "                                            min_child_weight=1,\n",
      "                                            min_samples_split=1, missing=nan,\n",
      "                                            monotone_constraints='()',\n",
      "                                            n_estimators=10, n_jobs=8,\n",
      "                                            num_parallel_tree=1,\n",
      "                                            predictor='auto', random_state=3,\n",
      "                                            reg_alpha=0, reg_lambda=1,\n",
      "                                            scale_pos_weight=1, subsample=1,\n",
      "                                            tree_method='exact',\n",
      "                                            validate_parameters=1,\n",
      "                                            verbosity=None))])\n",
      "kfold  9  score: 87.64%\n",
      "kfold  9  score: 96.09%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21:47:56] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:47:56] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "VotingClassifier(estimators=[('gb',\n",
      "                              GradientBoostingClassifier(max_depth=12,\n",
      "                                                         min_samples_split=12,\n",
      "                                                         n_estimators=5,\n",
      "                                                         random_state=3)),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(max_depth=12,\n",
      "                                                     n_estimators=10,\n",
      "                                                     random_state=3)),\n",
      "                             ('xgb',\n",
      "                              XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "                                            colsample_bylevel=1,\n",
      "                                            colsample_bynode=1,\n",
      "                                            colsample_bytree=1,\n",
      "                                            enable_categorical=False, gamma=0,\n",
      "                                            gpu...\n",
      "                                            interaction_constraints='',\n",
      "                                            learning_rate=0.300000012,\n",
      "                                            max_delta_step=0, max_depth=12,\n",
      "                                            max_leaf_nodes=None,\n",
      "                                            min_child_weight=1,\n",
      "                                            min_samples_split=1, missing=nan,\n",
      "                                            monotone_constraints='()',\n",
      "                                            n_estimators=10, n_jobs=8,\n",
      "                                            num_parallel_tree=1,\n",
      "                                            predictor='auto', random_state=3,\n",
      "                                            reg_alpha=0, reg_lambda=1,\n",
      "                                            scale_pos_weight=1, subsample=1,\n",
      "                                            tree_method='exact',\n",
      "                                            validate_parameters=1,\n",
      "                                            verbosity=None))])\n",
      "kfold  9  score: 90.82%\n",
      "kfold  9  score: 97.25%\n",
      "[21:47:56] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:47:56] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "VotingClassifier(estimators=[('gb',\n",
      "                              GradientBoostingClassifier(max_depth=12,\n",
      "                                                         min_samples_split=12,\n",
      "                                                         n_estimators=5,\n",
      "                                                         random_state=3)),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(max_depth=12,\n",
      "                                                     n_estimators=10,\n",
      "                                                     random_state=3)),\n",
      "                             ('xgb',\n",
      "                              XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "                                            colsample_bylevel=1,\n",
      "                                            colsample_bynode=1,\n",
      "                                            colsample_bytree=1,\n",
      "                                            enable_categorical=False, gamma=0,\n",
      "                                            gpu...\n",
      "                                            interaction_constraints='',\n",
      "                                            learning_rate=0.300000012,\n",
      "                                            max_delta_step=0, max_depth=12,\n",
      "                                            max_leaf_nodes=None,\n",
      "                                            min_child_weight=1,\n",
      "                                            min_samples_split=1, missing=nan,\n",
      "                                            monotone_constraints='()',\n",
      "                                            n_estimators=10, n_jobs=8,\n",
      "                                            num_parallel_tree=1,\n",
      "                                            predictor='auto', random_state=3,\n",
      "                                            reg_alpha=0, reg_lambda=1,\n",
      "                                            scale_pos_weight=1, subsample=1,\n",
      "                                            tree_method='exact',\n",
      "                                            validate_parameters=1,\n",
      "                                            verbosity=None))])\n",
      "kfold  9  score: 93.63%\n",
      "kfold  9  score: 97.02%\n",
      "[21:47:57] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:47:57] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "VotingClassifier(estimators=[('gb',\n",
      "                              GradientBoostingClassifier(max_depth=12,\n",
      "                                                         min_samples_split=12,\n",
      "                                                         n_estimators=5,\n",
      "                                                         random_state=3)),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(max_depth=12,\n",
      "                                                     n_estimators=10,\n",
      "                                                     random_state=3)),\n",
      "                             ('xgb',\n",
      "                              XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "                                            colsample_bylevel=1,\n",
      "                                            colsample_bynode=1,\n",
      "                                            colsample_bytree=1,\n",
      "                                            enable_categorical=False, gamma=0,\n",
      "                                            gpu...\n",
      "                                            interaction_constraints='',\n",
      "                                            learning_rate=0.300000012,\n",
      "                                            max_delta_step=0, max_depth=12,\n",
      "                                            max_leaf_nodes=None,\n",
      "                                            min_child_weight=1,\n",
      "                                            min_samples_split=1, missing=nan,\n",
      "                                            monotone_constraints='()',\n",
      "                                            n_estimators=10, n_jobs=8,\n",
      "                                            num_parallel_tree=1,\n",
      "                                            predictor='auto', random_state=3,\n",
      "                                            reg_alpha=0, reg_lambda=1,\n",
      "                                            scale_pos_weight=1, subsample=1,\n",
      "                                            tree_method='exact',\n",
      "                                            validate_parameters=1,\n",
      "                                            verbosity=None))])\n",
      "kfold  9  score: 91.39%\n",
      "kfold  9  score: 97.27%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21:47:57] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:47:57] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "VotingClassifier(estimators=[('gb',\n",
      "                              GradientBoostingClassifier(max_depth=12,\n",
      "                                                         min_samples_split=12,\n",
      "                                                         n_estimators=5,\n",
      "                                                         random_state=3)),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(max_depth=12,\n",
      "                                                     n_estimators=10,\n",
      "                                                     random_state=3)),\n",
      "                             ('xgb',\n",
      "                              XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "                                            colsample_bylevel=1,\n",
      "                                            colsample_bynode=1,\n",
      "                                            colsample_bytree=1,\n",
      "                                            enable_categorical=False, gamma=0,\n",
      "                                            gpu...\n",
      "                                            interaction_constraints='',\n",
      "                                            learning_rate=0.300000012,\n",
      "                                            max_delta_step=0, max_depth=12,\n",
      "                                            max_leaf_nodes=None,\n",
      "                                            min_child_weight=1,\n",
      "                                            min_samples_split=1, missing=nan,\n",
      "                                            monotone_constraints='()',\n",
      "                                            n_estimators=10, n_jobs=8,\n",
      "                                            num_parallel_tree=1,\n",
      "                                            predictor='auto', random_state=3,\n",
      "                                            reg_alpha=0, reg_lambda=1,\n",
      "                                            scale_pos_weight=1, subsample=1,\n",
      "                                            tree_method='exact',\n",
      "                                            validate_parameters=1,\n",
      "                                            verbosity=None))])\n",
      "kfold  9  score: 90.26%\n",
      "kfold  9  score: 96.65%\n",
      "[21:47:58] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:47:58] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "VotingClassifier(estimators=[('gb',\n",
      "                              GradientBoostingClassifier(max_depth=12,\n",
      "                                                         min_samples_split=12,\n",
      "                                                         n_estimators=5,\n",
      "                                                         random_state=3)),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(max_depth=12,\n",
      "                                                     n_estimators=10,\n",
      "                                                     random_state=3)),\n",
      "                             ('xgb',\n",
      "                              XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "                                            colsample_bylevel=1,\n",
      "                                            colsample_bynode=1,\n",
      "                                            colsample_bytree=1,\n",
      "                                            enable_categorical=False, gamma=0,\n",
      "                                            gpu...\n",
      "                                            interaction_constraints='',\n",
      "                                            learning_rate=0.300000012,\n",
      "                                            max_delta_step=0, max_depth=12,\n",
      "                                            max_leaf_nodes=None,\n",
      "                                            min_child_weight=1,\n",
      "                                            min_samples_split=1, missing=nan,\n",
      "                                            monotone_constraints='()',\n",
      "                                            n_estimators=10, n_jobs=8,\n",
      "                                            num_parallel_tree=1,\n",
      "                                            predictor='auto', random_state=3,\n",
      "                                            reg_alpha=0, reg_lambda=1,\n",
      "                                            scale_pos_weight=1, subsample=1,\n",
      "                                            tree_method='exact',\n",
      "                                            validate_parameters=1,\n",
      "                                            verbosity=None))])\n",
      "kfold  9  score: 91.20%\n",
      "kfold  9  score: 97.46%\n",
      "[21:47:58] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:47:59] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "VotingClassifier(estimators=[('gb',\n",
      "                              GradientBoostingClassifier(max_depth=12,\n",
      "                                                         min_samples_split=12,\n",
      "                                                         n_estimators=5,\n",
      "                                                         random_state=3)),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(max_depth=12,\n",
      "                                                     n_estimators=10,\n",
      "                                                     random_state=3)),\n",
      "                             ('xgb',\n",
      "                              XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "                                            colsample_bylevel=1,\n",
      "                                            colsample_bynode=1,\n",
      "                                            colsample_bytree=1,\n",
      "                                            enable_categorical=False, gamma=0,\n",
      "                                            gpu...\n",
      "                                            interaction_constraints='',\n",
      "                                            learning_rate=0.300000012,\n",
      "                                            max_delta_step=0, max_depth=12,\n",
      "                                            max_leaf_nodes=None,\n",
      "                                            min_child_weight=1,\n",
      "                                            min_samples_split=1, missing=nan,\n",
      "                                            monotone_constraints='()',\n",
      "                                            n_estimators=10, n_jobs=8,\n",
      "                                            num_parallel_tree=1,\n",
      "                                            predictor='auto', random_state=3,\n",
      "                                            reg_alpha=0, reg_lambda=1,\n",
      "                                            scale_pos_weight=1, subsample=1,\n",
      "                                            tree_method='exact',\n",
      "                                            validate_parameters=1,\n",
      "                                            verbosity=None))])\n",
      "kfold  9  score: 91.57%\n",
      "kfold  9  score: 97.17%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21:47:59] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:47:59] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "VotingClassifier(estimators=[('gb',\n",
      "                              GradientBoostingClassifier(max_depth=12,\n",
      "                                                         min_samples_split=12,\n",
      "                                                         n_estimators=5,\n",
      "                                                         random_state=3)),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(max_depth=12,\n",
      "                                                     n_estimators=10,\n",
      "                                                     random_state=3)),\n",
      "                             ('xgb',\n",
      "                              XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "                                            colsample_bylevel=1,\n",
      "                                            colsample_bynode=1,\n",
      "                                            colsample_bytree=1,\n",
      "                                            enable_categorical=False, gamma=0,\n",
      "                                            gpu...\n",
      "                                            interaction_constraints='',\n",
      "                                            learning_rate=0.300000012,\n",
      "                                            max_delta_step=0, max_depth=12,\n",
      "                                            max_leaf_nodes=None,\n",
      "                                            min_child_weight=1,\n",
      "                                            min_samples_split=1, missing=nan,\n",
      "                                            monotone_constraints='()',\n",
      "                                            n_estimators=10, n_jobs=8,\n",
      "                                            num_parallel_tree=1,\n",
      "                                            predictor='auto', random_state=3,\n",
      "                                            reg_alpha=0, reg_lambda=1,\n",
      "                                            scale_pos_weight=1, subsample=1,\n",
      "                                            tree_method='exact',\n",
      "                                            validate_parameters=1,\n",
      "                                            verbosity=None))])\n",
      "kfold  9  score: 91.20%\n",
      "kfold  9  score: 97.17%\n",
      "Average score over k-fold 91.29% (+/-1.55%) [Ensemble Majority]\n",
      "./Files_Majority_vote/128_features/filter10.csv\n",
      "n_estimators = 100 \n",
      "\n",
      "10-fold cross validation for file :  10\n",
      "GradientBoostingClassifier(max_depth=12, min_samples_split=12, n_estimators=5,\n",
      "                           random_state=3)\n",
      "kfold  10  score: 86.18%\n",
      "kfold  10  score: 91.56%\n",
      "GradientBoostingClassifier(max_depth=12, min_samples_split=12, n_estimators=5,\n",
      "                           random_state=3)\n",
      "kfold  10  score: 87.43%\n",
      "kfold  10  score: 90.64%\n",
      "GradientBoostingClassifier(max_depth=12, min_samples_split=12, n_estimators=5,\n",
      "                           random_state=3)\n",
      "kfold  10  score: 91.92%\n",
      "kfold  10  score: 93.75%\n",
      "GradientBoostingClassifier(max_depth=12, min_samples_split=12, n_estimators=5,\n",
      "                           random_state=3)\n",
      "kfold  10  score: 89.77%\n",
      "kfold  10  score: 92.30%\n",
      "GradientBoostingClassifier(max_depth=12, min_samples_split=12, n_estimators=5,\n",
      "                           random_state=3)\n",
      "kfold  10  score: 87.79%\n",
      "kfold  10  score: 91.44%\n",
      "GradientBoostingClassifier(max_depth=12, min_samples_split=12, n_estimators=5,\n",
      "                           random_state=3)\n",
      "kfold  10  score: 85.82%\n",
      "kfold  10  score: 92.66%\n",
      "GradientBoostingClassifier(max_depth=12, min_samples_split=12, n_estimators=5,\n",
      "                           random_state=3)\n",
      "kfold  10  score: 91.56%\n",
      "kfold  10  score: 94.03%\n",
      "GradientBoostingClassifier(max_depth=12, min_samples_split=12, n_estimators=5,\n",
      "                           random_state=3)\n",
      "kfold  10  score: 78.64%\n",
      "kfold  10  score: 84.06%\n",
      "GradientBoostingClassifier(max_depth=12, min_samples_split=12, n_estimators=5,\n",
      "                           random_state=3)\n",
      "kfold  10  score: 87.07%\n",
      "kfold  10  score: 89.96%\n",
      "GradientBoostingClassifier(max_depth=12, min_samples_split=12, n_estimators=5,\n",
      "                           random_state=3)\n",
      "kfold  10  score: 89.21%\n",
      "kfold  10  score: 91.30%\n",
      "Average score over k-fold 87.54% (+/-3.57%) [Gradient Boosting]\n",
      "RandomForestClassifier(max_depth=12, n_estimators=10, random_state=3)\n",
      "kfold  10  score: 90.48%\n",
      "kfold  10  score: 96.51%\n",
      "RandomForestClassifier(max_depth=12, n_estimators=10, random_state=3)\n",
      "kfold  10  score: 93.54%\n",
      "kfold  10  score: 97.31%\n",
      "RandomForestClassifier(max_depth=12, n_estimators=10, random_state=3)\n",
      "kfold  10  score: 93.90%\n",
      "kfold  10  score: 98.00%\n",
      "RandomForestClassifier(max_depth=12, n_estimators=10, random_state=3)\n",
      "kfold  10  score: 93.18%\n",
      "kfold  10  score: 97.47%\n",
      "RandomForestClassifier(max_depth=12, n_estimators=10, random_state=3)\n",
      "kfold  10  score: 93.90%\n",
      "kfold  10  score: 97.85%\n",
      "RandomForestClassifier(max_depth=12, n_estimators=10, random_state=3)\n",
      "kfold  10  score: 90.48%\n",
      "kfold  10  score: 97.37%\n",
      "RandomForestClassifier(max_depth=12, n_estimators=10, random_state=3)\n",
      "kfold  10  score: 93.72%\n",
      "kfold  10  score: 97.05%\n",
      "RandomForestClassifier(max_depth=12, n_estimators=10, random_state=3)\n",
      "kfold  10  score: 91.02%\n",
      "kfold  10  score: 96.75%\n",
      "RandomForestClassifier(max_depth=12, n_estimators=10, random_state=3)\n",
      "kfold  10  score: 95.51%\n",
      "kfold  10  score: 97.43%\n",
      "RandomForestClassifier(max_depth=12, n_estimators=10, random_state=3)\n",
      "kfold  10  score: 94.06%\n",
      "kfold  10  score: 97.61%\n",
      "Average score over k-fold 92.98% (+/-1.63%) [Random Forest]\n",
      "[21:48:03] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:48:03] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
      "              gamma=0, gpu_id=-1, importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.300000012,\n",
      "              max_delta_step=0, max_depth=12, max_leaf_nodes=None,\n",
      "              min_child_weight=1, min_samples_split=1, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=10, n_jobs=8,\n",
      "              num_parallel_tree=1, predictor='auto', random_state=3,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)\n",
      "kfold  10  score: 92.10%\n",
      "kfold  10  score: 99.14%\n",
      "[21:48:03] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:48:03] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
      "              gamma=0, gpu_id=-1, importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.300000012,\n",
      "              max_delta_step=0, max_depth=12, max_leaf_nodes=None,\n",
      "              min_child_weight=1, min_samples_split=1, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=10, n_jobs=8,\n",
      "              num_parallel_tree=1, predictor='auto', random_state=3,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)\n",
      "kfold  10  score: 94.43%\n",
      "kfold  10  score: 98.62%\n",
      "[21:48:03] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:48:03] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
      "              gamma=0, gpu_id=-1, importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.300000012,\n",
      "              max_delta_step=0, max_depth=12, max_leaf_nodes=None,\n",
      "              min_child_weight=1, min_samples_split=1, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=10, n_jobs=8,\n",
      "              num_parallel_tree=1, predictor='auto', random_state=3,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)\n",
      "kfold  10  score: 94.79%\n",
      "kfold  10  score: 99.16%\n",
      "[21:48:03] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21:48:03] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
      "              gamma=0, gpu_id=-1, importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.300000012,\n",
      "              max_delta_step=0, max_depth=12, max_leaf_nodes=None,\n",
      "              min_child_weight=1, min_samples_split=1, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=10, n_jobs=8,\n",
      "              num_parallel_tree=1, predictor='auto', random_state=3,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)\n",
      "kfold  10  score: 94.61%\n",
      "kfold  10  score: 99.04%\n",
      "[21:48:03] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:48:03] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
      "              gamma=0, gpu_id=-1, importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.300000012,\n",
      "              max_delta_step=0, max_depth=12, max_leaf_nodes=None,\n",
      "              min_child_weight=1, min_samples_split=1, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=10, n_jobs=8,\n",
      "              num_parallel_tree=1, predictor='auto', random_state=3,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)\n",
      "kfold  10  score: 93.00%\n",
      "kfold  10  score: 98.78%\n",
      "[21:48:03] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:48:03] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
      "              gamma=0, gpu_id=-1, importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.300000012,\n",
      "              max_delta_step=0, max_depth=12, max_leaf_nodes=None,\n",
      "              min_child_weight=1, min_samples_split=1, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=10, n_jobs=8,\n",
      "              num_parallel_tree=1, predictor='auto', random_state=3,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)\n",
      "kfold  10  score: 93.18%\n",
      "kfold  10  score: 98.88%\n",
      "[21:48:03] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:48:03] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
      "              gamma=0, gpu_id=-1, importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.300000012,\n",
      "              max_delta_step=0, max_depth=12, max_leaf_nodes=None,\n",
      "              min_child_weight=1, min_samples_split=1, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=10, n_jobs=8,\n",
      "              num_parallel_tree=1, predictor='auto', random_state=3,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)\n",
      "kfold  10  score: 94.25%\n",
      "kfold  10  score: 99.14%\n",
      "[21:48:03] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:48:03] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
      "              gamma=0, gpu_id=-1, importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.300000012,\n",
      "              max_delta_step=0, max_depth=12, max_leaf_nodes=None,\n",
      "              min_child_weight=1, min_samples_split=1, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=10, n_jobs=8,\n",
      "              num_parallel_tree=1, predictor='auto', random_state=3,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)\n",
      "kfold  10  score: 94.79%\n",
      "kfold  10  score: 99.04%\n",
      "[21:48:04] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:48:04] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
      "              gamma=0, gpu_id=-1, importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.300000012,\n",
      "              max_delta_step=0, max_depth=12, max_leaf_nodes=None,\n",
      "              min_child_weight=1, min_samples_split=1, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=10, n_jobs=8,\n",
      "              num_parallel_tree=1, predictor='auto', random_state=3,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)\n",
      "kfold  10  score: 94.43%\n",
      "kfold  10  score: 98.90%\n",
      "[21:48:04] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:48:04] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
      "              gamma=0, gpu_id=-1, importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.300000012,\n",
      "              max_delta_step=0, max_depth=12, max_leaf_nodes=None,\n",
      "              min_child_weight=1, min_samples_split=1, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=10, n_jobs=8,\n",
      "              num_parallel_tree=1, predictor='auto', random_state=3,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)\n",
      "kfold  10  score: 95.50%\n",
      "kfold  10  score: 98.92%\n",
      "Average score over k-fold 94.11% (+/-0.97%) [XGBClassifier]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21:48:04] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:48:04] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "VotingClassifier(estimators=[('gb',\n",
      "                              GradientBoostingClassifier(max_depth=12,\n",
      "                                                         min_samples_split=12,\n",
      "                                                         n_estimators=5,\n",
      "                                                         random_state=3)),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(max_depth=12,\n",
      "                                                     n_estimators=10,\n",
      "                                                     random_state=3)),\n",
      "                             ('xgb',\n",
      "                              XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "                                            colsample_bylevel=1,\n",
      "                                            colsample_bynode=1,\n",
      "                                            colsample_bytree=1,\n",
      "                                            enable_categorical=False, gamma=0,\n",
      "                                            gpu...\n",
      "                                            interaction_constraints='',\n",
      "                                            learning_rate=0.300000012,\n",
      "                                            max_delta_step=0, max_depth=12,\n",
      "                                            max_leaf_nodes=None,\n",
      "                                            min_child_weight=1,\n",
      "                                            min_samples_split=1, missing=nan,\n",
      "                                            monotone_constraints='()',\n",
      "                                            n_estimators=10, n_jobs=8,\n",
      "                                            num_parallel_tree=1,\n",
      "                                            predictor='auto', random_state=3,\n",
      "                                            reg_alpha=0, reg_lambda=1,\n",
      "                                            scale_pos_weight=1, subsample=1,\n",
      "                                            tree_method='exact',\n",
      "                                            validate_parameters=1,\n",
      "                                            verbosity=None))])\n",
      "kfold  10  score: 90.84%\n",
      "kfold  10  score: 97.39%\n",
      "[21:48:05] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:48:05] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "VotingClassifier(estimators=[('gb',\n",
      "                              GradientBoostingClassifier(max_depth=12,\n",
      "                                                         min_samples_split=12,\n",
      "                                                         n_estimators=5,\n",
      "                                                         random_state=3)),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(max_depth=12,\n",
      "                                                     n_estimators=10,\n",
      "                                                     random_state=3)),\n",
      "                             ('xgb',\n",
      "                              XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "                                            colsample_bylevel=1,\n",
      "                                            colsample_bynode=1,\n",
      "                                            colsample_bytree=1,\n",
      "                                            enable_categorical=False, gamma=0,\n",
      "                                            gpu...\n",
      "                                            interaction_constraints='',\n",
      "                                            learning_rate=0.300000012,\n",
      "                                            max_delta_step=0, max_depth=12,\n",
      "                                            max_leaf_nodes=None,\n",
      "                                            min_child_weight=1,\n",
      "                                            min_samples_split=1, missing=nan,\n",
      "                                            monotone_constraints='()',\n",
      "                                            n_estimators=10, n_jobs=8,\n",
      "                                            num_parallel_tree=1,\n",
      "                                            predictor='auto', random_state=3,\n",
      "                                            reg_alpha=0, reg_lambda=1,\n",
      "                                            scale_pos_weight=1, subsample=1,\n",
      "                                            tree_method='exact',\n",
      "                                            validate_parameters=1,\n",
      "                                            verbosity=None))])\n",
      "kfold  10  score: 93.36%\n",
      "kfold  10  score: 97.43%\n",
      "[21:48:05] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:48:05] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "VotingClassifier(estimators=[('gb',\n",
      "                              GradientBoostingClassifier(max_depth=12,\n",
      "                                                         min_samples_split=12,\n",
      "                                                         n_estimators=5,\n",
      "                                                         random_state=3)),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(max_depth=12,\n",
      "                                                     n_estimators=10,\n",
      "                                                     random_state=3)),\n",
      "                             ('xgb',\n",
      "                              XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "                                            colsample_bylevel=1,\n",
      "                                            colsample_bynode=1,\n",
      "                                            colsample_bytree=1,\n",
      "                                            enable_categorical=False, gamma=0,\n",
      "                                            gpu...\n",
      "                                            interaction_constraints='',\n",
      "                                            learning_rate=0.300000012,\n",
      "                                            max_delta_step=0, max_depth=12,\n",
      "                                            max_leaf_nodes=None,\n",
      "                                            min_child_weight=1,\n",
      "                                            min_samples_split=1, missing=nan,\n",
      "                                            monotone_constraints='()',\n",
      "                                            n_estimators=10, n_jobs=8,\n",
      "                                            num_parallel_tree=1,\n",
      "                                            predictor='auto', random_state=3,\n",
      "                                            reg_alpha=0, reg_lambda=1,\n",
      "                                            scale_pos_weight=1, subsample=1,\n",
      "                                            tree_method='exact',\n",
      "                                            validate_parameters=1,\n",
      "                                            verbosity=None))])\n",
      "kfold  10  score: 94.43%\n",
      "kfold  10  score: 98.62%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21:48:06] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:48:06] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "VotingClassifier(estimators=[('gb',\n",
      "                              GradientBoostingClassifier(max_depth=12,\n",
      "                                                         min_samples_split=12,\n",
      "                                                         n_estimators=5,\n",
      "                                                         random_state=3)),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(max_depth=12,\n",
      "                                                     n_estimators=10,\n",
      "                                                     random_state=3)),\n",
      "                             ('xgb',\n",
      "                              XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "                                            colsample_bylevel=1,\n",
      "                                            colsample_bynode=1,\n",
      "                                            colsample_bytree=1,\n",
      "                                            enable_categorical=False, gamma=0,\n",
      "                                            gpu...\n",
      "                                            interaction_constraints='',\n",
      "                                            learning_rate=0.300000012,\n",
      "                                            max_delta_step=0, max_depth=12,\n",
      "                                            max_leaf_nodes=None,\n",
      "                                            min_child_weight=1,\n",
      "                                            min_samples_split=1, missing=nan,\n",
      "                                            monotone_constraints='()',\n",
      "                                            n_estimators=10, n_jobs=8,\n",
      "                                            num_parallel_tree=1,\n",
      "                                            predictor='auto', random_state=3,\n",
      "                                            reg_alpha=0, reg_lambda=1,\n",
      "                                            scale_pos_weight=1, subsample=1,\n",
      "                                            tree_method='exact',\n",
      "                                            validate_parameters=1,\n",
      "                                            verbosity=None))])\n",
      "kfold  10  score: 94.25%\n",
      "kfold  10  score: 97.89%\n",
      "[21:48:06] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:48:06] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "VotingClassifier(estimators=[('gb',\n",
      "                              GradientBoostingClassifier(max_depth=12,\n",
      "                                                         min_samples_split=12,\n",
      "                                                         n_estimators=5,\n",
      "                                                         random_state=3)),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(max_depth=12,\n",
      "                                                     n_estimators=10,\n",
      "                                                     random_state=3)),\n",
      "                             ('xgb',\n",
      "                              XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "                                            colsample_bylevel=1,\n",
      "                                            colsample_bynode=1,\n",
      "                                            colsample_bytree=1,\n",
      "                                            enable_categorical=False, gamma=0,\n",
      "                                            gpu...\n",
      "                                            interaction_constraints='',\n",
      "                                            learning_rate=0.300000012,\n",
      "                                            max_delta_step=0, max_depth=12,\n",
      "                                            max_leaf_nodes=None,\n",
      "                                            min_child_weight=1,\n",
      "                                            min_samples_split=1, missing=nan,\n",
      "                                            monotone_constraints='()',\n",
      "                                            n_estimators=10, n_jobs=8,\n",
      "                                            num_parallel_tree=1,\n",
      "                                            predictor='auto', random_state=3,\n",
      "                                            reg_alpha=0, reg_lambda=1,\n",
      "                                            scale_pos_weight=1, subsample=1,\n",
      "                                            tree_method='exact',\n",
      "                                            validate_parameters=1,\n",
      "                                            verbosity=None))])\n",
      "kfold  10  score: 92.46%\n",
      "kfold  10  score: 98.14%\n",
      "[21:48:07] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:48:07] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "VotingClassifier(estimators=[('gb',\n",
      "                              GradientBoostingClassifier(max_depth=12,\n",
      "                                                         min_samples_split=12,\n",
      "                                                         n_estimators=5,\n",
      "                                                         random_state=3)),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(max_depth=12,\n",
      "                                                     n_estimators=10,\n",
      "                                                     random_state=3)),\n",
      "                             ('xgb',\n",
      "                              XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "                                            colsample_bylevel=1,\n",
      "                                            colsample_bynode=1,\n",
      "                                            colsample_bytree=1,\n",
      "                                            enable_categorical=False, gamma=0,\n",
      "                                            gpu...\n",
      "                                            interaction_constraints='',\n",
      "                                            learning_rate=0.300000012,\n",
      "                                            max_delta_step=0, max_depth=12,\n",
      "                                            max_leaf_nodes=None,\n",
      "                                            min_child_weight=1,\n",
      "                                            min_samples_split=1, missing=nan,\n",
      "                                            monotone_constraints='()',\n",
      "                                            n_estimators=10, n_jobs=8,\n",
      "                                            num_parallel_tree=1,\n",
      "                                            predictor='auto', random_state=3,\n",
      "                                            reg_alpha=0, reg_lambda=1,\n",
      "                                            scale_pos_weight=1, subsample=1,\n",
      "                                            tree_method='exact',\n",
      "                                            validate_parameters=1,\n",
      "                                            verbosity=None))])\n",
      "kfold  10  score: 91.20%\n",
      "kfold  10  score: 97.59%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21:48:07] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:48:07] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "VotingClassifier(estimators=[('gb',\n",
      "                              GradientBoostingClassifier(max_depth=12,\n",
      "                                                         min_samples_split=12,\n",
      "                                                         n_estimators=5,\n",
      "                                                         random_state=3)),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(max_depth=12,\n",
      "                                                     n_estimators=10,\n",
      "                                                     random_state=3)),\n",
      "                             ('xgb',\n",
      "                              XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "                                            colsample_bylevel=1,\n",
      "                                            colsample_bynode=1,\n",
      "                                            colsample_bytree=1,\n",
      "                                            enable_categorical=False, gamma=0,\n",
      "                                            gpu...\n",
      "                                            interaction_constraints='',\n",
      "                                            learning_rate=0.300000012,\n",
      "                                            max_delta_step=0, max_depth=12,\n",
      "                                            max_leaf_nodes=None,\n",
      "                                            min_child_weight=1,\n",
      "                                            min_samples_split=1, missing=nan,\n",
      "                                            monotone_constraints='()',\n",
      "                                            n_estimators=10, n_jobs=8,\n",
      "                                            num_parallel_tree=1,\n",
      "                                            predictor='auto', random_state=3,\n",
      "                                            reg_alpha=0, reg_lambda=1,\n",
      "                                            scale_pos_weight=1, subsample=1,\n",
      "                                            tree_method='exact',\n",
      "                                            validate_parameters=1,\n",
      "                                            verbosity=None))])\n",
      "kfold  10  score: 94.61%\n",
      "kfold  10  score: 98.00%\n",
      "[21:48:08] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:48:08] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "VotingClassifier(estimators=[('gb',\n",
      "                              GradientBoostingClassifier(max_depth=12,\n",
      "                                                         min_samples_split=12,\n",
      "                                                         n_estimators=5,\n",
      "                                                         random_state=3)),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(max_depth=12,\n",
      "                                                     n_estimators=10,\n",
      "                                                     random_state=3)),\n",
      "                             ('xgb',\n",
      "                              XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "                                            colsample_bylevel=1,\n",
      "                                            colsample_bynode=1,\n",
      "                                            colsample_bytree=1,\n",
      "                                            enable_categorical=False, gamma=0,\n",
      "                                            gpu...\n",
      "                                            interaction_constraints='',\n",
      "                                            learning_rate=0.300000012,\n",
      "                                            max_delta_step=0, max_depth=12,\n",
      "                                            max_leaf_nodes=None,\n",
      "                                            min_child_weight=1,\n",
      "                                            min_samples_split=1, missing=nan,\n",
      "                                            monotone_constraints='()',\n",
      "                                            n_estimators=10, n_jobs=8,\n",
      "                                            num_parallel_tree=1,\n",
      "                                            predictor='auto', random_state=3,\n",
      "                                            reg_alpha=0, reg_lambda=1,\n",
      "                                            scale_pos_weight=1, subsample=1,\n",
      "                                            tree_method='exact',\n",
      "                                            validate_parameters=1,\n",
      "                                            verbosity=None))])\n",
      "kfold  10  score: 91.74%\n",
      "kfold  10  score: 97.15%\n",
      "[21:48:08] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:48:08] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "VotingClassifier(estimators=[('gb',\n",
      "                              GradientBoostingClassifier(max_depth=12,\n",
      "                                                         min_samples_split=12,\n",
      "                                                         n_estimators=5,\n",
      "                                                         random_state=3)),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(max_depth=12,\n",
      "                                                     n_estimators=10,\n",
      "                                                     random_state=3)),\n",
      "                             ('xgb',\n",
      "                              XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "                                            colsample_bylevel=1,\n",
      "                                            colsample_bynode=1,\n",
      "                                            colsample_bytree=1,\n",
      "                                            enable_categorical=False, gamma=0,\n",
      "                                            gpu...\n",
      "                                            interaction_constraints='',\n",
      "                                            learning_rate=0.300000012,\n",
      "                                            max_delta_step=0, max_depth=12,\n",
      "                                            max_leaf_nodes=None,\n",
      "                                            min_child_weight=1,\n",
      "                                            min_samples_split=1, missing=nan,\n",
      "                                            monotone_constraints='()',\n",
      "                                            n_estimators=10, n_jobs=8,\n",
      "                                            num_parallel_tree=1,\n",
      "                                            predictor='auto', random_state=3,\n",
      "                                            reg_alpha=0, reg_lambda=1,\n",
      "                                            scale_pos_weight=1, subsample=1,\n",
      "                                            tree_method='exact',\n",
      "                                            validate_parameters=1,\n",
      "                                            verbosity=None))])\n",
      "kfold  10  score: 95.33%\n",
      "kfold  10  score: 97.77%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21:48:09] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:48:09] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "VotingClassifier(estimators=[('gb',\n",
      "                              GradientBoostingClassifier(max_depth=12,\n",
      "                                                         min_samples_split=12,\n",
      "                                                         n_estimators=5,\n",
      "                                                         random_state=3)),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(max_depth=12,\n",
      "                                                     n_estimators=10,\n",
      "                                                     random_state=3)),\n",
      "                             ('xgb',\n",
      "                              XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "                                            colsample_bylevel=1,\n",
      "                                            colsample_bynode=1,\n",
      "                                            colsample_bytree=1,\n",
      "                                            enable_categorical=False, gamma=0,\n",
      "                                            gpu...\n",
      "                                            interaction_constraints='',\n",
      "                                            learning_rate=0.300000012,\n",
      "                                            max_delta_step=0, max_depth=12,\n",
      "                                            max_leaf_nodes=None,\n",
      "                                            min_child_weight=1,\n",
      "                                            min_samples_split=1, missing=nan,\n",
      "                                            monotone_constraints='()',\n",
      "                                            n_estimators=10, n_jobs=8,\n",
      "                                            num_parallel_tree=1,\n",
      "                                            predictor='auto', random_state=3,\n",
      "                                            reg_alpha=0, reg_lambda=1,\n",
      "                                            scale_pos_weight=1, subsample=1,\n",
      "                                            tree_method='exact',\n",
      "                                            validate_parameters=1,\n",
      "                                            verbosity=None))])\n",
      "kfold  10  score: 93.88%\n",
      "kfold  10  score: 98.03%\n",
      "Average score over k-fold 93.21% (+/-1.48%) [Ensemble Majority]\n",
      "./Files_Majority_vote/128_features/filter11.csv\n",
      "n_estimators = 100 \n",
      "\n",
      "10-fold cross validation for file :  11\n",
      "GradientBoostingClassifier(max_depth=12, min_samples_split=12, n_estimators=5,\n",
      "                           random_state=3)\n",
      "kfold  11  score: 88.21%\n",
      "kfold  11  score: 92.40%\n",
      "GradientBoostingClassifier(max_depth=12, min_samples_split=12, n_estimators=5,\n",
      "                           random_state=3)\n",
      "kfold  11  score: 87.43%\n",
      "kfold  11  score: 91.68%\n",
      "GradientBoostingClassifier(max_depth=12, min_samples_split=12, n_estimators=5,\n",
      "                           random_state=3)\n",
      "kfold  11  score: 89.14%\n",
      "kfold  11  score: 91.75%\n",
      "GradientBoostingClassifier(max_depth=12, min_samples_split=12, n_estimators=5,\n",
      "                           random_state=3)\n",
      "kfold  11  score: 89.52%\n",
      "kfold  11  score: 92.30%\n",
      "GradientBoostingClassifier(max_depth=12, min_samples_split=12, n_estimators=5,\n",
      "                           random_state=3)\n",
      "kfold  11  score: 88.76%\n",
      "kfold  11  score: 91.56%\n",
      "GradientBoostingClassifier(max_depth=12, min_samples_split=12, n_estimators=5,\n",
      "                           random_state=3)\n",
      "kfold  11  score: 89.52%\n",
      "kfold  11  score: 93.78%\n",
      "GradientBoostingClassifier(max_depth=12, min_samples_split=12, n_estimators=5,\n",
      "                           random_state=3)\n",
      "kfold  11  score: 86.86%\n",
      "kfold  11  score: 88.76%\n",
      "GradientBoostingClassifier(max_depth=12, min_samples_split=12, n_estimators=5,\n",
      "                           random_state=3)\n",
      "kfold  11  score: 89.33%\n",
      "kfold  11  score: 91.32%\n",
      "GradientBoostingClassifier(max_depth=12, min_samples_split=12, n_estimators=5,\n",
      "                           random_state=3)\n",
      "kfold  11  score: 84.95%\n",
      "kfold  11  score: 88.30%\n",
      "GradientBoostingClassifier(max_depth=12, min_samples_split=12, n_estimators=5,\n",
      "                           random_state=3)\n",
      "kfold  11  score: 86.67%\n",
      "kfold  11  score: 92.38%\n",
      "Average score over k-fold 88.04% (+/-1.45%) [Gradient Boosting]\n",
      "RandomForestClassifier(max_depth=12, n_estimators=10, random_state=3)\n",
      "kfold  11  score: 92.02%\n",
      "kfold  11  score: 97.21%\n",
      "RandomForestClassifier(max_depth=12, n_estimators=10, random_state=3)\n",
      "kfold  11  score: 92.76%\n",
      "kfold  11  score: 97.23%\n",
      "RandomForestClassifier(max_depth=12, n_estimators=10, random_state=3)\n",
      "kfold  11  score: 92.38%\n",
      "kfold  11  score: 98.05%\n",
      "RandomForestClassifier(max_depth=12, n_estimators=10, random_state=3)\n",
      "kfold  11  score: 94.86%\n",
      "kfold  11  score: 98.46%\n",
      "RandomForestClassifier(max_depth=12, n_estimators=10, random_state=3)\n",
      "kfold  11  score: 91.81%\n",
      "kfold  11  score: 97.95%\n",
      "RandomForestClassifier(max_depth=12, n_estimators=10, random_state=3)\n",
      "kfold  11  score: 91.81%\n",
      "kfold  11  score: 97.59%\n",
      "RandomForestClassifier(max_depth=12, n_estimators=10, random_state=3)\n",
      "kfold  11  score: 94.10%\n",
      "kfold  11  score: 96.64%\n",
      "RandomForestClassifier(max_depth=12, n_estimators=10, random_state=3)\n",
      "kfold  11  score: 92.38%\n",
      "kfold  11  score: 97.23%\n",
      "RandomForestClassifier(max_depth=12, n_estimators=10, random_state=3)\n",
      "kfold  11  score: 92.76%\n",
      "kfold  11  score: 97.33%\n",
      "RandomForestClassifier(max_depth=12, n_estimators=10, random_state=3)\n",
      "kfold  11  score: 93.33%\n",
      "kfold  11  score: 97.29%\n",
      "Average score over k-fold 92.82% (+/-0.96%) [Random Forest]\n",
      "[21:48:12] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:48:12] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
      "              gamma=0, gpu_id=-1, importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.300000012,\n",
      "              max_delta_step=0, max_depth=12, max_leaf_nodes=None,\n",
      "              min_child_weight=1, min_samples_split=1, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=10, n_jobs=8,\n",
      "              num_parallel_tree=1, predictor='auto', random_state=3,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)\n",
      "kfold  11  score: 93.35%\n",
      "kfold  11  score: 99.39%\n",
      "[21:48:12] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:48:13] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
      "              gamma=0, gpu_id=-1, importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.300000012,\n",
      "              max_delta_step=0, max_depth=12, max_leaf_nodes=None,\n",
      "              min_child_weight=1, min_samples_split=1, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=10, n_jobs=8,\n",
      "              num_parallel_tree=1, predictor='auto', random_state=3,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)\n",
      "kfold  11  score: 95.05%\n",
      "kfold  11  score: 99.26%\n",
      "[21:48:13] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21:48:13] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
      "              gamma=0, gpu_id=-1, importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.300000012,\n",
      "              max_delta_step=0, max_depth=12, max_leaf_nodes=None,\n",
      "              min_child_weight=1, min_samples_split=1, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=10, n_jobs=8,\n",
      "              num_parallel_tree=1, predictor='auto', random_state=3,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)\n",
      "kfold  11  score: 93.90%\n",
      "kfold  11  score: 98.88%\n",
      "[21:48:13] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:48:13] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
      "              gamma=0, gpu_id=-1, importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.300000012,\n",
      "              max_delta_step=0, max_depth=12, max_leaf_nodes=None,\n",
      "              min_child_weight=1, min_samples_split=1, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=10, n_jobs=8,\n",
      "              num_parallel_tree=1, predictor='auto', random_state=3,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)\n",
      "kfold  11  score: 95.62%\n",
      "kfold  11  score: 99.34%\n",
      "[21:48:13] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:48:13] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
      "              gamma=0, gpu_id=-1, importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.300000012,\n",
      "              max_delta_step=0, max_depth=12, max_leaf_nodes=None,\n",
      "              min_child_weight=1, min_samples_split=1, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=10, n_jobs=8,\n",
      "              num_parallel_tree=1, predictor='auto', random_state=3,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)\n",
      "kfold  11  score: 93.71%\n",
      "kfold  11  score: 99.11%\n",
      "[21:48:13] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:48:13] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
      "              gamma=0, gpu_id=-1, importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.300000012,\n",
      "              max_delta_step=0, max_depth=12, max_leaf_nodes=None,\n",
      "              min_child_weight=1, min_samples_split=1, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=10, n_jobs=8,\n",
      "              num_parallel_tree=1, predictor='auto', random_state=3,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)\n",
      "kfold  11  score: 92.00%\n",
      "kfold  11  score: 98.98%\n",
      "[21:48:13] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:48:13] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
      "              gamma=0, gpu_id=-1, importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.300000012,\n",
      "              max_delta_step=0, max_depth=12, max_leaf_nodes=None,\n",
      "              min_child_weight=1, min_samples_split=1, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=10, n_jobs=8,\n",
      "              num_parallel_tree=1, predictor='auto', random_state=3,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)\n",
      "kfold  11  score: 94.48%\n",
      "kfold  11  score: 99.01%\n",
      "[21:48:13] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:48:13] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
      "              gamma=0, gpu_id=-1, importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.300000012,\n",
      "              max_delta_step=0, max_depth=12, max_leaf_nodes=None,\n",
      "              min_child_weight=1, min_samples_split=1, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=10, n_jobs=8,\n",
      "              num_parallel_tree=1, predictor='auto', random_state=3,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)\n",
      "kfold  11  score: 95.24%\n",
      "kfold  11  score: 99.01%\n",
      "[21:48:13] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:48:13] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
      "              gamma=0, gpu_id=-1, importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.300000012,\n",
      "              max_delta_step=0, max_depth=12, max_leaf_nodes=None,\n",
      "              min_child_weight=1, min_samples_split=1, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=10, n_jobs=8,\n",
      "              num_parallel_tree=1, predictor='auto', random_state=3,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)\n",
      "kfold  11  score: 94.10%\n",
      "kfold  11  score: 98.62%\n",
      "[21:48:13] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:48:13] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
      "              gamma=0, gpu_id=-1, importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.300000012,\n",
      "              max_delta_step=0, max_depth=12, max_leaf_nodes=None,\n",
      "              min_child_weight=1, min_samples_split=1, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=10, n_jobs=8,\n",
      "              num_parallel_tree=1, predictor='auto', random_state=3,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)\n",
      "kfold  11  score: 94.48%\n",
      "kfold  11  score: 99.17%\n",
      "Average score over k-fold 94.19% (+/-0.99%) [XGBClassifier]\n",
      "[21:48:14] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:48:14] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "VotingClassifier(estimators=[('gb',\n",
      "                              GradientBoostingClassifier(max_depth=12,\n",
      "                                                         min_samples_split=12,\n",
      "                                                         n_estimators=5,\n",
      "                                                         random_state=3)),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(max_depth=12,\n",
      "                                                     n_estimators=10,\n",
      "                                                     random_state=3)),\n",
      "                             ('xgb',\n",
      "                              XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "                                            colsample_bylevel=1,\n",
      "                                            colsample_bynode=1,\n",
      "                                            colsample_bytree=1,\n",
      "                                            enable_categorical=False, gamma=0,\n",
      "                                            gpu...\n",
      "                                            interaction_constraints='',\n",
      "                                            learning_rate=0.300000012,\n",
      "                                            max_delta_step=0, max_depth=12,\n",
      "                                            max_leaf_nodes=None,\n",
      "                                            min_child_weight=1,\n",
      "                                            min_samples_split=1, missing=nan,\n",
      "                                            monotone_constraints='()',\n",
      "                                            n_estimators=10, n_jobs=8,\n",
      "                                            num_parallel_tree=1,\n",
      "                                            predictor='auto', random_state=3,\n",
      "                                            reg_alpha=0, reg_lambda=1,\n",
      "                                            scale_pos_weight=1, subsample=1,\n",
      "                                            tree_method='exact',\n",
      "                                            validate_parameters=1,\n",
      "                                            verbosity=None))])\n",
      "kfold  11  score: 92.40%\n",
      "kfold  11  score: 97.74%\n",
      "[21:48:14] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:48:14] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "VotingClassifier(estimators=[('gb',\n",
      "                              GradientBoostingClassifier(max_depth=12,\n",
      "                                                         min_samples_split=12,\n",
      "                                                         n_estimators=5,\n",
      "                                                         random_state=3)),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(max_depth=12,\n",
      "                                                     n_estimators=10,\n",
      "                                                     random_state=3)),\n",
      "                             ('xgb',\n",
      "                              XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "                                            colsample_bylevel=1,\n",
      "                                            colsample_bynode=1,\n",
      "                                            colsample_bytree=1,\n",
      "                                            enable_categorical=False, gamma=0,\n",
      "                                            gpu...\n",
      "                                            interaction_constraints='',\n",
      "                                            learning_rate=0.300000012,\n",
      "                                            max_delta_step=0, max_depth=12,\n",
      "                                            max_leaf_nodes=None,\n",
      "                                            min_child_weight=1,\n",
      "                                            min_samples_split=1, missing=nan,\n",
      "                                            monotone_constraints='()',\n",
      "                                            n_estimators=10, n_jobs=8,\n",
      "                                            num_parallel_tree=1,\n",
      "                                            predictor='auto', random_state=3,\n",
      "                                            reg_alpha=0, reg_lambda=1,\n",
      "                                            scale_pos_weight=1, subsample=1,\n",
      "                                            tree_method='exact',\n",
      "                                            validate_parameters=1,\n",
      "                                            verbosity=None))])\n",
      "kfold  11  score: 93.52%\n",
      "kfold  11  score: 97.69%\n",
      "[21:48:15] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:48:15] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "VotingClassifier(estimators=[('gb',\n",
      "                              GradientBoostingClassifier(max_depth=12,\n",
      "                                                         min_samples_split=12,\n",
      "                                                         n_estimators=5,\n",
      "                                                         random_state=3)),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(max_depth=12,\n",
      "                                                     n_estimators=10,\n",
      "                                                     random_state=3)),\n",
      "                             ('xgb',\n",
      "                              XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "                                            colsample_bylevel=1,\n",
      "                                            colsample_bynode=1,\n",
      "                                            colsample_bytree=1,\n",
      "                                            enable_categorical=False, gamma=0,\n",
      "                                            gpu...\n",
      "                                            interaction_constraints='',\n",
      "                                            learning_rate=0.300000012,\n",
      "                                            max_delta_step=0, max_depth=12,\n",
      "                                            max_leaf_nodes=None,\n",
      "                                            min_child_weight=1,\n",
      "                                            min_samples_split=1, missing=nan,\n",
      "                                            monotone_constraints='()',\n",
      "                                            n_estimators=10, n_jobs=8,\n",
      "                                            num_parallel_tree=1,\n",
      "                                            predictor='auto', random_state=3,\n",
      "                                            reg_alpha=0, reg_lambda=1,\n",
      "                                            scale_pos_weight=1, subsample=1,\n",
      "                                            tree_method='exact',\n",
      "                                            validate_parameters=1,\n",
      "                                            verbosity=None))])\n",
      "kfold  11  score: 94.29%\n",
      "kfold  11  score: 98.12%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21:48:15] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:48:15] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "VotingClassifier(estimators=[('gb',\n",
      "                              GradientBoostingClassifier(max_depth=12,\n",
      "                                                         min_samples_split=12,\n",
      "                                                         n_estimators=5,\n",
      "                                                         random_state=3)),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(max_depth=12,\n",
      "                                                     n_estimators=10,\n",
      "                                                     random_state=3)),\n",
      "                             ('xgb',\n",
      "                              XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "                                            colsample_bylevel=1,\n",
      "                                            colsample_bynode=1,\n",
      "                                            colsample_bytree=1,\n",
      "                                            enable_categorical=False, gamma=0,\n",
      "                                            gpu...\n",
      "                                            interaction_constraints='',\n",
      "                                            learning_rate=0.300000012,\n",
      "                                            max_delta_step=0, max_depth=12,\n",
      "                                            max_leaf_nodes=None,\n",
      "                                            min_child_weight=1,\n",
      "                                            min_samples_split=1, missing=nan,\n",
      "                                            monotone_constraints='()',\n",
      "                                            n_estimators=10, n_jobs=8,\n",
      "                                            num_parallel_tree=1,\n",
      "                                            predictor='auto', random_state=3,\n",
      "                                            reg_alpha=0, reg_lambda=1,\n",
      "                                            scale_pos_weight=1, subsample=1,\n",
      "                                            tree_method='exact',\n",
      "                                            validate_parameters=1,\n",
      "                                            verbosity=None))])\n",
      "kfold  11  score: 95.81%\n",
      "kfold  11  score: 98.56%\n",
      "[21:48:16] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:48:16] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "VotingClassifier(estimators=[('gb',\n",
      "                              GradientBoostingClassifier(max_depth=12,\n",
      "                                                         min_samples_split=12,\n",
      "                                                         n_estimators=5,\n",
      "                                                         random_state=3)),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(max_depth=12,\n",
      "                                                     n_estimators=10,\n",
      "                                                     random_state=3)),\n",
      "                             ('xgb',\n",
      "                              XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "                                            colsample_bylevel=1,\n",
      "                                            colsample_bynode=1,\n",
      "                                            colsample_bytree=1,\n",
      "                                            enable_categorical=False, gamma=0,\n",
      "                                            gpu...\n",
      "                                            interaction_constraints='',\n",
      "                                            learning_rate=0.300000012,\n",
      "                                            max_delta_step=0, max_depth=12,\n",
      "                                            max_leaf_nodes=None,\n",
      "                                            min_child_weight=1,\n",
      "                                            min_samples_split=1, missing=nan,\n",
      "                                            monotone_constraints='()',\n",
      "                                            n_estimators=10, n_jobs=8,\n",
      "                                            num_parallel_tree=1,\n",
      "                                            predictor='auto', random_state=3,\n",
      "                                            reg_alpha=0, reg_lambda=1,\n",
      "                                            scale_pos_weight=1, subsample=1,\n",
      "                                            tree_method='exact',\n",
      "                                            validate_parameters=1,\n",
      "                                            verbosity=None))])\n",
      "kfold  11  score: 92.95%\n",
      "kfold  11  score: 98.24%\n",
      "[21:48:16] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:48:16] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "VotingClassifier(estimators=[('gb',\n",
      "                              GradientBoostingClassifier(max_depth=12,\n",
      "                                                         min_samples_split=12,\n",
      "                                                         n_estimators=5,\n",
      "                                                         random_state=3)),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(max_depth=12,\n",
      "                                                     n_estimators=10,\n",
      "                                                     random_state=3)),\n",
      "                             ('xgb',\n",
      "                              XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "                                            colsample_bylevel=1,\n",
      "                                            colsample_bynode=1,\n",
      "                                            colsample_bytree=1,\n",
      "                                            enable_categorical=False, gamma=0,\n",
      "                                            gpu...\n",
      "                                            interaction_constraints='',\n",
      "                                            learning_rate=0.300000012,\n",
      "                                            max_delta_step=0, max_depth=12,\n",
      "                                            max_leaf_nodes=None,\n",
      "                                            min_child_weight=1,\n",
      "                                            min_samples_split=1, missing=nan,\n",
      "                                            monotone_constraints='()',\n",
      "                                            n_estimators=10, n_jobs=8,\n",
      "                                            num_parallel_tree=1,\n",
      "                                            predictor='auto', random_state=3,\n",
      "                                            reg_alpha=0, reg_lambda=1,\n",
      "                                            scale_pos_weight=1, subsample=1,\n",
      "                                            tree_method='exact',\n",
      "                                            validate_parameters=1,\n",
      "                                            verbosity=None))])\n",
      "kfold  11  score: 92.38%\n",
      "kfold  11  score: 98.26%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21:48:17] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:48:17] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "VotingClassifier(estimators=[('gb',\n",
      "                              GradientBoostingClassifier(max_depth=12,\n",
      "                                                         min_samples_split=12,\n",
      "                                                         n_estimators=5,\n",
      "                                                         random_state=3)),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(max_depth=12,\n",
      "                                                     n_estimators=10,\n",
      "                                                     random_state=3)),\n",
      "                             ('xgb',\n",
      "                              XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "                                            colsample_bylevel=1,\n",
      "                                            colsample_bynode=1,\n",
      "                                            colsample_bytree=1,\n",
      "                                            enable_categorical=False, gamma=0,\n",
      "                                            gpu...\n",
      "                                            interaction_constraints='',\n",
      "                                            learning_rate=0.300000012,\n",
      "                                            max_delta_step=0, max_depth=12,\n",
      "                                            max_leaf_nodes=None,\n",
      "                                            min_child_weight=1,\n",
      "                                            min_samples_split=1, missing=nan,\n",
      "                                            monotone_constraints='()',\n",
      "                                            n_estimators=10, n_jobs=8,\n",
      "                                            num_parallel_tree=1,\n",
      "                                            predictor='auto', random_state=3,\n",
      "                                            reg_alpha=0, reg_lambda=1,\n",
      "                                            scale_pos_weight=1, subsample=1,\n",
      "                                            tree_method='exact',\n",
      "                                            validate_parameters=1,\n",
      "                                            verbosity=None))])\n",
      "kfold  11  score: 93.71%\n",
      "kfold  11  score: 97.29%\n",
      "[21:48:17] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:48:17] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "VotingClassifier(estimators=[('gb',\n",
      "                              GradientBoostingClassifier(max_depth=12,\n",
      "                                                         min_samples_split=12,\n",
      "                                                         n_estimators=5,\n",
      "                                                         random_state=3)),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(max_depth=12,\n",
      "                                                     n_estimators=10,\n",
      "                                                     random_state=3)),\n",
      "                             ('xgb',\n",
      "                              XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "                                            colsample_bylevel=1,\n",
      "                                            colsample_bynode=1,\n",
      "                                            colsample_bytree=1,\n",
      "                                            enable_categorical=False, gamma=0,\n",
      "                                            gpu...\n",
      "                                            interaction_constraints='',\n",
      "                                            learning_rate=0.300000012,\n",
      "                                            max_delta_step=0, max_depth=12,\n",
      "                                            max_leaf_nodes=None,\n",
      "                                            min_child_weight=1,\n",
      "                                            min_samples_split=1, missing=nan,\n",
      "                                            monotone_constraints='()',\n",
      "                                            n_estimators=10, n_jobs=8,\n",
      "                                            num_parallel_tree=1,\n",
      "                                            predictor='auto', random_state=3,\n",
      "                                            reg_alpha=0, reg_lambda=1,\n",
      "                                            scale_pos_weight=1, subsample=1,\n",
      "                                            tree_method='exact',\n",
      "                                            validate_parameters=1,\n",
      "                                            verbosity=None))])\n",
      "kfold  11  score: 93.71%\n",
      "kfold  11  score: 97.84%\n",
      "[21:48:18] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:48:18] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "VotingClassifier(estimators=[('gb',\n",
      "                              GradientBoostingClassifier(max_depth=12,\n",
      "                                                         min_samples_split=12,\n",
      "                                                         n_estimators=5,\n",
      "                                                         random_state=3)),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(max_depth=12,\n",
      "                                                     n_estimators=10,\n",
      "                                                     random_state=3)),\n",
      "                             ('xgb',\n",
      "                              XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "                                            colsample_bylevel=1,\n",
      "                                            colsample_bynode=1,\n",
      "                                            colsample_bytree=1,\n",
      "                                            enable_categorical=False, gamma=0,\n",
      "                                            gpu...\n",
      "                                            interaction_constraints='',\n",
      "                                            learning_rate=0.300000012,\n",
      "                                            max_delta_step=0, max_depth=12,\n",
      "                                            max_leaf_nodes=None,\n",
      "                                            min_child_weight=1,\n",
      "                                            min_samples_split=1, missing=nan,\n",
      "                                            monotone_constraints='()',\n",
      "                                            n_estimators=10, n_jobs=8,\n",
      "                                            num_parallel_tree=1,\n",
      "                                            predictor='auto', random_state=3,\n",
      "                                            reg_alpha=0, reg_lambda=1,\n",
      "                                            scale_pos_weight=1, subsample=1,\n",
      "                                            tree_method='exact',\n",
      "                                            validate_parameters=1,\n",
      "                                            verbosity=None))])\n",
      "kfold  11  score: 93.33%\n",
      "kfold  11  score: 97.31%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21:48:18] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:48:18] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "VotingClassifier(estimators=[('gb',\n",
      "                              GradientBoostingClassifier(max_depth=12,\n",
      "                                                         min_samples_split=12,\n",
      "                                                         n_estimators=5,\n",
      "                                                         random_state=3)),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(max_depth=12,\n",
      "                                                     n_estimators=10,\n",
      "                                                     random_state=3)),\n",
      "                             ('xgb',\n",
      "                              XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "                                            colsample_bylevel=1,\n",
      "                                            colsample_bynode=1,\n",
      "                                            colsample_bytree=1,\n",
      "                                            enable_categorical=False, gamma=0,\n",
      "                                            gpu...\n",
      "                                            interaction_constraints='',\n",
      "                                            learning_rate=0.300000012,\n",
      "                                            max_delta_step=0, max_depth=12,\n",
      "                                            max_leaf_nodes=None,\n",
      "                                            min_child_weight=1,\n",
      "                                            min_samples_split=1, missing=nan,\n",
      "                                            monotone_constraints='()',\n",
      "                                            n_estimators=10, n_jobs=8,\n",
      "                                            num_parallel_tree=1,\n",
      "                                            predictor='auto', random_state=3,\n",
      "                                            reg_alpha=0, reg_lambda=1,\n",
      "                                            scale_pos_weight=1, subsample=1,\n",
      "                                            tree_method='exact',\n",
      "                                            validate_parameters=1,\n",
      "                                            verbosity=None))])\n",
      "kfold  11  score: 93.33%\n",
      "kfold  11  score: 97.50%\n",
      "Average score over k-fold 93.54% (+/-0.94%) [Ensemble Majority]\n",
      "./Files_Majority_vote/128_features/filter12.csv\n",
      "n_estimators = 100 \n",
      "\n",
      "10-fold cross validation for file :  12\n",
      "GradientBoostingClassifier(max_depth=12, min_samples_split=12, n_estimators=5,\n",
      "                           random_state=3)\n",
      "kfold  12  score: 90.25%\n",
      "kfold  12  score: 93.28%\n",
      "GradientBoostingClassifier(max_depth=12, min_samples_split=12, n_estimators=5,\n",
      "                           random_state=3)\n",
      "kfold  12  score: 88.15%\n",
      "kfold  12  score: 93.13%\n",
      "GradientBoostingClassifier(max_depth=12, min_samples_split=12, n_estimators=5,\n",
      "                           random_state=3)\n",
      "kfold  12  score: 89.29%\n",
      "kfold  12  score: 92.47%\n",
      "GradientBoostingClassifier(max_depth=12, min_samples_split=12, n_estimators=5,\n",
      "                           random_state=3)\n",
      "kfold  12  score: 90.06%\n",
      "kfold  12  score: 94.13%\n",
      "GradientBoostingClassifier(max_depth=12, min_samples_split=12, n_estimators=5,\n",
      "                           random_state=3)\n",
      "kfold  12  score: 88.89%\n",
      "kfold  12  score: 94.07%\n",
      "GradientBoostingClassifier(max_depth=12, min_samples_split=12, n_estimators=5,\n",
      "                           random_state=3)\n",
      "kfold  12  score: 89.85%\n",
      "kfold  12  score: 92.00%\n",
      "GradientBoostingClassifier(max_depth=12, min_samples_split=12, n_estimators=5,\n",
      "                           random_state=3)\n",
      "kfold  12  score: 89.46%\n",
      "kfold  12  score: 93.96%\n",
      "GradientBoostingClassifier(max_depth=12, min_samples_split=12, n_estimators=5,\n",
      "                           random_state=3)\n",
      "kfold  12  score: 92.91%\n",
      "kfold  12  score: 95.02%\n",
      "GradientBoostingClassifier(max_depth=12, min_samples_split=12, n_estimators=5,\n",
      "                           random_state=3)\n",
      "kfold  12  score: 90.80%\n",
      "kfold  12  score: 93.26%\n",
      "GradientBoostingClassifier(max_depth=12, min_samples_split=12, n_estimators=5,\n",
      "                           random_state=3)\n",
      "kfold  12  score: 91.00%\n",
      "kfold  12  score: 93.62%\n",
      "Average score over k-fold 90.07% (+/-1.25%) [Gradient Boosting]\n",
      "RandomForestClassifier(max_depth=12, n_estimators=10, random_state=3)\n",
      "kfold  12  score: 92.73%\n",
      "kfold  12  score: 96.72%\n",
      "RandomForestClassifier(max_depth=12, n_estimators=10, random_state=3)\n",
      "kfold  12  score: 92.35%\n",
      "kfold  12  score: 96.51%\n",
      "RandomForestClassifier(max_depth=12, n_estimators=10, random_state=3)\n",
      "kfold  12  score: 92.35%\n",
      "kfold  12  score: 96.68%\n",
      "RandomForestClassifier(max_depth=12, n_estimators=10, random_state=3)\n",
      "kfold  12  score: 91.59%\n",
      "kfold  12  score: 96.75%\n",
      "RandomForestClassifier(max_depth=12, n_estimators=10, random_state=3)\n",
      "kfold  12  score: 92.53%\n",
      "kfold  12  score: 97.81%\n",
      "RandomForestClassifier(max_depth=12, n_estimators=10, random_state=3)\n",
      "kfold  12  score: 93.10%\n",
      "kfold  12  score: 96.77%\n",
      "RandomForestClassifier(max_depth=12, n_estimators=10, random_state=3)\n",
      "kfold  12  score: 94.06%\n",
      "kfold  12  score: 97.60%\n",
      "RandomForestClassifier(max_depth=12, n_estimators=10, random_state=3)\n",
      "kfold  12  score: 92.91%\n",
      "kfold  12  score: 96.21%\n",
      "RandomForestClassifier(max_depth=12, n_estimators=10, random_state=3)\n",
      "kfold  12  score: 93.30%\n",
      "kfold  12  score: 97.00%\n",
      "RandomForestClassifier(max_depth=12, n_estimators=10, random_state=3)\n",
      "kfold  12  score: 93.49%\n",
      "kfold  12  score: 96.51%\n",
      "Average score over k-fold 92.84% (+/-0.66%) [Random Forest]\n",
      "[21:48:22] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:48:22] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
      "              gamma=0, gpu_id=-1, importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.300000012,\n",
      "              max_delta_step=0, max_depth=12, max_leaf_nodes=None,\n",
      "              min_child_weight=1, min_samples_split=1, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=10, n_jobs=8,\n",
      "              num_parallel_tree=1, predictor='auto', random_state=3,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)\n",
      "kfold  12  score: 95.03%\n",
      "kfold  12  score: 99.32%\n",
      "[21:48:22] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:48:22] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
      "              gamma=0, gpu_id=-1, importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.300000012,\n",
      "              max_delta_step=0, max_depth=12, max_leaf_nodes=None,\n",
      "              min_child_weight=1, min_samples_split=1, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=10, n_jobs=8,\n",
      "              num_parallel_tree=1, predictor='auto', random_state=3,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)\n",
      "kfold  12  score: 94.65%\n",
      "kfold  12  score: 99.04%\n",
      "[21:48:22] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:48:22] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
      "              gamma=0, gpu_id=-1, importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.300000012,\n",
      "              max_delta_step=0, max_depth=12, max_leaf_nodes=None,\n",
      "              min_child_weight=1, min_samples_split=1, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=10, n_jobs=8,\n",
      "              num_parallel_tree=1, predictor='auto', random_state=3,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)\n",
      "kfold  12  score: 95.98%\n",
      "kfold  12  score: 99.64%\n",
      "[21:48:22] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:48:22] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
      "              gamma=0, gpu_id=-1, importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.300000012,\n",
      "              max_delta_step=0, max_depth=12, max_leaf_nodes=None,\n",
      "              min_child_weight=1, min_samples_split=1, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=10, n_jobs=8,\n",
      "              num_parallel_tree=1, predictor='auto', random_state=3,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)\n",
      "kfold  12  score: 95.22%\n",
      "kfold  12  score: 99.49%\n",
      "[21:48:23] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:48:23] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
      "              gamma=0, gpu_id=-1, importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.300000012,\n",
      "              max_delta_step=0, max_depth=12, max_leaf_nodes=None,\n",
      "              min_child_weight=1, min_samples_split=1, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=10, n_jobs=8,\n",
      "              num_parallel_tree=1, predictor='auto', random_state=3,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)\n",
      "kfold  12  score: 92.91%\n",
      "kfold  12  score: 99.51%\n",
      "[21:48:23] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:48:23] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
      "              gamma=0, gpu_id=-1, importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.300000012,\n",
      "              max_delta_step=0, max_depth=12, max_leaf_nodes=None,\n",
      "              min_child_weight=1, min_samples_split=1, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=10, n_jobs=8,\n",
      "              num_parallel_tree=1, predictor='auto', random_state=3,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)\n",
      "kfold  12  score: 95.79%\n",
      "kfold  12  score: 99.40%\n",
      "[21:48:23] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:48:23] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
      "              gamma=0, gpu_id=-1, importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.300000012,\n",
      "              max_delta_step=0, max_depth=12, max_leaf_nodes=None,\n",
      "              min_child_weight=1, min_samples_split=1, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=10, n_jobs=8,\n",
      "              num_parallel_tree=1, predictor='auto', random_state=3,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)\n",
      "kfold  12  score: 94.06%\n",
      "kfold  12  score: 99.51%\n",
      "[21:48:23] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:48:23] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
      "              gamma=0, gpu_id=-1, importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.300000012,\n",
      "              max_delta_step=0, max_depth=12, max_leaf_nodes=None,\n",
      "              min_child_weight=1, min_samples_split=1, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=10, n_jobs=8,\n",
      "              num_parallel_tree=1, predictor='auto', random_state=3,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)\n",
      "kfold  12  score: 94.64%\n",
      "kfold  12  score: 99.32%\n",
      "[21:48:23] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:48:23] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
      "              gamma=0, gpu_id=-1, importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.300000012,\n",
      "              max_delta_step=0, max_depth=12, max_leaf_nodes=None,\n",
      "              min_child_weight=1, min_samples_split=1, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=10, n_jobs=8,\n",
      "              num_parallel_tree=1, predictor='auto', random_state=3,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)\n",
      "kfold  12  score: 94.83%\n",
      "kfold  12  score: 99.45%\n",
      "[21:48:23] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:48:23] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
      "              gamma=0, gpu_id=-1, importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.300000012,\n",
      "              max_delta_step=0, max_depth=12, max_leaf_nodes=None,\n",
      "              min_child_weight=1, min_samples_split=1, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=10, n_jobs=8,\n",
      "              num_parallel_tree=1, predictor='auto', random_state=3,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)\n",
      "kfold  12  score: 95.21%\n",
      "kfold  12  score: 99.34%\n",
      "Average score over k-fold 94.83% (+/-0.83%) [XGBClassifier]\n",
      "[21:48:24] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:48:24] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "VotingClassifier(estimators=[('gb',\n",
      "                              GradientBoostingClassifier(max_depth=12,\n",
      "                                                         min_samples_split=12,\n",
      "                                                         n_estimators=5,\n",
      "                                                         random_state=3)),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(max_depth=12,\n",
      "                                                     n_estimators=10,\n",
      "                                                     random_state=3)),\n",
      "                             ('xgb',\n",
      "                              XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "                                            colsample_bylevel=1,\n",
      "                                            colsample_bynode=1,\n",
      "                                            colsample_bytree=1,\n",
      "                                            enable_categorical=False, gamma=0,\n",
      "                                            gpu...\n",
      "                                            interaction_constraints='',\n",
      "                                            learning_rate=0.300000012,\n",
      "                                            max_delta_step=0, max_depth=12,\n",
      "                                            max_leaf_nodes=None,\n",
      "                                            min_child_weight=1,\n",
      "                                            min_samples_split=1, missing=nan,\n",
      "                                            monotone_constraints='()',\n",
      "                                            n_estimators=10, n_jobs=8,\n",
      "                                            num_parallel_tree=1,\n",
      "                                            predictor='auto', random_state=3,\n",
      "                                            reg_alpha=0, reg_lambda=1,\n",
      "                                            scale_pos_weight=1, subsample=1,\n",
      "                                            tree_method='exact',\n",
      "                                            validate_parameters=1,\n",
      "                                            verbosity=None))])\n",
      "kfold  12  score: 93.31%\n",
      "kfold  12  score: 97.00%\n",
      "[21:48:25] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:48:25] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "VotingClassifier(estimators=[('gb',\n",
      "                              GradientBoostingClassifier(max_depth=12,\n",
      "                                                         min_samples_split=12,\n",
      "                                                         n_estimators=5,\n",
      "                                                         random_state=3)),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(max_depth=12,\n",
      "                                                     n_estimators=10,\n",
      "                                                     random_state=3)),\n",
      "                             ('xgb',\n",
      "                              XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "                                            colsample_bylevel=1,\n",
      "                                            colsample_bynode=1,\n",
      "                                            colsample_bytree=1,\n",
      "                                            enable_categorical=False, gamma=0,\n",
      "                                            gpu...\n",
      "                                            interaction_constraints='',\n",
      "                                            learning_rate=0.300000012,\n",
      "                                            max_delta_step=0, max_depth=12,\n",
      "                                            max_leaf_nodes=None,\n",
      "                                            min_child_weight=1,\n",
      "                                            min_samples_split=1, missing=nan,\n",
      "                                            monotone_constraints='()',\n",
      "                                            n_estimators=10, n_jobs=8,\n",
      "                                            num_parallel_tree=1,\n",
      "                                            predictor='auto', random_state=3,\n",
      "                                            reg_alpha=0, reg_lambda=1,\n",
      "                                            scale_pos_weight=1, subsample=1,\n",
      "                                            tree_method='exact',\n",
      "                                            validate_parameters=1,\n",
      "                                            verbosity=None))])\n",
      "kfold  12  score: 92.73%\n",
      "kfold  12  score: 97.00%\n",
      "[21:48:25] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:48:25] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "VotingClassifier(estimators=[('gb',\n",
      "                              GradientBoostingClassifier(max_depth=12,\n",
      "                                                         min_samples_split=12,\n",
      "                                                         n_estimators=5,\n",
      "                                                         random_state=3)),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(max_depth=12,\n",
      "                                                     n_estimators=10,\n",
      "                                                     random_state=3)),\n",
      "                             ('xgb',\n",
      "                              XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "                                            colsample_bylevel=1,\n",
      "                                            colsample_bynode=1,\n",
      "                                            colsample_bytree=1,\n",
      "                                            enable_categorical=False, gamma=0,\n",
      "                                            gpu...\n",
      "                                            interaction_constraints='',\n",
      "                                            learning_rate=0.300000012,\n",
      "                                            max_delta_step=0, max_depth=12,\n",
      "                                            max_leaf_nodes=None,\n",
      "                                            min_child_weight=1,\n",
      "                                            min_samples_split=1, missing=nan,\n",
      "                                            monotone_constraints='()',\n",
      "                                            n_estimators=10, n_jobs=8,\n",
      "                                            num_parallel_tree=1,\n",
      "                                            predictor='auto', random_state=3,\n",
      "                                            reg_alpha=0, reg_lambda=1,\n",
      "                                            scale_pos_weight=1, subsample=1,\n",
      "                                            tree_method='exact',\n",
      "                                            validate_parameters=1,\n",
      "                                            verbosity=None))])\n",
      "kfold  12  score: 93.88%\n",
      "kfold  12  score: 97.57%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21:48:26] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:48:26] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "VotingClassifier(estimators=[('gb',\n",
      "                              GradientBoostingClassifier(max_depth=12,\n",
      "                                                         min_samples_split=12,\n",
      "                                                         n_estimators=5,\n",
      "                                                         random_state=3)),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(max_depth=12,\n",
      "                                                     n_estimators=10,\n",
      "                                                     random_state=3)),\n",
      "                             ('xgb',\n",
      "                              XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "                                            colsample_bylevel=1,\n",
      "                                            colsample_bynode=1,\n",
      "                                            colsample_bytree=1,\n",
      "                                            enable_categorical=False, gamma=0,\n",
      "                                            gpu...\n",
      "                                            interaction_constraints='',\n",
      "                                            learning_rate=0.300000012,\n",
      "                                            max_delta_step=0, max_depth=12,\n",
      "                                            max_leaf_nodes=None,\n",
      "                                            min_child_weight=1,\n",
      "                                            min_samples_split=1, missing=nan,\n",
      "                                            monotone_constraints='()',\n",
      "                                            n_estimators=10, n_jobs=8,\n",
      "                                            num_parallel_tree=1,\n",
      "                                            predictor='auto', random_state=3,\n",
      "                                            reg_alpha=0, reg_lambda=1,\n",
      "                                            scale_pos_weight=1, subsample=1,\n",
      "                                            tree_method='exact',\n",
      "                                            validate_parameters=1,\n",
      "                                            verbosity=None))])\n",
      "kfold  12  score: 92.73%\n",
      "kfold  12  score: 97.17%\n",
      "[21:48:26] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:48:26] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "VotingClassifier(estimators=[('gb',\n",
      "                              GradientBoostingClassifier(max_depth=12,\n",
      "                                                         min_samples_split=12,\n",
      "                                                         n_estimators=5,\n",
      "                                                         random_state=3)),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(max_depth=12,\n",
      "                                                     n_estimators=10,\n",
      "                                                     random_state=3)),\n",
      "                             ('xgb',\n",
      "                              XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "                                            colsample_bylevel=1,\n",
      "                                            colsample_bynode=1,\n",
      "                                            colsample_bytree=1,\n",
      "                                            enable_categorical=False, gamma=0,\n",
      "                                            gpu...\n",
      "                                            interaction_constraints='',\n",
      "                                            learning_rate=0.300000012,\n",
      "                                            max_delta_step=0, max_depth=12,\n",
      "                                            max_leaf_nodes=None,\n",
      "                                            min_child_weight=1,\n",
      "                                            min_samples_split=1, missing=nan,\n",
      "                                            monotone_constraints='()',\n",
      "                                            n_estimators=10, n_jobs=8,\n",
      "                                            num_parallel_tree=1,\n",
      "                                            predictor='auto', random_state=3,\n",
      "                                            reg_alpha=0, reg_lambda=1,\n",
      "                                            scale_pos_weight=1, subsample=1,\n",
      "                                            tree_method='exact',\n",
      "                                            validate_parameters=1,\n",
      "                                            verbosity=None))])\n",
      "kfold  12  score: 91.76%\n",
      "kfold  12  score: 98.19%\n",
      "[21:48:27] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:48:27] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "VotingClassifier(estimators=[('gb',\n",
      "                              GradientBoostingClassifier(max_depth=12,\n",
      "                                                         min_samples_split=12,\n",
      "                                                         n_estimators=5,\n",
      "                                                         random_state=3)),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(max_depth=12,\n",
      "                                                     n_estimators=10,\n",
      "                                                     random_state=3)),\n",
      "                             ('xgb',\n",
      "                              XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "                                            colsample_bylevel=1,\n",
      "                                            colsample_bynode=1,\n",
      "                                            colsample_bytree=1,\n",
      "                                            enable_categorical=False, gamma=0,\n",
      "                                            gpu...\n",
      "                                            interaction_constraints='',\n",
      "                                            learning_rate=0.300000012,\n",
      "                                            max_delta_step=0, max_depth=12,\n",
      "                                            max_leaf_nodes=None,\n",
      "                                            min_child_weight=1,\n",
      "                                            min_samples_split=1, missing=nan,\n",
      "                                            monotone_constraints='()',\n",
      "                                            n_estimators=10, n_jobs=8,\n",
      "                                            num_parallel_tree=1,\n",
      "                                            predictor='auto', random_state=3,\n",
      "                                            reg_alpha=0, reg_lambda=1,\n",
      "                                            scale_pos_weight=1, subsample=1,\n",
      "                                            tree_method='exact',\n",
      "                                            validate_parameters=1,\n",
      "                                            verbosity=None))])\n",
      "kfold  12  score: 93.68%\n",
      "kfold  12  score: 97.09%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21:48:27] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:48:27] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "VotingClassifier(estimators=[('gb',\n",
      "                              GradientBoostingClassifier(max_depth=12,\n",
      "                                                         min_samples_split=12,\n",
      "                                                         n_estimators=5,\n",
      "                                                         random_state=3)),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(max_depth=12,\n",
      "                                                     n_estimators=10,\n",
      "                                                     random_state=3)),\n",
      "                             ('xgb',\n",
      "                              XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "                                            colsample_bylevel=1,\n",
      "                                            colsample_bynode=1,\n",
      "                                            colsample_bytree=1,\n",
      "                                            enable_categorical=False, gamma=0,\n",
      "                                            gpu...\n",
      "                                            interaction_constraints='',\n",
      "                                            learning_rate=0.300000012,\n",
      "                                            max_delta_step=0, max_depth=12,\n",
      "                                            max_leaf_nodes=None,\n",
      "                                            min_child_weight=1,\n",
      "                                            min_samples_split=1, missing=nan,\n",
      "                                            monotone_constraints='()',\n",
      "                                            n_estimators=10, n_jobs=8,\n",
      "                                            num_parallel_tree=1,\n",
      "                                            predictor='auto', random_state=3,\n",
      "                                            reg_alpha=0, reg_lambda=1,\n",
      "                                            scale_pos_weight=1, subsample=1,\n",
      "                                            tree_method='exact',\n",
      "                                            validate_parameters=1,\n",
      "                                            verbosity=None))])\n",
      "kfold  12  score: 94.25%\n",
      "kfold  12  score: 98.13%\n",
      "[21:48:28] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:48:28] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "VotingClassifier(estimators=[('gb',\n",
      "                              GradientBoostingClassifier(max_depth=12,\n",
      "                                                         min_samples_split=12,\n",
      "                                                         n_estimators=5,\n",
      "                                                         random_state=3)),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(max_depth=12,\n",
      "                                                     n_estimators=10,\n",
      "                                                     random_state=3)),\n",
      "                             ('xgb',\n",
      "                              XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "                                            colsample_bylevel=1,\n",
      "                                            colsample_bynode=1,\n",
      "                                            colsample_bytree=1,\n",
      "                                            enable_categorical=False, gamma=0,\n",
      "                                            gpu...\n",
      "                                            interaction_constraints='',\n",
      "                                            learning_rate=0.300000012,\n",
      "                                            max_delta_step=0, max_depth=12,\n",
      "                                            max_leaf_nodes=None,\n",
      "                                            min_child_weight=1,\n",
      "                                            min_samples_split=1, missing=nan,\n",
      "                                            monotone_constraints='()',\n",
      "                                            n_estimators=10, n_jobs=8,\n",
      "                                            num_parallel_tree=1,\n",
      "                                            predictor='auto', random_state=3,\n",
      "                                            reg_alpha=0, reg_lambda=1,\n",
      "                                            scale_pos_weight=1, subsample=1,\n",
      "                                            tree_method='exact',\n",
      "                                            validate_parameters=1,\n",
      "                                            verbosity=None))])\n",
      "kfold  12  score: 93.30%\n",
      "kfold  12  score: 97.09%\n",
      "[21:48:28] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:48:28] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "VotingClassifier(estimators=[('gb',\n",
      "                              GradientBoostingClassifier(max_depth=12,\n",
      "                                                         min_samples_split=12,\n",
      "                                                         n_estimators=5,\n",
      "                                                         random_state=3)),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(max_depth=12,\n",
      "                                                     n_estimators=10,\n",
      "                                                     random_state=3)),\n",
      "                             ('xgb',\n",
      "                              XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "                                            colsample_bylevel=1,\n",
      "                                            colsample_bynode=1,\n",
      "                                            colsample_bytree=1,\n",
      "                                            enable_categorical=False, gamma=0,\n",
      "                                            gpu...\n",
      "                                            interaction_constraints='',\n",
      "                                            learning_rate=0.300000012,\n",
      "                                            max_delta_step=0, max_depth=12,\n",
      "                                            max_leaf_nodes=None,\n",
      "                                            min_child_weight=1,\n",
      "                                            min_samples_split=1, missing=nan,\n",
      "                                            monotone_constraints='()',\n",
      "                                            n_estimators=10, n_jobs=8,\n",
      "                                            num_parallel_tree=1,\n",
      "                                            predictor='auto', random_state=3,\n",
      "                                            reg_alpha=0, reg_lambda=1,\n",
      "                                            scale_pos_weight=1, subsample=1,\n",
      "                                            tree_method='exact',\n",
      "                                            validate_parameters=1,\n",
      "                                            verbosity=None))])\n",
      "kfold  12  score: 94.06%\n",
      "kfold  12  score: 97.83%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21:48:29] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:48:29] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "VotingClassifier(estimators=[('gb',\n",
      "                              GradientBoostingClassifier(max_depth=12,\n",
      "                                                         min_samples_split=12,\n",
      "                                                         n_estimators=5,\n",
      "                                                         random_state=3)),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(max_depth=12,\n",
      "                                                     n_estimators=10,\n",
      "                                                     random_state=3)),\n",
      "                             ('xgb',\n",
      "                              XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "                                            colsample_bylevel=1,\n",
      "                                            colsample_bynode=1,\n",
      "                                            colsample_bytree=1,\n",
      "                                            enable_categorical=False, gamma=0,\n",
      "                                            gpu...\n",
      "                                            interaction_constraints='',\n",
      "                                            learning_rate=0.300000012,\n",
      "                                            max_delta_step=0, max_depth=12,\n",
      "                                            max_leaf_nodes=None,\n",
      "                                            min_child_weight=1,\n",
      "                                            min_samples_split=1, missing=nan,\n",
      "                                            monotone_constraints='()',\n",
      "                                            n_estimators=10, n_jobs=8,\n",
      "                                            num_parallel_tree=1,\n",
      "                                            predictor='auto', random_state=3,\n",
      "                                            reg_alpha=0, reg_lambda=1,\n",
      "                                            scale_pos_weight=1, subsample=1,\n",
      "                                            tree_method='exact',\n",
      "                                            validate_parameters=1,\n",
      "                                            verbosity=None))])\n",
      "kfold  12  score: 93.87%\n",
      "kfold  12  score: 97.28%\n",
      "Average score over k-fold 93.36% (+/-0.73%) [Ensemble Majority]\n",
      "./Files_Majority_vote/128_features/filter13.csv\n",
      "n_estimators = 100 \n",
      "\n",
      "10-fold cross validation for file :  13\n",
      "GradientBoostingClassifier(max_depth=12, min_samples_split=12, n_estimators=5,\n",
      "                           random_state=3)\n",
      "kfold  13  score: 89.39%\n",
      "kfold  13  score: 90.11%\n",
      "GradientBoostingClassifier(max_depth=12, min_samples_split=12, n_estimators=5,\n",
      "                           random_state=3)\n",
      "kfold  13  score: 87.10%\n",
      "kfold  13  score: 89.00%\n",
      "GradientBoostingClassifier(max_depth=12, min_samples_split=12, n_estimators=5,\n",
      "                           random_state=3)\n",
      "kfold  13  score: 91.46%\n",
      "kfold  13  score: 93.87%\n",
      "GradientBoostingClassifier(max_depth=12, min_samples_split=12, n_estimators=5,\n",
      "                           random_state=3)\n",
      "kfold  13  score: 88.99%\n",
      "kfold  13  score: 91.23%\n",
      "GradientBoostingClassifier(max_depth=12, min_samples_split=12, n_estimators=5,\n",
      "                           random_state=3)\n",
      "kfold  13  score: 84.44%\n",
      "kfold  13  score: 88.58%\n",
      "GradientBoostingClassifier(max_depth=12, min_samples_split=12, n_estimators=5,\n",
      "                           random_state=3)\n",
      "kfold  13  score: 89.75%\n",
      "kfold  13  score: 92.05%\n",
      "GradientBoostingClassifier(max_depth=12, min_samples_split=12, n_estimators=5,\n",
      "                           random_state=3)\n",
      "kfold  13  score: 86.72%\n",
      "kfold  13  score: 89.44%\n",
      "GradientBoostingClassifier(max_depth=12, min_samples_split=12, n_estimators=5,\n",
      "                           random_state=3)\n",
      "kfold  13  score: 86.53%\n",
      "kfold  13  score: 90.43%\n",
      "GradientBoostingClassifier(max_depth=12, min_samples_split=12, n_estimators=5,\n",
      "                           random_state=3)\n",
      "kfold  13  score: 88.43%\n",
      "kfold  13  score: 91.06%\n",
      "GradientBoostingClassifier(max_depth=12, min_samples_split=12, n_estimators=5,\n",
      "                           random_state=3)\n",
      "kfold  13  score: 88.43%\n",
      "kfold  13  score: 89.40%\n",
      "Average score over k-fold 88.12% (+/-1.88%) [Gradient Boosting]\n",
      "RandomForestClassifier(max_depth=12, n_estimators=10, random_state=3)\n",
      "kfold  13  score: 92.80%\n",
      "kfold  13  score: 96.25%\n",
      "RandomForestClassifier(max_depth=12, n_estimators=10, random_state=3)\n",
      "kfold  13  score: 89.94%\n",
      "kfold  13  score: 96.80%\n",
      "RandomForestClassifier(max_depth=12, n_estimators=10, random_state=3)\n",
      "kfold  13  score: 93.17%\n",
      "kfold  13  score: 97.68%\n",
      "RandomForestClassifier(max_depth=12, n_estimators=10, random_state=3)\n",
      "kfold  13  score: 91.46%\n",
      "kfold  13  score: 96.84%\n",
      "RandomForestClassifier(max_depth=12, n_estimators=10, random_state=3)\n",
      "kfold  13  score: 90.13%\n",
      "kfold  13  score: 95.95%\n",
      "RandomForestClassifier(max_depth=12, n_estimators=10, random_state=3)\n",
      "kfold  13  score: 93.36%\n",
      "kfold  13  score: 96.96%\n",
      "RandomForestClassifier(max_depth=12, n_estimators=10, random_state=3)\n",
      "kfold  13  score: 91.27%\n",
      "kfold  13  score: 95.05%\n",
      "RandomForestClassifier(max_depth=12, n_estimators=10, random_state=3)\n",
      "kfold  13  score: 92.41%\n",
      "kfold  13  score: 96.46%\n",
      "RandomForestClassifier(max_depth=12, n_estimators=10, random_state=3)\n",
      "kfold  13  score: 93.36%\n",
      "kfold  13  score: 97.70%\n",
      "RandomForestClassifier(max_depth=12, n_estimators=10, random_state=3)\n",
      "kfold  13  score: 91.46%\n",
      "kfold  13  score: 96.02%\n",
      "Average score over k-fold 91.94% (+/-1.21%) [Random Forest]\n",
      "[21:48:33] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:48:33] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
      "              gamma=0, gpu_id=-1, importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.300000012,\n",
      "              max_delta_step=0, max_depth=12, max_leaf_nodes=None,\n",
      "              min_child_weight=1, min_samples_split=1, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=10, n_jobs=8,\n",
      "              num_parallel_tree=1, predictor='auto', random_state=3,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)\n",
      "kfold  13  score: 94.70%\n",
      "kfold  13  score: 99.07%\n",
      "[21:48:33] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:48:33] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
      "              gamma=0, gpu_id=-1, importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.300000012,\n",
      "              max_delta_step=0, max_depth=12, max_leaf_nodes=None,\n",
      "              min_child_weight=1, min_samples_split=1, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=10, n_jobs=8,\n",
      "              num_parallel_tree=1, predictor='auto', random_state=3,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)\n",
      "kfold  13  score: 93.74%\n",
      "kfold  13  score: 99.01%\n",
      "[21:48:33] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:48:33] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
      "              gamma=0, gpu_id=-1, importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.300000012,\n",
      "              max_delta_step=0, max_depth=12, max_leaf_nodes=None,\n",
      "              min_child_weight=1, min_samples_split=1, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=10, n_jobs=8,\n",
      "              num_parallel_tree=1, predictor='auto', random_state=3,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)\n",
      "kfold  13  score: 94.69%\n",
      "kfold  13  score: 99.22%\n",
      "[21:48:33] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:48:33] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
      "              gamma=0, gpu_id=-1, importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.300000012,\n",
      "              max_delta_step=0, max_depth=12, max_leaf_nodes=None,\n",
      "              min_child_weight=1, min_samples_split=1, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=10, n_jobs=8,\n",
      "              num_parallel_tree=1, predictor='auto', random_state=3,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)\n",
      "kfold  13  score: 95.45%\n",
      "kfold  13  score: 99.24%\n",
      "[21:48:33] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:48:33] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
      "              gamma=0, gpu_id=-1, importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.300000012,\n",
      "              max_delta_step=0, max_depth=12, max_leaf_nodes=None,\n",
      "              min_child_weight=1, min_samples_split=1, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=10, n_jobs=8,\n",
      "              num_parallel_tree=1, predictor='auto', random_state=3,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)\n",
      "kfold  13  score: 93.17%\n",
      "kfold  13  score: 98.50%\n",
      "[21:48:33] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:48:33] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
      "              gamma=0, gpu_id=-1, importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.300000012,\n",
      "              max_delta_step=0, max_depth=12, max_leaf_nodes=None,\n",
      "              min_child_weight=1, min_samples_split=1, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=10, n_jobs=8,\n",
      "              num_parallel_tree=1, predictor='auto', random_state=3,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)\n",
      "kfold  13  score: 93.74%\n",
      "kfold  13  score: 98.80%\n",
      "[21:48:33] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:48:33] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
      "              gamma=0, gpu_id=-1, importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.300000012,\n",
      "              max_delta_step=0, max_depth=12, max_leaf_nodes=None,\n",
      "              min_child_weight=1, min_samples_split=1, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=10, n_jobs=8,\n",
      "              num_parallel_tree=1, predictor='auto', random_state=3,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)\n",
      "kfold  13  score: 94.88%\n",
      "kfold  13  score: 99.05%\n",
      "[21:48:34] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:48:34] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
      "              gamma=0, gpu_id=-1, importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.300000012,\n",
      "              max_delta_step=0, max_depth=12, max_leaf_nodes=None,\n",
      "              min_child_weight=1, min_samples_split=1, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=10, n_jobs=8,\n",
      "              num_parallel_tree=1, predictor='auto', random_state=3,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)\n",
      "kfold  13  score: 95.64%\n",
      "kfold  13  score: 99.03%\n",
      "[21:48:34] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:48:34] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
      "              gamma=0, gpu_id=-1, importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.300000012,\n",
      "              max_delta_step=0, max_depth=12, max_leaf_nodes=None,\n",
      "              min_child_weight=1, min_samples_split=1, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=10, n_jobs=8,\n",
      "              num_parallel_tree=1, predictor='auto', random_state=3,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)\n",
      "kfold  13  score: 95.45%\n",
      "kfold  13  score: 98.69%\n",
      "[21:48:34] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:48:34] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
      "              gamma=0, gpu_id=-1, importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.300000012,\n",
      "              max_delta_step=0, max_depth=12, max_leaf_nodes=None,\n",
      "              min_child_weight=1, min_samples_split=1, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=10, n_jobs=8,\n",
      "              num_parallel_tree=1, predictor='auto', random_state=3,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)\n",
      "kfold  13  score: 94.88%\n",
      "kfold  13  score: 98.67%\n",
      "Average score over k-fold 94.63% (+/-0.79%) [XGBClassifier]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21:48:34] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:48:34] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "VotingClassifier(estimators=[('gb',\n",
      "                              GradientBoostingClassifier(max_depth=12,\n",
      "                                                         min_samples_split=12,\n",
      "                                                         n_estimators=5,\n",
      "                                                         random_state=3)),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(max_depth=12,\n",
      "                                                     n_estimators=10,\n",
      "                                                     random_state=3)),\n",
      "                             ('xgb',\n",
      "                              XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "                                            colsample_bylevel=1,\n",
      "                                            colsample_bynode=1,\n",
      "                                            colsample_bytree=1,\n",
      "                                            enable_categorical=False, gamma=0,\n",
      "                                            gpu...\n",
      "                                            interaction_constraints='',\n",
      "                                            learning_rate=0.300000012,\n",
      "                                            max_delta_step=0, max_depth=12,\n",
      "                                            max_leaf_nodes=None,\n",
      "                                            min_child_weight=1,\n",
      "                                            min_samples_split=1, missing=nan,\n",
      "                                            monotone_constraints='()',\n",
      "                                            n_estimators=10, n_jobs=8,\n",
      "                                            num_parallel_tree=1,\n",
      "                                            predictor='auto', random_state=3,\n",
      "                                            reg_alpha=0, reg_lambda=1,\n",
      "                                            scale_pos_weight=1, subsample=1,\n",
      "                                            tree_method='exact',\n",
      "                                            validate_parameters=1,\n",
      "                                            verbosity=None))])\n",
      "kfold  13  score: 93.94%\n",
      "kfold  13  score: 96.37%\n",
      "[21:48:35] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:48:35] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "VotingClassifier(estimators=[('gb',\n",
      "                              GradientBoostingClassifier(max_depth=12,\n",
      "                                                         min_samples_split=12,\n",
      "                                                         n_estimators=5,\n",
      "                                                         random_state=3)),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(max_depth=12,\n",
      "                                                     n_estimators=10,\n",
      "                                                     random_state=3)),\n",
      "                             ('xgb',\n",
      "                              XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "                                            colsample_bylevel=1,\n",
      "                                            colsample_bynode=1,\n",
      "                                            colsample_bytree=1,\n",
      "                                            enable_categorical=False, gamma=0,\n",
      "                                            gpu...\n",
      "                                            interaction_constraints='',\n",
      "                                            learning_rate=0.300000012,\n",
      "                                            max_delta_step=0, max_depth=12,\n",
      "                                            max_leaf_nodes=None,\n",
      "                                            min_child_weight=1,\n",
      "                                            min_samples_split=1, missing=nan,\n",
      "                                            monotone_constraints='()',\n",
      "                                            n_estimators=10, n_jobs=8,\n",
      "                                            num_parallel_tree=1,\n",
      "                                            predictor='auto', random_state=3,\n",
      "                                            reg_alpha=0, reg_lambda=1,\n",
      "                                            scale_pos_weight=1, subsample=1,\n",
      "                                            tree_method='exact',\n",
      "                                            validate_parameters=1,\n",
      "                                            verbosity=None))])\n",
      "kfold  13  score: 90.89%\n",
      "kfold  13  score: 96.56%\n",
      "[21:48:36] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:48:36] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "VotingClassifier(estimators=[('gb',\n",
      "                              GradientBoostingClassifier(max_depth=12,\n",
      "                                                         min_samples_split=12,\n",
      "                                                         n_estimators=5,\n",
      "                                                         random_state=3)),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(max_depth=12,\n",
      "                                                     n_estimators=10,\n",
      "                                                     random_state=3)),\n",
      "                             ('xgb',\n",
      "                              XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "                                            colsample_bylevel=1,\n",
      "                                            colsample_bynode=1,\n",
      "                                            colsample_bytree=1,\n",
      "                                            enable_categorical=False, gamma=0,\n",
      "                                            gpu...\n",
      "                                            interaction_constraints='',\n",
      "                                            learning_rate=0.300000012,\n",
      "                                            max_delta_step=0, max_depth=12,\n",
      "                                            max_leaf_nodes=None,\n",
      "                                            min_child_weight=1,\n",
      "                                            min_samples_split=1, missing=nan,\n",
      "                                            monotone_constraints='()',\n",
      "                                            n_estimators=10, n_jobs=8,\n",
      "                                            num_parallel_tree=1,\n",
      "                                            predictor='auto', random_state=3,\n",
      "                                            reg_alpha=0, reg_lambda=1,\n",
      "                                            scale_pos_weight=1, subsample=1,\n",
      "                                            tree_method='exact',\n",
      "                                            validate_parameters=1,\n",
      "                                            verbosity=None))])\n",
      "kfold  13  score: 93.74%\n",
      "kfold  13  score: 98.23%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21:48:36] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:48:36] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "VotingClassifier(estimators=[('gb',\n",
      "                              GradientBoostingClassifier(max_depth=12,\n",
      "                                                         min_samples_split=12,\n",
      "                                                         n_estimators=5,\n",
      "                                                         random_state=3)),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(max_depth=12,\n",
      "                                                     n_estimators=10,\n",
      "                                                     random_state=3)),\n",
      "                             ('xgb',\n",
      "                              XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "                                            colsample_bylevel=1,\n",
      "                                            colsample_bynode=1,\n",
      "                                            colsample_bytree=1,\n",
      "                                            enable_categorical=False, gamma=0,\n",
      "                                            gpu...\n",
      "                                            interaction_constraints='',\n",
      "                                            learning_rate=0.300000012,\n",
      "                                            max_delta_step=0, max_depth=12,\n",
      "                                            max_leaf_nodes=None,\n",
      "                                            min_child_weight=1,\n",
      "                                            min_samples_split=1, missing=nan,\n",
      "                                            monotone_constraints='()',\n",
      "                                            n_estimators=10, n_jobs=8,\n",
      "                                            num_parallel_tree=1,\n",
      "                                            predictor='auto', random_state=3,\n",
      "                                            reg_alpha=0, reg_lambda=1,\n",
      "                                            scale_pos_weight=1, subsample=1,\n",
      "                                            tree_method='exact',\n",
      "                                            validate_parameters=1,\n",
      "                                            verbosity=None))])\n",
      "kfold  13  score: 92.22%\n",
      "kfold  13  score: 97.07%\n",
      "[21:48:37] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:48:37] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "VotingClassifier(estimators=[('gb',\n",
      "                              GradientBoostingClassifier(max_depth=12,\n",
      "                                                         min_samples_split=12,\n",
      "                                                         n_estimators=5,\n",
      "                                                         random_state=3)),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(max_depth=12,\n",
      "                                                     n_estimators=10,\n",
      "                                                     random_state=3)),\n",
      "                             ('xgb',\n",
      "                              XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "                                            colsample_bylevel=1,\n",
      "                                            colsample_bynode=1,\n",
      "                                            colsample_bytree=1,\n",
      "                                            enable_categorical=False, gamma=0,\n",
      "                                            gpu...\n",
      "                                            interaction_constraints='',\n",
      "                                            learning_rate=0.300000012,\n",
      "                                            max_delta_step=0, max_depth=12,\n",
      "                                            max_leaf_nodes=None,\n",
      "                                            min_child_weight=1,\n",
      "                                            min_samples_split=1, missing=nan,\n",
      "                                            monotone_constraints='()',\n",
      "                                            n_estimators=10, n_jobs=8,\n",
      "                                            num_parallel_tree=1,\n",
      "                                            predictor='auto', random_state=3,\n",
      "                                            reg_alpha=0, reg_lambda=1,\n",
      "                                            scale_pos_weight=1, subsample=1,\n",
      "                                            tree_method='exact',\n",
      "                                            validate_parameters=1,\n",
      "                                            verbosity=None))])\n",
      "kfold  13  score: 89.37%\n",
      "kfold  13  score: 95.66%\n",
      "[21:48:37] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:48:37] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "VotingClassifier(estimators=[('gb',\n",
      "                              GradientBoostingClassifier(max_depth=12,\n",
      "                                                         min_samples_split=12,\n",
      "                                                         n_estimators=5,\n",
      "                                                         random_state=3)),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(max_depth=12,\n",
      "                                                     n_estimators=10,\n",
      "                                                     random_state=3)),\n",
      "                             ('xgb',\n",
      "                              XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "                                            colsample_bylevel=1,\n",
      "                                            colsample_bynode=1,\n",
      "                                            colsample_bytree=1,\n",
      "                                            enable_categorical=False, gamma=0,\n",
      "                                            gpu...\n",
      "                                            interaction_constraints='',\n",
      "                                            learning_rate=0.300000012,\n",
      "                                            max_delta_step=0, max_depth=12,\n",
      "                                            max_leaf_nodes=None,\n",
      "                                            min_child_weight=1,\n",
      "                                            min_samples_split=1, missing=nan,\n",
      "                                            monotone_constraints='()',\n",
      "                                            n_estimators=10, n_jobs=8,\n",
      "                                            num_parallel_tree=1,\n",
      "                                            predictor='auto', random_state=3,\n",
      "                                            reg_alpha=0, reg_lambda=1,\n",
      "                                            scale_pos_weight=1, subsample=1,\n",
      "                                            tree_method='exact',\n",
      "                                            validate_parameters=1,\n",
      "                                            verbosity=None))])\n",
      "kfold  13  score: 93.17%\n",
      "kfold  13  score: 97.22%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21:48:38] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:48:38] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "VotingClassifier(estimators=[('gb',\n",
      "                              GradientBoostingClassifier(max_depth=12,\n",
      "                                                         min_samples_split=12,\n",
      "                                                         n_estimators=5,\n",
      "                                                         random_state=3)),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(max_depth=12,\n",
      "                                                     n_estimators=10,\n",
      "                                                     random_state=3)),\n",
      "                             ('xgb',\n",
      "                              XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "                                            colsample_bylevel=1,\n",
      "                                            colsample_bynode=1,\n",
      "                                            colsample_bytree=1,\n",
      "                                            enable_categorical=False, gamma=0,\n",
      "                                            gpu...\n",
      "                                            interaction_constraints='',\n",
      "                                            learning_rate=0.300000012,\n",
      "                                            max_delta_step=0, max_depth=12,\n",
      "                                            max_leaf_nodes=None,\n",
      "                                            min_child_weight=1,\n",
      "                                            min_samples_split=1, missing=nan,\n",
      "                                            monotone_constraints='()',\n",
      "                                            n_estimators=10, n_jobs=8,\n",
      "                                            num_parallel_tree=1,\n",
      "                                            predictor='auto', random_state=3,\n",
      "                                            reg_alpha=0, reg_lambda=1,\n",
      "                                            scale_pos_weight=1, subsample=1,\n",
      "                                            tree_method='exact',\n",
      "                                            validate_parameters=1,\n",
      "                                            verbosity=None))])\n",
      "kfold  13  score: 91.65%\n",
      "kfold  13  score: 95.24%\n",
      "[21:48:38] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:48:38] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "VotingClassifier(estimators=[('gb',\n",
      "                              GradientBoostingClassifier(max_depth=12,\n",
      "                                                         min_samples_split=12,\n",
      "                                                         n_estimators=5,\n",
      "                                                         random_state=3)),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(max_depth=12,\n",
      "                                                     n_estimators=10,\n",
      "                                                     random_state=3)),\n",
      "                             ('xgb',\n",
      "                              XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "                                            colsample_bylevel=1,\n",
      "                                            colsample_bynode=1,\n",
      "                                            colsample_bytree=1,\n",
      "                                            enable_categorical=False, gamma=0,\n",
      "                                            gpu...\n",
      "                                            interaction_constraints='',\n",
      "                                            learning_rate=0.300000012,\n",
      "                                            max_delta_step=0, max_depth=12,\n",
      "                                            max_leaf_nodes=None,\n",
      "                                            min_child_weight=1,\n",
      "                                            min_samples_split=1, missing=nan,\n",
      "                                            monotone_constraints='()',\n",
      "                                            n_estimators=10, n_jobs=8,\n",
      "                                            num_parallel_tree=1,\n",
      "                                            predictor='auto', random_state=3,\n",
      "                                            reg_alpha=0, reg_lambda=1,\n",
      "                                            scale_pos_weight=1, subsample=1,\n",
      "                                            tree_method='exact',\n",
      "                                            validate_parameters=1,\n",
      "                                            verbosity=None))])\n",
      "kfold  13  score: 92.79%\n",
      "kfold  13  score: 96.65%\n",
      "[21:48:39] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:48:39] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "VotingClassifier(estimators=[('gb',\n",
      "                              GradientBoostingClassifier(max_depth=12,\n",
      "                                                         min_samples_split=12,\n",
      "                                                         n_estimators=5,\n",
      "                                                         random_state=3)),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(max_depth=12,\n",
      "                                                     n_estimators=10,\n",
      "                                                     random_state=3)),\n",
      "                             ('xgb',\n",
      "                              XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "                                            colsample_bylevel=1,\n",
      "                                            colsample_bynode=1,\n",
      "                                            colsample_bytree=1,\n",
      "                                            enable_categorical=False, gamma=0,\n",
      "                                            gpu...\n",
      "                                            interaction_constraints='',\n",
      "                                            learning_rate=0.300000012,\n",
      "                                            max_delta_step=0, max_depth=12,\n",
      "                                            max_leaf_nodes=None,\n",
      "                                            min_child_weight=1,\n",
      "                                            min_samples_split=1, missing=nan,\n",
      "                                            monotone_constraints='()',\n",
      "                                            n_estimators=10, n_jobs=8,\n",
      "                                            num_parallel_tree=1,\n",
      "                                            predictor='auto', random_state=3,\n",
      "                                            reg_alpha=0, reg_lambda=1,\n",
      "                                            scale_pos_weight=1, subsample=1,\n",
      "                                            tree_method='exact',\n",
      "                                            validate_parameters=1,\n",
      "                                            verbosity=None))])\n",
      "kfold  13  score: 93.36%\n",
      "kfold  13  score: 97.30%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21:48:39] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:48:39] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "VotingClassifier(estimators=[('gb',\n",
      "                              GradientBoostingClassifier(max_depth=12,\n",
      "                                                         min_samples_split=12,\n",
      "                                                         n_estimators=5,\n",
      "                                                         random_state=3)),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(max_depth=12,\n",
      "                                                     n_estimators=10,\n",
      "                                                     random_state=3)),\n",
      "                             ('xgb',\n",
      "                              XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "                                            colsample_bylevel=1,\n",
      "                                            colsample_bynode=1,\n",
      "                                            colsample_bytree=1,\n",
      "                                            enable_categorical=False, gamma=0,\n",
      "                                            gpu...\n",
      "                                            interaction_constraints='',\n",
      "                                            learning_rate=0.300000012,\n",
      "                                            max_delta_step=0, max_depth=12,\n",
      "                                            max_leaf_nodes=None,\n",
      "                                            min_child_weight=1,\n",
      "                                            min_samples_split=1, missing=nan,\n",
      "                                            monotone_constraints='()',\n",
      "                                            n_estimators=10, n_jobs=8,\n",
      "                                            num_parallel_tree=1,\n",
      "                                            predictor='auto', random_state=3,\n",
      "                                            reg_alpha=0, reg_lambda=1,\n",
      "                                            scale_pos_weight=1, subsample=1,\n",
      "                                            tree_method='exact',\n",
      "                                            validate_parameters=1,\n",
      "                                            verbosity=None))])\n",
      "kfold  13  score: 91.84%\n",
      "kfold  13  score: 95.87%\n",
      "Average score over k-fold 92.30% (+/-1.34%) [Ensemble Majority]\n",
      "./Files_Majority_vote/128_features/filter14.csv\n",
      "n_estimators = 100 \n",
      "\n",
      "10-fold cross validation for file :  14\n",
      "GradientBoostingClassifier(max_depth=12, min_samples_split=12, n_estimators=5,\n",
      "                           random_state=3)\n",
      "kfold  14  score: 87.11%\n",
      "kfold  14  score: 91.74%\n",
      "GradientBoostingClassifier(max_depth=12, min_samples_split=12, n_estimators=5,\n",
      "                           random_state=3)\n",
      "kfold  14  score: 87.11%\n",
      "kfold  14  score: 90.68%\n",
      "GradientBoostingClassifier(max_depth=12, min_samples_split=12, n_estimators=5,\n",
      "                           random_state=3)\n",
      "kfold  14  score: 86.72%\n",
      "kfold  14  score: 90.75%\n",
      "GradientBoostingClassifier(max_depth=12, min_samples_split=12, n_estimators=5,\n",
      "                           random_state=3)\n",
      "kfold  14  score: 84.77%\n",
      "kfold  14  score: 89.92%\n",
      "GradientBoostingClassifier(max_depth=12, min_samples_split=12, n_estimators=5,\n",
      "                           random_state=3)\n",
      "kfold  14  score: 86.72%\n",
      "kfold  14  score: 89.98%\n",
      "GradientBoostingClassifier(max_depth=12, min_samples_split=12, n_estimators=5,\n",
      "                           random_state=3)\n",
      "kfold  14  score: 86.50%\n",
      "kfold  14  score: 90.62%\n",
      "GradientBoostingClassifier(max_depth=12, min_samples_split=12, n_estimators=5,\n",
      "                           random_state=3)\n",
      "kfold  14  score: 86.89%\n",
      "kfold  14  score: 89.73%\n",
      "GradientBoostingClassifier(max_depth=12, min_samples_split=12, n_estimators=5,\n",
      "                           random_state=3)\n",
      "kfold  14  score: 87.67%\n",
      "kfold  14  score: 90.31%\n",
      "GradientBoostingClassifier(max_depth=12, min_samples_split=12, n_estimators=5,\n",
      "                           random_state=3)\n",
      "kfold  14  score: 88.65%\n",
      "kfold  14  score: 92.14%\n",
      "GradientBoostingClassifier(max_depth=12, min_samples_split=12, n_estimators=5,\n",
      "                           random_state=3)\n",
      "kfold  14  score: 88.45%\n",
      "kfold  14  score: 90.53%\n",
      "Average score over k-fold 87.06% (+/-1.03%) [Gradient Boosting]\n",
      "RandomForestClassifier(max_depth=12, n_estimators=10, random_state=3)\n",
      "kfold  14  score: 90.62%\n",
      "kfold  14  score: 95.85%\n",
      "RandomForestClassifier(max_depth=12, n_estimators=10, random_state=3)\n",
      "kfold  14  score: 91.99%\n",
      "kfold  14  score: 96.65%\n",
      "RandomForestClassifier(max_depth=12, n_estimators=10, random_state=3)\n",
      "kfold  14  score: 91.41%\n",
      "kfold  14  score: 95.61%\n",
      "RandomForestClassifier(max_depth=12, n_estimators=10, random_state=3)\n",
      "kfold  14  score: 91.80%\n",
      "kfold  14  score: 95.18%\n",
      "RandomForestClassifier(max_depth=12, n_estimators=10, random_state=3)\n",
      "kfold  14  score: 89.65%\n",
      "kfold  14  score: 94.46%\n",
      "RandomForestClassifier(max_depth=12, n_estimators=10, random_state=3)\n",
      "kfold  14  score: 89.24%\n",
      "kfold  14  score: 94.40%\n",
      "RandomForestClassifier(max_depth=12, n_estimators=10, random_state=3)\n",
      "kfold  14  score: 91.98%\n",
      "kfold  14  score: 95.09%\n",
      "RandomForestClassifier(max_depth=12, n_estimators=10, random_state=3)\n",
      "kfold  14  score: 92.37%\n",
      "kfold  14  score: 95.37%\n",
      "RandomForestClassifier(max_depth=12, n_estimators=10, random_state=3)\n",
      "kfold  14  score: 91.98%\n",
      "kfold  14  score: 95.79%\n",
      "RandomForestClassifier(max_depth=12, n_estimators=10, random_state=3)\n",
      "kfold  14  score: 89.82%\n",
      "kfold  14  score: 94.40%\n",
      "Average score over k-fold 91.09% (+/-1.09%) [Random Forest]\n",
      "[21:48:43] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:48:43] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
      "              gamma=0, gpu_id=-1, importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.300000012,\n",
      "              max_delta_step=0, max_depth=12, max_leaf_nodes=None,\n",
      "              min_child_weight=1, min_samples_split=1, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=10, n_jobs=8,\n",
      "              num_parallel_tree=1, predictor='auto', random_state=3,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)\n",
      "kfold  14  score: 93.36%\n",
      "kfold  14  score: 98.94%\n",
      "[21:48:43] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:48:43] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
      "              gamma=0, gpu_id=-1, importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.300000012,\n",
      "              max_delta_step=0, max_depth=12, max_leaf_nodes=None,\n",
      "              min_child_weight=1, min_samples_split=1, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=10, n_jobs=8,\n",
      "              num_parallel_tree=1, predictor='auto', random_state=3,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)\n",
      "kfold  14  score: 93.55%\n",
      "kfold  14  score: 99.13%\n",
      "[21:48:43] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:48:43] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
      "              gamma=0, gpu_id=-1, importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.300000012,\n",
      "              max_delta_step=0, max_depth=12, max_leaf_nodes=None,\n",
      "              min_child_weight=1, min_samples_split=1, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=10, n_jobs=8,\n",
      "              num_parallel_tree=1, predictor='auto', random_state=3,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)\n",
      "kfold  14  score: 94.14%\n",
      "kfold  14  score: 99.26%\n",
      "[21:48:44] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:48:44] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
      "              gamma=0, gpu_id=-1, importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.300000012,\n",
      "              max_delta_step=0, max_depth=12, max_leaf_nodes=None,\n",
      "              min_child_weight=1, min_samples_split=1, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=10, n_jobs=8,\n",
      "              num_parallel_tree=1, predictor='auto', random_state=3,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              tree_method='exact', validate_parameters=1, verbosity=None)\n",
      "kfold  14  score: 93.55%\n",
      "kfold  14  score: 98.59%\n",
      "[21:48:44] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:48:44] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
      "              gamma=0, gpu_id=-1, importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.300000012,\n",
      "              max_delta_step=0, max_depth=12, max_leaf_nodes=None,\n",
      "              min_child_weight=1, min_samples_split=1, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=10, n_jobs=8,\n",
      "              num_parallel_tree=1, predictor='auto', random_state=3,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)\n",
      "kfold  14  score: 92.77%\n",
      "kfold  14  score: 98.46%\n",
      "[21:48:44] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:48:44] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
      "              gamma=0, gpu_id=-1, importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.300000012,\n",
      "              max_delta_step=0, max_depth=12, max_leaf_nodes=None,\n",
      "              min_child_weight=1, min_samples_split=1, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=10, n_jobs=8,\n",
      "              num_parallel_tree=1, predictor='auto', random_state=3,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)\n",
      "kfold  14  score: 93.15%\n",
      "kfold  14  score: 98.94%\n",
      "[21:48:44] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:48:44] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
      "              gamma=0, gpu_id=-1, importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.300000012,\n",
      "              max_delta_step=0, max_depth=12, max_leaf_nodes=None,\n",
      "              min_child_weight=1, min_samples_split=1, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=10, n_jobs=8,\n",
      "              num_parallel_tree=1, predictor='auto', random_state=3,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)\n",
      "kfold  14  score: 93.54%\n",
      "kfold  14  score: 98.52%\n",
      "[21:48:44] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:48:44] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
      "              gamma=0, gpu_id=-1, importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.300000012,\n",
      "              max_delta_step=0, max_depth=12, max_leaf_nodes=None,\n",
      "              min_child_weight=1, min_samples_split=1, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=10, n_jobs=8,\n",
      "              num_parallel_tree=1, predictor='auto', random_state=3,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)\n",
      "kfold  14  score: 94.32%\n",
      "kfold  14  score: 98.94%\n",
      "[21:48:44] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:48:44] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
      "              gamma=0, gpu_id=-1, importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.300000012,\n",
      "              max_delta_step=0, max_depth=12, max_leaf_nodes=None,\n",
      "              min_child_weight=1, min_samples_split=1, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=10, n_jobs=8,\n",
      "              num_parallel_tree=1, predictor='auto', random_state=3,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)\n",
      "kfold  14  score: 93.54%\n",
      "kfold  14  score: 99.20%\n",
      "[21:48:44] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:48:44] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
      "              gamma=0, gpu_id=-1, importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.300000012,\n",
      "              max_delta_step=0, max_depth=12, max_leaf_nodes=None,\n",
      "              min_child_weight=1, min_samples_split=1, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=10, n_jobs=8,\n",
      "              num_parallel_tree=1, predictor='auto', random_state=3,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)\n",
      "kfold  14  score: 93.15%\n",
      "kfold  14  score: 98.76%\n",
      "Average score over k-fold 93.51% (+/-0.43%) [XGBClassifier]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21:48:45] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:48:45] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "VotingClassifier(estimators=[('gb',\n",
      "                              GradientBoostingClassifier(max_depth=12,\n",
      "                                                         min_samples_split=12,\n",
      "                                                         n_estimators=5,\n",
      "                                                         random_state=3)),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(max_depth=12,\n",
      "                                                     n_estimators=10,\n",
      "                                                     random_state=3)),\n",
      "                             ('xgb',\n",
      "                              XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "                                            colsample_bylevel=1,\n",
      "                                            colsample_bynode=1,\n",
      "                                            colsample_bytree=1,\n",
      "                                            enable_categorical=False, gamma=0,\n",
      "                                            gpu...\n",
      "                                            interaction_constraints='',\n",
      "                                            learning_rate=0.300000012,\n",
      "                                            max_delta_step=0, max_depth=12,\n",
      "                                            max_leaf_nodes=None,\n",
      "                                            min_child_weight=1,\n",
      "                                            min_samples_split=1, missing=nan,\n",
      "                                            monotone_constraints='()',\n",
      "                                            n_estimators=10, n_jobs=8,\n",
      "                                            num_parallel_tree=1,\n",
      "                                            predictor='auto', random_state=3,\n",
      "                                            reg_alpha=0, reg_lambda=1,\n",
      "                                            scale_pos_weight=1, subsample=1,\n",
      "                                            tree_method='exact',\n",
      "                                            validate_parameters=1,\n",
      "                                            verbosity=None))])\n",
      "kfold  14  score: 91.02%\n",
      "kfold  14  score: 96.13%\n",
      "[21:48:45] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:48:45] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "VotingClassifier(estimators=[('gb',\n",
      "                              GradientBoostingClassifier(max_depth=12,\n",
      "                                                         min_samples_split=12,\n",
      "                                                         n_estimators=5,\n",
      "                                                         random_state=3)),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(max_depth=12,\n",
      "                                                     n_estimators=10,\n",
      "                                                     random_state=3)),\n",
      "                             ('xgb',\n",
      "                              XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "                                            colsample_bylevel=1,\n",
      "                                            colsample_bynode=1,\n",
      "                                            colsample_bytree=1,\n",
      "                                            enable_categorical=False, gamma=0,\n",
      "                                            gpu...\n",
      "                                            interaction_constraints='',\n",
      "                                            learning_rate=0.300000012,\n",
      "                                            max_delta_step=0, max_depth=12,\n",
      "                                            max_leaf_nodes=None,\n",
      "                                            min_child_weight=1,\n",
      "                                            min_samples_split=1, missing=nan,\n",
      "                                            monotone_constraints='()',\n",
      "                                            n_estimators=10, n_jobs=8,\n",
      "                                            num_parallel_tree=1,\n",
      "                                            predictor='auto', random_state=3,\n",
      "                                            reg_alpha=0, reg_lambda=1,\n",
      "                                            scale_pos_weight=1, subsample=1,\n",
      "                                            tree_method='exact',\n",
      "                                            validate_parameters=1,\n",
      "                                            verbosity=None))])\n",
      "kfold  14  score: 91.80%\n",
      "kfold  14  score: 96.76%\n",
      "[21:48:46] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:48:46] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "VotingClassifier(estimators=[('gb',\n",
      "                              GradientBoostingClassifier(max_depth=12,\n",
      "                                                         min_samples_split=12,\n",
      "                                                         n_estimators=5,\n",
      "                                                         random_state=3)),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(max_depth=12,\n",
      "                                                     n_estimators=10,\n",
      "                                                     random_state=3)),\n",
      "                             ('xgb',\n",
      "                              XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "                                            colsample_bylevel=1,\n",
      "                                            colsample_bynode=1,\n",
      "                                            colsample_bytree=1,\n",
      "                                            enable_categorical=False, gamma=0,\n",
      "                                            gpu...\n",
      "                                            interaction_constraints='',\n",
      "                                            learning_rate=0.300000012,\n",
      "                                            max_delta_step=0, max_depth=12,\n",
      "                                            max_leaf_nodes=None,\n",
      "                                            min_child_weight=1,\n",
      "                                            min_samples_split=1, missing=nan,\n",
      "                                            monotone_constraints='()',\n",
      "                                            n_estimators=10, n_jobs=8,\n",
      "                                            num_parallel_tree=1,\n",
      "                                            predictor='auto', random_state=3,\n",
      "                                            reg_alpha=0, reg_lambda=1,\n",
      "                                            scale_pos_weight=1, subsample=1,\n",
      "                                            tree_method='exact',\n",
      "                                            validate_parameters=1,\n",
      "                                            verbosity=None))])\n",
      "kfold  14  score: 91.02%\n",
      "kfold  14  score: 95.96%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21:48:46] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:48:46] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "VotingClassifier(estimators=[('gb',\n",
      "                              GradientBoostingClassifier(max_depth=12,\n",
      "                                                         min_samples_split=12,\n",
      "                                                         n_estimators=5,\n",
      "                                                         random_state=3)),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(max_depth=12,\n",
      "                                                     n_estimators=10,\n",
      "                                                     random_state=3)),\n",
      "                             ('xgb',\n",
      "                              XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "                                            colsample_bylevel=1,\n",
      "                                            colsample_bynode=1,\n",
      "                                            colsample_bytree=1,\n",
      "                                            enable_categorical=False, gamma=0,\n",
      "                                            gpu...\n",
      "                                            interaction_constraints='',\n",
      "                                            learning_rate=0.300000012,\n",
      "                                            max_delta_step=0, max_depth=12,\n",
      "                                            max_leaf_nodes=None,\n",
      "                                            min_child_weight=1,\n",
      "                                            min_samples_split=1, missing=nan,\n",
      "                                            monotone_constraints='()',\n",
      "                                            n_estimators=10, n_jobs=8,\n",
      "                                            num_parallel_tree=1,\n",
      "                                            predictor='auto', random_state=3,\n",
      "                                            reg_alpha=0, reg_lambda=1,\n",
      "                                            scale_pos_weight=1, subsample=1,\n",
      "                                            tree_method='exact',\n",
      "                                            validate_parameters=1,\n",
      "                                            verbosity=None))])\n",
      "kfold  14  score: 91.80%\n",
      "kfold  14  score: 96.11%\n",
      "[21:48:47] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:48:47] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "VotingClassifier(estimators=[('gb',\n",
      "                              GradientBoostingClassifier(max_depth=12,\n",
      "                                                         min_samples_split=12,\n",
      "                                                         n_estimators=5,\n",
      "                                                         random_state=3)),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(max_depth=12,\n",
      "                                                     n_estimators=10,\n",
      "                                                     random_state=3)),\n",
      "                             ('xgb',\n",
      "                              XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "                                            colsample_bylevel=1,\n",
      "                                            colsample_bynode=1,\n",
      "                                            colsample_bytree=1,\n",
      "                                            enable_categorical=False, gamma=0,\n",
      "                                            gpu...\n",
      "                                            interaction_constraints='',\n",
      "                                            learning_rate=0.300000012,\n",
      "                                            max_delta_step=0, max_depth=12,\n",
      "                                            max_leaf_nodes=None,\n",
      "                                            min_child_weight=1,\n",
      "                                            min_samples_split=1, missing=nan,\n",
      "                                            monotone_constraints='()',\n",
      "                                            n_estimators=10, n_jobs=8,\n",
      "                                            num_parallel_tree=1,\n",
      "                                            predictor='auto', random_state=3,\n",
      "                                            reg_alpha=0, reg_lambda=1,\n",
      "                                            scale_pos_weight=1, subsample=1,\n",
      "                                            tree_method='exact',\n",
      "                                            validate_parameters=1,\n",
      "                                            verbosity=None))])\n",
      "kfold  14  score: 90.62%\n",
      "kfold  14  score: 95.50%\n",
      "[21:48:47] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:48:47] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "VotingClassifier(estimators=[('gb',\n",
      "                              GradientBoostingClassifier(max_depth=12,\n",
      "                                                         min_samples_split=12,\n",
      "                                                         n_estimators=5,\n",
      "                                                         random_state=3)),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(max_depth=12,\n",
      "                                                     n_estimators=10,\n",
      "                                                     random_state=3)),\n",
      "                             ('xgb',\n",
      "                              XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "                                            colsample_bylevel=1,\n",
      "                                            colsample_bynode=1,\n",
      "                                            colsample_bytree=1,\n",
      "                                            enable_categorical=False, gamma=0,\n",
      "                                            gpu...\n",
      "                                            interaction_constraints='',\n",
      "                                            learning_rate=0.300000012,\n",
      "                                            max_delta_step=0, max_depth=12,\n",
      "                                            max_leaf_nodes=None,\n",
      "                                            min_child_weight=1,\n",
      "                                            min_samples_split=1, missing=nan,\n",
      "                                            monotone_constraints='()',\n",
      "                                            n_estimators=10, n_jobs=8,\n",
      "                                            num_parallel_tree=1,\n",
      "                                            predictor='auto', random_state=3,\n",
      "                                            reg_alpha=0, reg_lambda=1,\n",
      "                                            scale_pos_weight=1, subsample=1,\n",
      "                                            tree_method='exact',\n",
      "                                            validate_parameters=1,\n",
      "                                            verbosity=None))])\n",
      "kfold  14  score: 90.22%\n",
      "kfold  14  score: 95.22%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21:48:48] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:48:48] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "VotingClassifier(estimators=[('gb',\n",
      "                              GradientBoostingClassifier(max_depth=12,\n",
      "                                                         min_samples_split=12,\n",
      "                                                         n_estimators=5,\n",
      "                                                         random_state=3)),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(max_depth=12,\n",
      "                                                     n_estimators=10,\n",
      "                                                     random_state=3)),\n",
      "                             ('xgb',\n",
      "                              XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "                                            colsample_bylevel=1,\n",
      "                                            colsample_bynode=1,\n",
      "                                            colsample_bytree=1,\n",
      "                                            enable_categorical=False, gamma=0,\n",
      "                                            gpu...\n",
      "                                            interaction_constraints='',\n",
      "                                            learning_rate=0.300000012,\n",
      "                                            max_delta_step=0, max_depth=12,\n",
      "                                            max_leaf_nodes=None,\n",
      "                                            min_child_weight=1,\n",
      "                                            min_samples_split=1, missing=nan,\n",
      "                                            monotone_constraints='()',\n",
      "                                            n_estimators=10, n_jobs=8,\n",
      "                                            num_parallel_tree=1,\n",
      "                                            predictor='auto', random_state=3,\n",
      "                                            reg_alpha=0, reg_lambda=1,\n",
      "                                            scale_pos_weight=1, subsample=1,\n",
      "                                            tree_method='exact',\n",
      "                                            validate_parameters=1,\n",
      "                                            verbosity=None))])\n",
      "kfold  14  score: 91.98%\n",
      "kfold  14  score: 95.68%\n",
      "[21:48:48] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:48:48] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "VotingClassifier(estimators=[('gb',\n",
      "                              GradientBoostingClassifier(max_depth=12,\n",
      "                                                         min_samples_split=12,\n",
      "                                                         n_estimators=5,\n",
      "                                                         random_state=3)),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(max_depth=12,\n",
      "                                                     n_estimators=10,\n",
      "                                                     random_state=3)),\n",
      "                             ('xgb',\n",
      "                              XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "                                            colsample_bylevel=1,\n",
      "                                            colsample_bynode=1,\n",
      "                                            colsample_bytree=1,\n",
      "                                            enable_categorical=False, gamma=0,\n",
      "                                            gpu...\n",
      "                                            interaction_constraints='',\n",
      "                                            learning_rate=0.300000012,\n",
      "                                            max_delta_step=0, max_depth=12,\n",
      "                                            max_leaf_nodes=None,\n",
      "                                            min_child_weight=1,\n",
      "                                            min_samples_split=1, missing=nan,\n",
      "                                            monotone_constraints='()',\n",
      "                                            n_estimators=10, n_jobs=8,\n",
      "                                            num_parallel_tree=1,\n",
      "                                            predictor='auto', random_state=3,\n",
      "                                            reg_alpha=0, reg_lambda=1,\n",
      "                                            scale_pos_weight=1, subsample=1,\n",
      "                                            tree_method='exact',\n",
      "                                            validate_parameters=1,\n",
      "                                            verbosity=None))])\n",
      "kfold  14  score: 92.17%\n",
      "kfold  14  score: 95.89%\n",
      "[21:48:49] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:48:49] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "VotingClassifier(estimators=[('gb',\n",
      "                              GradientBoostingClassifier(max_depth=12,\n",
      "                                                         min_samples_split=12,\n",
      "                                                         n_estimators=5,\n",
      "                                                         random_state=3)),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(max_depth=12,\n",
      "                                                     n_estimators=10,\n",
      "                                                     random_state=3)),\n",
      "                             ('xgb',\n",
      "                              XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "                                            colsample_bylevel=1,\n",
      "                                            colsample_bynode=1,\n",
      "                                            colsample_bytree=1,\n",
      "                                            enable_categorical=False, gamma=0,\n",
      "                                            gpu...\n",
      "                                            interaction_constraints='',\n",
      "                                            learning_rate=0.300000012,\n",
      "                                            max_delta_step=0, max_depth=12,\n",
      "                                            max_leaf_nodes=None,\n",
      "                                            min_child_weight=1,\n",
      "                                            min_samples_split=1, missing=nan,\n",
      "                                            monotone_constraints='()',\n",
      "                                            n_estimators=10, n_jobs=8,\n",
      "                                            num_parallel_tree=1,\n",
      "                                            predictor='auto', random_state=3,\n",
      "                                            reg_alpha=0, reg_lambda=1,\n",
      "                                            scale_pos_weight=1, subsample=1,\n",
      "                                            tree_method='exact',\n",
      "                                            validate_parameters=1,\n",
      "                                            verbosity=None))])\n",
      "kfold  14  score: 91.59%\n",
      "kfold  14  score: 96.33%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21:48:49] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:48:49] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "VotingClassifier(estimators=[('gb',\n",
      "                              GradientBoostingClassifier(max_depth=12,\n",
      "                                                         min_samples_split=12,\n",
      "                                                         n_estimators=5,\n",
      "                                                         random_state=3)),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(max_depth=12,\n",
      "                                                     n_estimators=10,\n",
      "                                                     random_state=3)),\n",
      "                             ('xgb',\n",
      "                              XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "                                            colsample_bylevel=1,\n",
      "                                            colsample_bynode=1,\n",
      "                                            colsample_bytree=1,\n",
      "                                            enable_categorical=False, gamma=0,\n",
      "                                            gpu...\n",
      "                                            interaction_constraints='',\n",
      "                                            learning_rate=0.300000012,\n",
      "                                            max_delta_step=0, max_depth=12,\n",
      "                                            max_leaf_nodes=None,\n",
      "                                            min_child_weight=1,\n",
      "                                            min_samples_split=1, missing=nan,\n",
      "                                            monotone_constraints='()',\n",
      "                                            n_estimators=10, n_jobs=8,\n",
      "                                            num_parallel_tree=1,\n",
      "                                            predictor='auto', random_state=3,\n",
      "                                            reg_alpha=0, reg_lambda=1,\n",
      "                                            scale_pos_weight=1, subsample=1,\n",
      "                                            tree_method='exact',\n",
      "                                            validate_parameters=1,\n",
      "                                            verbosity=None))])\n",
      "kfold  14  score: 90.41%\n",
      "kfold  14  score: 94.81%\n",
      "Average score over k-fold 91.26% (+/-0.66%) [Ensemble Majority]\n",
      "./Files_Majority_vote/128_features/filter15.csv\n",
      "n_estimators = 100 \n",
      "\n",
      "10-fold cross validation for file :  15\n",
      "GradientBoostingClassifier(max_depth=12, min_samples_split=12, n_estimators=5,\n",
      "                           random_state=3)\n",
      "kfold  15  score: 89.02%\n",
      "kfold  15  score: 93.60%\n",
      "GradientBoostingClassifier(max_depth=12, min_samples_split=12, n_estimators=5,\n",
      "                           random_state=3)\n",
      "kfold  15  score: 90.15%\n",
      "kfold  15  score: 92.69%\n",
      "GradientBoostingClassifier(max_depth=12, min_samples_split=12, n_estimators=5,\n",
      "                           random_state=3)\n",
      "kfold  15  score: 86.93%\n",
      "kfold  15  score: 93.60%\n",
      "GradientBoostingClassifier(max_depth=12, min_samples_split=12, n_estimators=5,\n",
      "                           random_state=3)\n",
      "kfold  15  score: 92.99%\n",
      "kfold  15  score: 96.38%\n",
      "GradientBoostingClassifier(max_depth=12, min_samples_split=12, n_estimators=5,\n",
      "                           random_state=3)\n",
      "kfold  15  score: 91.10%\n",
      "kfold  15  score: 95.37%\n",
      "GradientBoostingClassifier(max_depth=12, min_samples_split=12, n_estimators=5,\n",
      "                           random_state=3)\n",
      "kfold  15  score: 87.88%\n",
      "kfold  15  score: 92.94%\n",
      "GradientBoostingClassifier(max_depth=12, min_samples_split=12, n_estimators=5,\n",
      "                           random_state=3)\n",
      "kfold  15  score: 91.46%\n",
      "kfold  15  score: 95.75%\n",
      "GradientBoostingClassifier(max_depth=12, min_samples_split=12, n_estimators=5,\n",
      "                           random_state=3)\n",
      "kfold  15  score: 89.56%\n",
      "kfold  15  score: 95.11%\n",
      "GradientBoostingClassifier(max_depth=12, min_samples_split=12, n_estimators=5,\n",
      "                           random_state=3)\n",
      "kfold  15  score: 86.34%\n",
      "kfold  15  score: 92.52%\n",
      "GradientBoostingClassifier(max_depth=12, min_samples_split=12, n_estimators=5,\n",
      "                           random_state=3)\n",
      "kfold  15  score: 91.27%\n",
      "kfold  15  score: 95.94%\n",
      "Average score over k-fold 89.67% (+/-2.03%) [Gradient Boosting]\n",
      "RandomForestClassifier(max_depth=12, n_estimators=10, random_state=3)\n",
      "kfold  15  score: 95.27%\n",
      "kfold  15  score: 99.07%\n",
      "RandomForestClassifier(max_depth=12, n_estimators=10, random_state=3)\n",
      "kfold  15  score: 95.83%\n",
      "kfold  15  score: 99.18%\n",
      "RandomForestClassifier(max_depth=12, n_estimators=10, random_state=3)\n",
      "kfold  15  score: 92.42%\n",
      "kfold  15  score: 98.78%\n",
      "RandomForestClassifier(max_depth=12, n_estimators=10, random_state=3)\n",
      "kfold  15  score: 94.51%\n",
      "kfold  15  score: 98.65%\n",
      "RandomForestClassifier(max_depth=12, n_estimators=10, random_state=3)\n",
      "kfold  15  score: 93.56%\n",
      "kfold  15  score: 98.29%\n",
      "RandomForestClassifier(max_depth=12, n_estimators=10, random_state=3)\n",
      "kfold  15  score: 91.86%\n",
      "kfold  15  score: 98.72%\n",
      "RandomForestClassifier(max_depth=12, n_estimators=10, random_state=3)\n",
      "kfold  15  score: 92.79%\n",
      "kfold  15  score: 98.34%\n",
      "RandomForestClassifier(max_depth=12, n_estimators=10, random_state=3)\n",
      "kfold  15  score: 92.79%\n",
      "kfold  15  score: 98.32%\n",
      "RandomForestClassifier(max_depth=12, n_estimators=10, random_state=3)\n",
      "kfold  15  score: 92.41%\n",
      "kfold  15  score: 98.82%\n",
      "RandomForestClassifier(max_depth=12, n_estimators=10, random_state=3)\n",
      "kfold  15  score: 92.60%\n",
      "kfold  15  score: 98.78%\n",
      "Average score over k-fold 93.40% (+/-1.28%) [Random Forest]\n",
      "[21:48:52] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:48:52] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
      "              gamma=0, gpu_id=-1, importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.300000012,\n",
      "              max_delta_step=0, max_depth=12, max_leaf_nodes=None,\n",
      "              min_child_weight=1, min_samples_split=1, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=10, n_jobs=8,\n",
      "              num_parallel_tree=1, predictor='auto', random_state=3,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)\n",
      "kfold  15  score: 94.70%\n",
      "kfold  15  score: 99.47%\n",
      "[21:48:53] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:48:53] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
      "              gamma=0, gpu_id=-1, importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.300000012,\n",
      "              max_delta_step=0, max_depth=12, max_leaf_nodes=None,\n",
      "              min_child_weight=1, min_samples_split=1, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=10, n_jobs=8,\n",
      "              num_parallel_tree=1, predictor='auto', random_state=3,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)\n",
      "kfold  15  score: 95.27%\n",
      "kfold  15  score: 99.43%\n",
      "[21:48:53] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:48:53] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
      "              gamma=0, gpu_id=-1, importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.300000012,\n",
      "              max_delta_step=0, max_depth=12, max_leaf_nodes=None,\n",
      "              min_child_weight=1, min_samples_split=1, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=10, n_jobs=8,\n",
      "              num_parallel_tree=1, predictor='auto', random_state=3,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)\n",
      "kfold  15  score: 92.42%\n",
      "kfold  15  score: 99.49%\n",
      "[21:48:53] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:48:53] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
      "              gamma=0, gpu_id=-1, importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.300000012,\n",
      "              max_delta_step=0, max_depth=12, max_leaf_nodes=None,\n",
      "              min_child_weight=1, min_samples_split=1, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=10, n_jobs=8,\n",
      "              num_parallel_tree=1, predictor='auto', random_state=3,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)\n",
      "kfold  15  score: 94.89%\n",
      "kfold  15  score: 99.54%\n",
      "[21:48:53] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:48:53] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
      "              gamma=0, gpu_id=-1, importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.300000012,\n",
      "              max_delta_step=0, max_depth=12, max_leaf_nodes=None,\n",
      "              min_child_weight=1, min_samples_split=1, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=10, n_jobs=8,\n",
      "              num_parallel_tree=1, predictor='auto', random_state=3,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)\n",
      "kfold  15  score: 93.94%\n",
      "kfold  15  score: 99.37%\n",
      "[21:48:53] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:48:53] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
      "              gamma=0, gpu_id=-1, importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.300000012,\n",
      "              max_delta_step=0, max_depth=12, max_leaf_nodes=None,\n",
      "              min_child_weight=1, min_samples_split=1, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=10, n_jobs=8,\n",
      "              num_parallel_tree=1, predictor='auto', random_state=3,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)\n",
      "kfold  15  score: 92.99%\n",
      "kfold  15  score: 99.47%\n",
      "[21:48:53] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:48:53] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
      "              gamma=0, gpu_id=-1, importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.300000012,\n",
      "              max_delta_step=0, max_depth=12, max_leaf_nodes=None,\n",
      "              min_child_weight=1, min_samples_split=1, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=10, n_jobs=8,\n",
      "              num_parallel_tree=1, predictor='auto', random_state=3,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)\n",
      "kfold  15  score: 94.88%\n",
      "kfold  15  score: 99.58%\n",
      "[21:48:53] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:48:53] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
      "              gamma=0, gpu_id=-1, importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.300000012,\n",
      "              max_delta_step=0, max_depth=12, max_leaf_nodes=None,\n",
      "              min_child_weight=1, min_samples_split=1, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=10, n_jobs=8,\n",
      "              num_parallel_tree=1, predictor='auto', random_state=3,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)\n",
      "kfold  15  score: 93.36%\n",
      "kfold  15  score: 99.39%\n",
      "[21:48:53] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:48:53] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
      "              gamma=0, gpu_id=-1, importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.300000012,\n",
      "              max_delta_step=0, max_depth=12, max_leaf_nodes=None,\n",
      "              min_child_weight=1, min_samples_split=1, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=10, n_jobs=8,\n",
      "              num_parallel_tree=1, predictor='auto', random_state=3,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)\n",
      "kfold  15  score: 92.22%\n",
      "kfold  15  score: 99.37%\n",
      "[21:48:53] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:48:53] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
      "              gamma=0, gpu_id=-1, importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.300000012,\n",
      "              max_delta_step=0, max_depth=12, max_leaf_nodes=None,\n",
      "              min_child_weight=1, min_samples_split=1, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=10, n_jobs=8,\n",
      "              num_parallel_tree=1, predictor='auto', random_state=3,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)\n",
      "kfold  15  score: 93.17%\n",
      "kfold  15  score: 99.41%\n",
      "Average score over k-fold 93.78% (+/-1.05%) [XGBClassifier]\n",
      "[21:48:54] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:48:54] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "VotingClassifier(estimators=[('gb',\n",
      "                              GradientBoostingClassifier(max_depth=12,\n",
      "                                                         min_samples_split=12,\n",
      "                                                         n_estimators=5,\n",
      "                                                         random_state=3)),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(max_depth=12,\n",
      "                                                     n_estimators=10,\n",
      "                                                     random_state=3)),\n",
      "                             ('xgb',\n",
      "                              XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "                                            colsample_bylevel=1,\n",
      "                                            colsample_bynode=1,\n",
      "                                            colsample_bytree=1,\n",
      "                                            enable_categorical=False, gamma=0,\n",
      "                                            gpu...\n",
      "                                            interaction_constraints='',\n",
      "                                            learning_rate=0.300000012,\n",
      "                                            max_delta_step=0, max_depth=12,\n",
      "                                            max_leaf_nodes=None,\n",
      "                                            min_child_weight=1,\n",
      "                                            min_samples_split=1, missing=nan,\n",
      "                                            monotone_constraints='()',\n",
      "                                            n_estimators=10, n_jobs=8,\n",
      "                                            num_parallel_tree=1,\n",
      "                                            predictor='auto', random_state=3,\n",
      "                                            reg_alpha=0, reg_lambda=1,\n",
      "                                            scale_pos_weight=1, subsample=1,\n",
      "                                            tree_method='exact',\n",
      "                                            validate_parameters=1,\n",
      "                                            verbosity=None))])\n",
      "kfold  15  score: 94.70%\n",
      "kfold  15  score: 99.49%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21:48:54] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:48:54] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "VotingClassifier(estimators=[('gb',\n",
      "                              GradientBoostingClassifier(max_depth=12,\n",
      "                                                         min_samples_split=12,\n",
      "                                                         n_estimators=5,\n",
      "                                                         random_state=3)),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(max_depth=12,\n",
      "                                                     n_estimators=10,\n",
      "                                                     random_state=3)),\n",
      "                             ('xgb',\n",
      "                              XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "                                            colsample_bylevel=1,\n",
      "                                            colsample_bynode=1,\n",
      "                                            colsample_bytree=1,\n",
      "                                            enable_categorical=False, gamma=0,\n",
      "                                            gpu...\n",
      "                                            interaction_constraints='',\n",
      "                                            learning_rate=0.300000012,\n",
      "                                            max_delta_step=0, max_depth=12,\n",
      "                                            max_leaf_nodes=None,\n",
      "                                            min_child_weight=1,\n",
      "                                            min_samples_split=1, missing=nan,\n",
      "                                            monotone_constraints='()',\n",
      "                                            n_estimators=10, n_jobs=8,\n",
      "                                            num_parallel_tree=1,\n",
      "                                            predictor='auto', random_state=3,\n",
      "                                            reg_alpha=0, reg_lambda=1,\n",
      "                                            scale_pos_weight=1, subsample=1,\n",
      "                                            tree_method='exact',\n",
      "                                            validate_parameters=1,\n",
      "                                            verbosity=None))])\n",
      "kfold  15  score: 96.02%\n",
      "kfold  15  score: 99.30%\n",
      "[21:48:55] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:48:55] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "VotingClassifier(estimators=[('gb',\n",
      "                              GradientBoostingClassifier(max_depth=12,\n",
      "                                                         min_samples_split=12,\n",
      "                                                         n_estimators=5,\n",
      "                                                         random_state=3)),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(max_depth=12,\n",
      "                                                     n_estimators=10,\n",
      "                                                     random_state=3)),\n",
      "                             ('xgb',\n",
      "                              XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "                                            colsample_bylevel=1,\n",
      "                                            colsample_bynode=1,\n",
      "                                            colsample_bytree=1,\n",
      "                                            enable_categorical=False, gamma=0,\n",
      "                                            gpu...\n",
      "                                            interaction_constraints='',\n",
      "                                            learning_rate=0.300000012,\n",
      "                                            max_delta_step=0, max_depth=12,\n",
      "                                            max_leaf_nodes=None,\n",
      "                                            min_child_weight=1,\n",
      "                                            min_samples_split=1, missing=nan,\n",
      "                                            monotone_constraints='()',\n",
      "                                            n_estimators=10, n_jobs=8,\n",
      "                                            num_parallel_tree=1,\n",
      "                                            predictor='auto', random_state=3,\n",
      "                                            reg_alpha=0, reg_lambda=1,\n",
      "                                            scale_pos_weight=1, subsample=1,\n",
      "                                            tree_method='exact',\n",
      "                                            validate_parameters=1,\n",
      "                                            verbosity=None))])\n",
      "kfold  15  score: 91.86%\n",
      "kfold  15  score: 99.03%\n"
     ]
    }
   ],
   "source": [
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "import warnings\n",
    "warnings.warn = warn\n",
    "\n",
    "name = \"./Files_Majority_vote/128_features/filter\"\n",
    "#name = \"e:/$Notebooks/Original_files_128_features/filter\"\n",
    "#name = \"G:/Darshana/Scada/Cistel/Marzia/Practical/Feature Selection/Multi Files/output files/Multifilter\"\n",
    "i = 0\n",
    "j = 1\n",
    "\n",
    "for i in range (1,16):\n",
    "    file_name = name+str(i)+\".csv\"\n",
    "    print(file_name)\n",
    "    df_file = pd.read_csv(file_name)\n",
    "    enc = LabelEncoder()\n",
    "    df_file[\"marker\"] = enc.fit_transform(df_file[\"marker\"])\n",
    "    df1_file = df_file.replace([np.inf, -np.inf], np.nan)\n",
    "    m1 = df1_file.dropna()\n",
    "    X= m1.iloc[:,0:14]\n",
    "    y1= m1.iloc[:,15]\n",
    "    label_encoder = LabelEncoder()\n",
    "    y = label_encoder.fit_transform(y1)\n",
    "    #y1 = to_categorical(y)\n",
    "    sc = StandardScaler()\n",
    "    X = sc.fit_transform(X)\n",
    "\n",
    "    clf1 = GradientBoostingClassifier(n_estimators= 5, max_leaf_nodes= None, max_depth=12, \n",
    "                                  random_state= 3, min_samples_split= 12, learning_rate=0.1)\n",
    "    clf2 = RandomForestClassifier(n_estimators=10,bootstrap=True, class_weight=None, \n",
    "                                  criterion='gini', max_depth=12, max_features='auto', max_leaf_nodes=None, \n",
    "                                  min_impurity_decrease=0.0, min_samples_leaf=1, \n",
    "                                  min_samples_split=2, min_weight_fraction_leaf=0.0, n_jobs=None, \n",
    "                                  oob_score=False, random_state=3, verbose=0, warm_start=False) \n",
    "    clf3 = XGBClassifier(n_estimators=10, max_leaf_nodes= None, max_depth=12, random_state= 3, \n",
    "                                  min_samples_split= 1)\n",
    "    eclf = VotingClassifier(estimators=[('gb', clf1), ('rf', clf2), ('xgb', clf3)],voting='hard')\n",
    "    \n",
    "    mean_fpr = np.linspace(0, 1, 100)\n",
    "    tprs = []\n",
    "    aucs = []\n",
    "    fig, ax = plt.subplots()\n",
    "    \n",
    "    print ('n_estimators = 100 \\n')\n",
    "    print('10-fold cross validation for file : ', i)\n",
    "    df_results = pd.DataFrame() \n",
    "    for clf, label in zip([clf1, clf2, clf3, eclf], ['Gradient Boosting', 'Random Forest', 'XGBClassifier', 'Ensemble Majority']):\n",
    "        seed = 123\n",
    "        kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=seed)\n",
    "        cvscores = []\n",
    "        for train, test in kfold.split(X,y):\n",
    "            history = clf.fit(X[train],y1[train])\n",
    "            scoresTrain = clf.score(X[train], y1[train]) \n",
    "            scores = clf.score(X[test], y1[test])\n",
    "            print(clf)\n",
    "#            visual_roc = skplt.metrics.plot_roc(label, X[test], y1[test])\n",
    "#\n",
    "#            interp_tpr = interp(mean_fpr, visual_roc.fpr, visual_roc.tpr)\n",
    "#            interp_tpr[0] = 0.0\n",
    "#            tprs.append(interp_tpr)\n",
    "#            aucs.append(viz.roc_auc)\n",
    "            \n",
    "            \n",
    "            print(\"kfold \",i,\" score: %.2f%%\" % ( scores*100))\n",
    "            print(\"kfold \",i,\" score: %.2f%%\" % (scoresTrain*100)) \n",
    "            cvscores.append(scores * 100)\n",
    "        df_results[label] = cvscores\n",
    "        print(\"Average score over k-fold %.2f%% (+/-%.2f%%) [%s]\" % (np.mean(cvscores), np.std(cvscores), label))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ax.plot([0, 1], [0, 1], linestyle='--', lw=2, color='r',\n",
    "        label='Chance', alpha=.8)\n",
    "\n",
    "mean_tpr = np.mean(tprs, axis=0)\n",
    "mean_tpr[-1] = 1.0\n",
    "mean_auc = auc(mean_fpr, mean_tpr)\n",
    "std_auc = np.std(aucs)\n",
    "ax.plot(mean_fpr, mean_tpr, color='b',\n",
    "        label=r'Mean ROC (AUC = %0.2f $\\pm$ %0.2f)' % (mean_auc, std_auc),\n",
    "        lw=2, alpha=.8)\n",
    "\n",
    "std_tpr = np.std(tprs, axis=0)\n",
    "tprs_upper = np.minimum(mean_tpr + std_tpr, 1)\n",
    "tprs_lower = np.maximum(mean_tpr - std_tpr, 0)\n",
    "ax.fill_between(mean_fpr, tprs_lower, tprs_upper, color='grey', alpha=.2,\n",
    "                label=r'$\\pm$ 1 std. dev.')\n",
    "\n",
    "ax.set(xlim=[-0.05, 1.05], ylim=[-0.05, 1.05],\n",
    "       title=\"Receiver operating characteristic example\")\n",
    "ax.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "est = [('gb', clf1), ('rf', clf2), ('xgb', clf3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state= 0)\n",
    "\n",
    "vot_hard = VotingClassifier(estimators = est, voting ='soft') \n",
    "vot_hard.fit(X_train, y_train) \n",
    "y_pred = vot_hard.predict(X_test) \n",
    "y_proba = vot_hard.predict_proba(X_test)\n",
    "print(y_proba[:,1])\n",
    "print(y_proba.shape, y_pred.shape)\n",
    "score = accuracy_score(y_test, y_pred) \n",
    "print(\"Hard Voting Score \", score) \n",
    "\n",
    "ns_fpr, ns_tpr, _ = roc_curve(y_test, y_proba[:,1])\n",
    "plt.plot(ns_fpr, ns_tpr, linestyle='--', label='Soft Vote')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#EOF"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL",
   "language": "python",
   "name": "dl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
